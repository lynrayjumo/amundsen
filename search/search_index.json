{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amundsen is a metadata driven application for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as Google search for data. The project is named after Norwegian explorer Roald Amundsen , the first person to discover the South Pole. It includes three microservices, one data ingestion library and one common library. amundsenfrontendlibrary : Frontend service which is a Flask application with a React frontend. amundsensearchlibrary : Search service, which leverages Elasticsearch for search capabilities, is used to power frontend metadata searching. amundsenmetadatalibrary : Metadata service, which leverages Neo4j or Apache Atlas as the persistent layer, to provide various metadata. amundsendatabuilder : Data ingestion library for building metadata graph and search index. Users could either load the data with a python script with the library or with an Airflow DAG importing the library. amundsencommon : Amundsen Common library holds common codes among microservices in Amundsen. Documentation \u00b6 https://lyft.github.io/amundsen/ Requirements \u00b6 Python = 3.6 or 3.7 Node = v10 or v12 (v14 may have compatibility issues) npm >= 6 User Interface \u00b6 Please note that the mock images only served as demonstration purpose. Landing Page : The landing page for Amundsen including 1. search bars; 2. popular used tables; Search Preview : See inline search results as you type Table Detail Page : Visualization of a Hive / Redshift table Column detail : Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page : Visualization of table data preview which could integrate with Apache Superset or other Data Visualization Tools. Get Involved in the Community \u00b6 Want help or want to help? Use the button in our header to join our slack channel. Getting Started \u00b6 Please visit the Amundsen installation documentation for a quick start to bootstrap a default version of Amundsen with dummy data. Architecture Overview \u00b6 Please visit Architecture for Amundsen architecture overview. Supported Entities \u00b6 Tables (from Databases) People (from HR systems) Dashboards Supported Integrations \u00b6 Table Connectors \u00b6 Amazon Athena Amazon Glue and anything built over it (like Databricks Delta - which is a work in progress). Amazon Redshift Apache Cassandra Apache Druid Apache Hive CSV Google BigQuery IBM DB2 Microsoft SQL Server MySQL Oracle (through dbapi or sql_alchemy) PostgreSQL Presto Snowflake Amundsen can also connect to any database that provides dbapi or sql_alchemy interface (which most DBs provide). Dashboard Connectors \u00b6 Mode Analytics ETL Orchestration Integrations \u00b6 Apache Airflow BI Viz Tool Integrations \u00b6 Apache Superset Installation \u00b6 Please visit Installation guideline on how to install Amundsen. Roadmap \u00b6 Please visit Roadmap if you are interested in Amundsen upcoming roadmap items. Blog Posts and Interviews \u00b6 Amundsen - Lyft\u2019s data discovery & metadata engine (April 2019) Software Engineering Daily podcast on Amundsen (April 2019) How Lyft Drives Data Discovery (July 2019) Data Engineering podcast on Solving Data Discovery At Lyft (Aug 2019) Open Sourcing Amundsen: A Data Discovery And Metadata Platform (Oct 2019) Talks \u00b6 Disrupting Data Discovery { slides , recording } (Strata SF, March 2019) Amundsen: A Data Discovery Platform from Lyft { slides } (Data Council SF, April 2019) Disrupting Data Discovery { slides } (Strata London, May 2019) ING Data Analytics Platform (Amundsen is mentioned) { slides , recording } (Kubecon Barcelona, May 2019) Disrupting Data Discovery { slides , recording } (Making Big Data Easy SF, May 2019) Disrupting Data Discovery { slides , recording } (Neo4j Graph Tour Santa Monica, September 2019) Disrupting Data Discovery { slides } (IDEAS SoCal AI & Data Science Conference, Oct 2019) Data Discovery with Amundsen by Gerard Toonstra from Coolblue { slides } and { talk } (BigData Vilnius 2019) Towards Enterprise Grade Data Discovery and Data Lineage with Apache Atlas and Amundsen by Verdan Mahmood and Marek Wiewiorka from ING { slides , talk } (Big Data Technology Warsaw Summit 2020) Related Articles \u00b6 How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions Data Discovery in 2020 4 Data Trends to Watch in 2020 Work-Bench Snapshot: The Evolution of Data Discovery & Catalog Future of Data Engineering Governance and Discovery A Data Engineer\u2019s Perspective On Data Democratization Data Discovery in 2020 Graph Technology Landscape 2020 Community meetings \u00b6 Community meetings are held on the first Thursday of every month at 9 AM Pacific, Noon Eastern, 6 PM Central European Time Link to join Upcoming meetings \u00b6 2020/06/04 at 9 AM Pacific. Notes Past meetings \u00b6 2020/05/07 { recording } 2020/01/23 { recording } 2019/11/05 2019/09/05 2019/07/30 All notes here . Who uses Amundsen? \u00b6 Here is the list of organizations that are using Amundsen today. If your organization uses Amundsen, please file a PR and update this list. Currently officially using Amundsen: Bang & Olufsen Cameo Cimpress Technology Data Sprints Devoted Health Edmunds Everfi ING iRobot LMC Lyft Merlin Remitly Square Workday License \u00b6 Apache 2.0 License.","title":"Overview"},{"location":"#documentation","text":"https://lyft.github.io/amundsen/","title":"Documentation"},{"location":"#requirements","text":"Python = 3.6 or 3.7 Node = v10 or v12 (v14 may have compatibility issues) npm >= 6","title":"Requirements"},{"location":"#user-interface","text":"Please note that the mock images only served as demonstration purpose. Landing Page : The landing page for Amundsen including 1. search bars; 2. popular used tables; Search Preview : See inline search results as you type Table Detail Page : Visualization of a Hive / Redshift table Column detail : Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page : Visualization of table data preview which could integrate with Apache Superset or other Data Visualization Tools.","title":"User Interface"},{"location":"#get-involved-in-the-community","text":"Want help or want to help? Use the button in our header to join our slack channel.","title":"Get Involved in the Community"},{"location":"#getting-started","text":"Please visit the Amundsen installation documentation for a quick start to bootstrap a default version of Amundsen with dummy data.","title":"Getting Started"},{"location":"#architecture-overview","text":"Please visit Architecture for Amundsen architecture overview.","title":"Architecture Overview"},{"location":"#supported-entities","text":"Tables (from Databases) People (from HR systems) Dashboards","title":"Supported Entities"},{"location":"#supported-integrations","text":"","title":"Supported Integrations"},{"location":"#table-connectors","text":"Amazon Athena Amazon Glue and anything built over it (like Databricks Delta - which is a work in progress). Amazon Redshift Apache Cassandra Apache Druid Apache Hive CSV Google BigQuery IBM DB2 Microsoft SQL Server MySQL Oracle (through dbapi or sql_alchemy) PostgreSQL Presto Snowflake Amundsen can also connect to any database that provides dbapi or sql_alchemy interface (which most DBs provide).","title":"Table Connectors"},{"location":"#dashboard-connectors","text":"Mode Analytics","title":"Dashboard Connectors"},{"location":"#etl-orchestration-integrations","text":"Apache Airflow","title":"ETL Orchestration Integrations"},{"location":"#bi-viz-tool-integrations","text":"Apache Superset","title":"BI Viz Tool Integrations"},{"location":"#installation","text":"Please visit Installation guideline on how to install Amundsen.","title":"Installation"},{"location":"#roadmap","text":"Please visit Roadmap if you are interested in Amundsen upcoming roadmap items.","title":"Roadmap"},{"location":"#blog-posts-and-interviews","text":"Amundsen - Lyft\u2019s data discovery & metadata engine (April 2019) Software Engineering Daily podcast on Amundsen (April 2019) How Lyft Drives Data Discovery (July 2019) Data Engineering podcast on Solving Data Discovery At Lyft (Aug 2019) Open Sourcing Amundsen: A Data Discovery And Metadata Platform (Oct 2019)","title":"Blog Posts and Interviews"},{"location":"#talks","text":"Disrupting Data Discovery { slides , recording } (Strata SF, March 2019) Amundsen: A Data Discovery Platform from Lyft { slides } (Data Council SF, April 2019) Disrupting Data Discovery { slides } (Strata London, May 2019) ING Data Analytics Platform (Amundsen is mentioned) { slides , recording } (Kubecon Barcelona, May 2019) Disrupting Data Discovery { slides , recording } (Making Big Data Easy SF, May 2019) Disrupting Data Discovery { slides , recording } (Neo4j Graph Tour Santa Monica, September 2019) Disrupting Data Discovery { slides } (IDEAS SoCal AI & Data Science Conference, Oct 2019) Data Discovery with Amundsen by Gerard Toonstra from Coolblue { slides } and { talk } (BigData Vilnius 2019) Towards Enterprise Grade Data Discovery and Data Lineage with Apache Atlas and Amundsen by Verdan Mahmood and Marek Wiewiorka from ING { slides , talk } (Big Data Technology Warsaw Summit 2020)","title":"Talks"},{"location":"#related-articles","text":"How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions Data Discovery in 2020 4 Data Trends to Watch in 2020 Work-Bench Snapshot: The Evolution of Data Discovery & Catalog Future of Data Engineering Governance and Discovery A Data Engineer\u2019s Perspective On Data Democratization Data Discovery in 2020 Graph Technology Landscape 2020","title":"Related Articles"},{"location":"#community-meetings","text":"Community meetings are held on the first Thursday of every month at 9 AM Pacific, Noon Eastern, 6 PM Central European Time Link to join","title":"Community meetings"},{"location":"#upcoming-meetings","text":"2020/06/04 at 9 AM Pacific. Notes","title":"Upcoming meetings"},{"location":"#past-meetings","text":"2020/05/07 { recording } 2020/01/23 { recording } 2019/11/05 2019/09/05 2019/07/30 All notes here .","title":"Past meetings"},{"location":"#who-uses-amundsen","text":"Here is the list of organizations that are using Amundsen today. If your organization uses Amundsen, please file a PR and update this list. Currently officially using Amundsen: Bang & Olufsen Cameo Cimpress Technology Data Sprints Devoted Health Edmunds Everfi ING iRobot LMC Lyft Merlin Remitly Square Workday","title":"Who uses Amundsen?"},{"location":"#license","text":"Apache 2.0 License.","title":"License"},{"location":"architecture/","text":"Architecture \u00b6 The following diagram shows the overall architecture for Amundsen. Frontend \u00b6 The frontend service serves as web UI portal for users interaction. It is Flask-based web app which representation layer is built with React with Redux, Bootstrap, Webpack, and Babel. Search \u00b6 The search service proxy leverages Elasticsearch\u2019s search functionality (or Apache Atlas\u2019s search API, if that\u2019s the backend you picked) and provides a RESTful API to serve search requests from the frontend service. This API is documented and live explorable through OpenAPI aka \u201cSwagger\u201d. Currently only table resources are indexed and searchable. The search index is built with the databuilder elasticsearch publisher . Metadata \u00b6 The metadata service currently uses a Neo4j proxy to interact with Neo4j graph db and serves frontend service\u2019s metadata. The metadata is represented as a graph model: The above diagram shows how metadata is modeled in Amundsen. Databuilder \u00b6 Amundsen provides a data ingestion library for building the metadata. At Lyft, we build the metadata once a day using an Airflow DAG ( examples ). In addition to \u201creal use\u201d the databuilder is also employed as a handy tool to ingest some \u201cpre-cooked\u201d demo data used in the Quickstart guide. This allows you to have a supersmall sample of data to explore so many of the features in Amundsen are lit up without you even having to setup any connections to databases etc. to ingest real data.","title":"Architecture"},{"location":"architecture/#architecture","text":"The following diagram shows the overall architecture for Amundsen.","title":"Architecture"},{"location":"architecture/#frontend","text":"The frontend service serves as web UI portal for users interaction. It is Flask-based web app which representation layer is built with React with Redux, Bootstrap, Webpack, and Babel.","title":"Frontend"},{"location":"architecture/#search","text":"The search service proxy leverages Elasticsearch\u2019s search functionality (or Apache Atlas\u2019s search API, if that\u2019s the backend you picked) and provides a RESTful API to serve search requests from the frontend service. This API is documented and live explorable through OpenAPI aka \u201cSwagger\u201d. Currently only table resources are indexed and searchable. The search index is built with the databuilder elasticsearch publisher .","title":"Search"},{"location":"architecture/#metadata","text":"The metadata service currently uses a Neo4j proxy to interact with Neo4j graph db and serves frontend service\u2019s metadata. The metadata is represented as a graph model: The above diagram shows how metadata is modeled in Amundsen.","title":"Metadata"},{"location":"architecture/#databuilder","text":"Amundsen provides a data ingestion library for building the metadata. At Lyft, we build the metadata once a day using an Airflow DAG ( examples ). In addition to \u201creal use\u201d the databuilder is also employed as a handy tool to ingest some \u201cpre-cooked\u201d demo data used in the Quickstart guide. This allows you to have a supersmall sample of data to explore so many of the features in Amundsen are lit up without you even having to setup any connections to databases etc. to ingest real data.","title":"Databuilder"},{"location":"developer_guide/","text":"Developer Guide \u00b6 This repository uses git submodules to link the code for all of Amundsen\u2019s libraries into a central location. This document offers guidance on how to develop locally with this setup. This workflow leverages docker and docker-compose in a very similar manner to our installation documentation , to spin up instances of all 3 of Amundsen\u2019s services connected with an instances of Neo4j and ElasticSearch which ingest dummy data. Cloning the Repository \u00b6 If cloning the repository for the first time, run the following command to clone the repository and pull the submodules: $ git clone --recursive git@github.com:lyft/amundsen.git If you have already cloned the repository but your submodules are empty, from your cloned amundsen directory run: $ git submodule init $ git submodule update After cloning the repository you can change directories into any of the upstream folders and work in those directories as you normally would. You will have full access to all of the git features, and working in the upstream directories will function the same as if you were working in a cloned version of that repository. Local Development \u00b6 Ensure you have the latest code \u00b6 Beyond running git pull origin master in your local amundsen directory, the submodules for our libraries also have to be manually updated to point to the latest versions of each libraries\u2019 code. When creating a new branch on amundsen to begin local work, ensure your local submodules are pointing to the latest code for each library by running: $ git submodule update --remote Building local changes \u00b6 First, be sure that you have first followed the installation documentation and can spin up a default version of Amundsen without any issues. If you have already completed this step, be sure to have stopped and removed those containers by running: $ docker-compose -f docker-amundsen.yml down Launch the containers needed for local development (the -d option launches in background) : $ docker-compose -f docker-amundsen-local.yml up -d After making local changes rebuild and relaunch modified containers: $ docker-compose -f docker-amundsen-local.yml build \\ && docker-compose -f docker-amundsen-local.yml up -d Optionally, to still tail logs, in a different terminal you can: $ docker-compose -f docker-amundsen-local.yml logs --tail = 3 -f ## - or just tail single container(s): $ docker logs amundsenmetadata --tail 10 -f Local data \u00b6 Local data is persisted under .local/ (at the root of the project), clean up the following directories to reset the databases: # reset elasticsearch rm -rf .local/elasticsearch # reset neo4j rm -rf .local/neo4j Troubleshooting \u00b6 If you have made a change in amundsen/amundsenfrontendlibrary and do not see your changes, this could be due to your browser\u2019s caching behaviors. Either execute a hard refresh (recommended) or clear your browser cache (last resort). Testing Amundsen frontend locally \u00b6 Amundsen has an instruction regarding local frontend launch here Here are some additional changes you might need for windows (OS Win 10): amundsen_application/config.py, set LOCAL_HOST = \u2018127.0.0.1\u2019 amundsen_application/wsgi.py, set host=\u2018127.0.0.1\u2019 (for other microservices also need to change port here because the default is 5000) (using that approach you can run locally another microservices as well if needed) Once you have a running frontend microservice, the rest of Amundsen components can be launched with docker-compose from the root Amundsen project (don\u2019t forget to remove frontend microservice section from docker-amundsen.yml): docker-compose -f docker-amundsen.yml up https://github.com/lyft/amundsen/blob/master/docs/installation.md Developing Dockerbuild file \u00b6 When making edits to Dockerbuild file (docker-amundsen-local.yml) it is good to see what you are getting wrong locally. To do that you build it docker build . And then the output should include a line like so at the step right before it failed: Step 3 /20 : RUN git clone --recursive git://github.com/lyft/amundsenfrontendlibrary.git && cd amundsenfrontendlibrary && git submodule foreach git pull origin master ---> Using cache ---> ec052612747e You can then launch a container from this image like so docker container run -it --name = debug ec052612747e /bin/sh Building and Testing Amundsen Frontend Docker Image (or any other service) \u00b6 Build your image docker build --no-cache . it is recommended that you use \u2013no-cache so you aren\u2019t accidentally using an old version of an image. Determine the hash of your images by running docker images and getting the id of your most recent image Go to your locally cloned amundsen repo and edit the docker compose file \u201cdocker-amundsen.yml\u201d to have the amundsenfrontend image point to the hash of the image that you built amundsenfrontend : #image: amundsendev/amundsen-frontend:1.0.9 #image: 1234.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:2020-01-21 image : 0312d0ac3938 Pushing image to ECR and using in K8s \u00b6 Assumptions: - You have an aws account - You have aws command line set up and ready to go Choose an ECR repository you\u2019d like to push to (or create a new one) https://us-west-2.console.aws.amazon.com/ecr/repositories Click onto repository name and open \u201cView push commands\u201d cheat sheet 2b. Login it would look something like this: aws ecr get-login --no-include-email --region us-west-2 Then execute what is returned by above Follow the instructions (you may need to install first AWS CLI, aws-okta and configure your AWS credentials if you haven\u2019t done it before) Given image name is amundsen-frontend, build, tag and push commands will be the following: Here you can see the tag is YYYY-MM-dd but you should choose whatever you like. docker build -t amundsen-frontend:{YYYY-MM-dd} . docker tag amundsen-frontend:{YYYY-MM-dd} <?>.dkr.ecr.<?>.amazonaws.com/amundsen-frontend:{YYYY-MM-dd} docker push <?>.dkr.ecr.<?>.amazonaws.com/amundsen-frontend:{YYYY-MM-dd} Go to the helm/{env}/amundsen/values.yaml and modify to the image tag that you want to use. When updating amundsen-frontend, make sure to do a hard refresh of amundsen with emptying the cache, otherwise you will see stale version of webpage. Test search service in local using staging or production data \u00b6 To test in local, we need to stand up Elasticsearch, publish index data, and stand up Elastic search Standup Elasticsearch \u00b6 Running Elasticsearch via Docker. To install Docker, go here Example: 1 docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.2.4 (Optional) Standup Kibana \u00b6 1 docker run --link ecstatic_edison:elasticsearch -p 5601:5601 docker.elastic.co/kibana/kibana:6.2.4 *Note that ecstatic_edison is container_id of Elasticsearch container. Update it if it\u2019s different by looking at docker ps Publish Table index through Databuilder \u00b6 Install Databuilder \u00b6 1 2 3 4 5 6 7 cd ~/src/ git clone git@github.com:lyft/amundsendatabuilder.git cd ~/src/amundsendatabuilder virtualenv venv source venv/bin/activate python setup.py install pip install -r requirements.txt Publish Table index \u00b6 First fill this two environment variables: NEO4J_ENDPOINT , CREDENTIALS_NEO4J_PASSWORD 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 $ python import logging import os import uuid from elasticsearch import Elasticsearch from pyhocon import ConfigFactory from databuilder.extractor.neo4j_extractor import Neo4jExtractor from databuilder.extractor.neo4j_search_data_extractor import Neo4jSearchDataExtractor from databuilder.job.job import DefaultJob from databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader from databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher from databuilder.task.task import DefaultTask logging.basicConfig(level=logging.INFO) neo4j_user = 'neo4j' neo4j_password = os.getenv('CREDENTIALS_NEO4J_PASSWORD') neo4j_endpoint = os.getenv('NEO4J_ENDPOINT') elasticsearch_client = Elasticsearch([ {'host': 'localhost'}, ]) data_file_path = '/var/tmp/amundsen/elasticsearch_upload/es_data.json' elasticsearch_new_index = 'table_search_index_{hex_str}'.format(hex_str=uuid.uuid4().hex) logging.info(\"Elasticsearch new index: \" + elasticsearch_new_index) elasticsearch_doc_type = 'table' elasticsearch_index_alias = 'table_search_index' job_config = ConfigFactory.from_dict({ 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.GRAPH_URL_CONFIG_KEY): neo4j_endpoint, 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.MODEL_CLASS_CONFIG_KEY): 'databuilder.models.table_elasticsearch_document.TableESDocument', 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_USER): neo4j_user, 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_PW): neo4j_password, 'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY): data_file_path, 'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY): 'w', 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_PATH_CONFIG_KEY): data_file_path, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_MODE_CONFIG_KEY): 'r', 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY): elasticsearch_client, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY): elasticsearch_new_index, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY): elasticsearch_doc_type, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY): elasticsearch_index_alias, }) job = DefaultJob(conf=job_config, task=DefaultTask(extractor=Neo4jSearchDataExtractor(), loader=FSElasticsearchJSONLoader()), publisher=ElasticsearchPublisher()) if neo4j_password: job.launch() else: raise ValueError('Add environment variable CREDENTIALS_NEO4J_PASSWORD') Standup Search service \u00b6 Follow this instruction Test the search API with this command: 1 curl -vv \"http://localhost:5001/search?query_term=test&page_index=0\"","title":"Overview"},{"location":"developer_guide/#developer-guide","text":"This repository uses git submodules to link the code for all of Amundsen\u2019s libraries into a central location. This document offers guidance on how to develop locally with this setup. This workflow leverages docker and docker-compose in a very similar manner to our installation documentation , to spin up instances of all 3 of Amundsen\u2019s services connected with an instances of Neo4j and ElasticSearch which ingest dummy data.","title":"Developer Guide"},{"location":"developer_guide/#cloning-the-repository","text":"If cloning the repository for the first time, run the following command to clone the repository and pull the submodules: $ git clone --recursive git@github.com:lyft/amundsen.git If you have already cloned the repository but your submodules are empty, from your cloned amundsen directory run: $ git submodule init $ git submodule update After cloning the repository you can change directories into any of the upstream folders and work in those directories as you normally would. You will have full access to all of the git features, and working in the upstream directories will function the same as if you were working in a cloned version of that repository.","title":"Cloning the Repository"},{"location":"developer_guide/#local-development","text":"","title":"Local Development"},{"location":"developer_guide/#ensure-you-have-the-latest-code","text":"Beyond running git pull origin master in your local amundsen directory, the submodules for our libraries also have to be manually updated to point to the latest versions of each libraries\u2019 code. When creating a new branch on amundsen to begin local work, ensure your local submodules are pointing to the latest code for each library by running: $ git submodule update --remote","title":"Ensure you have the latest code"},{"location":"developer_guide/#building-local-changes","text":"First, be sure that you have first followed the installation documentation and can spin up a default version of Amundsen without any issues. If you have already completed this step, be sure to have stopped and removed those containers by running: $ docker-compose -f docker-amundsen.yml down Launch the containers needed for local development (the -d option launches in background) : $ docker-compose -f docker-amundsen-local.yml up -d After making local changes rebuild and relaunch modified containers: $ docker-compose -f docker-amundsen-local.yml build \\ && docker-compose -f docker-amundsen-local.yml up -d Optionally, to still tail logs, in a different terminal you can: $ docker-compose -f docker-amundsen-local.yml logs --tail = 3 -f ## - or just tail single container(s): $ docker logs amundsenmetadata --tail 10 -f","title":"Building local changes"},{"location":"developer_guide/#local-data","text":"Local data is persisted under .local/ (at the root of the project), clean up the following directories to reset the databases: # reset elasticsearch rm -rf .local/elasticsearch # reset neo4j rm -rf .local/neo4j","title":"Local data"},{"location":"developer_guide/#troubleshooting","text":"If you have made a change in amundsen/amundsenfrontendlibrary and do not see your changes, this could be due to your browser\u2019s caching behaviors. Either execute a hard refresh (recommended) or clear your browser cache (last resort).","title":"Troubleshooting"},{"location":"developer_guide/#testing-amundsen-frontend-locally","text":"Amundsen has an instruction regarding local frontend launch here Here are some additional changes you might need for windows (OS Win 10): amundsen_application/config.py, set LOCAL_HOST = \u2018127.0.0.1\u2019 amundsen_application/wsgi.py, set host=\u2018127.0.0.1\u2019 (for other microservices also need to change port here because the default is 5000) (using that approach you can run locally another microservices as well if needed) Once you have a running frontend microservice, the rest of Amundsen components can be launched with docker-compose from the root Amundsen project (don\u2019t forget to remove frontend microservice section from docker-amundsen.yml): docker-compose -f docker-amundsen.yml up https://github.com/lyft/amundsen/blob/master/docs/installation.md","title":"Testing Amundsen frontend locally"},{"location":"developer_guide/#developing-dockerbuild-file","text":"When making edits to Dockerbuild file (docker-amundsen-local.yml) it is good to see what you are getting wrong locally. To do that you build it docker build . And then the output should include a line like so at the step right before it failed: Step 3 /20 : RUN git clone --recursive git://github.com/lyft/amundsenfrontendlibrary.git && cd amundsenfrontendlibrary && git submodule foreach git pull origin master ---> Using cache ---> ec052612747e You can then launch a container from this image like so docker container run -it --name = debug ec052612747e /bin/sh","title":"Developing Dockerbuild file"},{"location":"developer_guide/#building-and-testing-amundsen-frontend-docker-image-or-any-other-service","text":"Build your image docker build --no-cache . it is recommended that you use \u2013no-cache so you aren\u2019t accidentally using an old version of an image. Determine the hash of your images by running docker images and getting the id of your most recent image Go to your locally cloned amundsen repo and edit the docker compose file \u201cdocker-amundsen.yml\u201d to have the amundsenfrontend image point to the hash of the image that you built amundsenfrontend : #image: amundsendev/amundsen-frontend:1.0.9 #image: 1234.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:2020-01-21 image : 0312d0ac3938","title":"Building and Testing Amundsen Frontend Docker Image (or any other service)"},{"location":"developer_guide/#pushing-image-to-ecr-and-using-in-k8s","text":"Assumptions: - You have an aws account - You have aws command line set up and ready to go Choose an ECR repository you\u2019d like to push to (or create a new one) https://us-west-2.console.aws.amazon.com/ecr/repositories Click onto repository name and open \u201cView push commands\u201d cheat sheet 2b. Login it would look something like this: aws ecr get-login --no-include-email --region us-west-2 Then execute what is returned by above Follow the instructions (you may need to install first AWS CLI, aws-okta and configure your AWS credentials if you haven\u2019t done it before) Given image name is amundsen-frontend, build, tag and push commands will be the following: Here you can see the tag is YYYY-MM-dd but you should choose whatever you like. docker build -t amundsen-frontend:{YYYY-MM-dd} . docker tag amundsen-frontend:{YYYY-MM-dd} <?>.dkr.ecr.<?>.amazonaws.com/amundsen-frontend:{YYYY-MM-dd} docker push <?>.dkr.ecr.<?>.amazonaws.com/amundsen-frontend:{YYYY-MM-dd} Go to the helm/{env}/amundsen/values.yaml and modify to the image tag that you want to use. When updating amundsen-frontend, make sure to do a hard refresh of amundsen with emptying the cache, otherwise you will see stale version of webpage.","title":"Pushing image to ECR and using in K8s"},{"location":"developer_guide/#test-search-service-in-local-using-staging-or-production-data","text":"To test in local, we need to stand up Elasticsearch, publish index data, and stand up Elastic search","title":"Test search service in local using staging or production data"},{"location":"developer_guide/#standup-elasticsearch","text":"Running Elasticsearch via Docker. To install Docker, go here Example: 1 docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.2.4","title":"Standup Elasticsearch"},{"location":"developer_guide/#optional-standup-kibana","text":"1 docker run --link ecstatic_edison:elasticsearch -p 5601:5601 docker.elastic.co/kibana/kibana:6.2.4 *Note that ecstatic_edison is container_id of Elasticsearch container. Update it if it\u2019s different by looking at docker ps","title":"(Optional) Standup Kibana"},{"location":"developer_guide/#publish-table-index-through-databuilder","text":"","title":"Publish Table index through Databuilder"},{"location":"developer_guide/#install-databuilder","text":"1 2 3 4 5 6 7 cd ~/src/ git clone git@github.com:lyft/amundsendatabuilder.git cd ~/src/amundsendatabuilder virtualenv venv source venv/bin/activate python setup.py install pip install -r requirements.txt","title":"Install Databuilder"},{"location":"developer_guide/#publish-table-index","text":"First fill this two environment variables: NEO4J_ENDPOINT , CREDENTIALS_NEO4J_PASSWORD 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 $ python import logging import os import uuid from elasticsearch import Elasticsearch from pyhocon import ConfigFactory from databuilder.extractor.neo4j_extractor import Neo4jExtractor from databuilder.extractor.neo4j_search_data_extractor import Neo4jSearchDataExtractor from databuilder.job.job import DefaultJob from databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader from databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher from databuilder.task.task import DefaultTask logging.basicConfig(level=logging.INFO) neo4j_user = 'neo4j' neo4j_password = os.getenv('CREDENTIALS_NEO4J_PASSWORD') neo4j_endpoint = os.getenv('NEO4J_ENDPOINT') elasticsearch_client = Elasticsearch([ {'host': 'localhost'}, ]) data_file_path = '/var/tmp/amundsen/elasticsearch_upload/es_data.json' elasticsearch_new_index = 'table_search_index_{hex_str}'.format(hex_str=uuid.uuid4().hex) logging.info(\"Elasticsearch new index: \" + elasticsearch_new_index) elasticsearch_doc_type = 'table' elasticsearch_index_alias = 'table_search_index' job_config = ConfigFactory.from_dict({ 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.GRAPH_URL_CONFIG_KEY): neo4j_endpoint, 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.MODEL_CLASS_CONFIG_KEY): 'databuilder.models.table_elasticsearch_document.TableESDocument', 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_USER): neo4j_user, 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_PW): neo4j_password, 'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY): data_file_path, 'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY): 'w', 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_PATH_CONFIG_KEY): data_file_path, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_MODE_CONFIG_KEY): 'r', 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY): elasticsearch_client, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY): elasticsearch_new_index, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY): elasticsearch_doc_type, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY): elasticsearch_index_alias, }) job = DefaultJob(conf=job_config, task=DefaultTask(extractor=Neo4jSearchDataExtractor(), loader=FSElasticsearchJSONLoader()), publisher=ElasticsearchPublisher()) if neo4j_password: job.launch() else: raise ValueError('Add environment variable CREDENTIALS_NEO4J_PASSWORD')","title":"Publish Table index"},{"location":"developer_guide/#standup-search-service","text":"Follow this instruction Test the search API with this command: 1 curl -vv \"http://localhost:5001/search?query_term=test&page_index=0\"","title":"Standup Search service"},{"location":"faq/","text":"FAQ \u00b6 How to select between Neo4j and Atlas as backend for Amundsen? \u00b6 Why Neo4j? \u00b6 Amundsen has direct influence over the data model if you use neo4j. This, at least initially, will benefit the speed by which new features in amundsen can arrive Atlas is developed with data governance in mind and not with data discovery. You could view \u201cslapping amundsen on top of Atlas\u201d as a kind of Frankenstein: never able to properly able to cater to your audience Atlas seems to have a slow development cycle and it\u2019s community is not very responsive although some small improvements have been made Atlas has the \u201cHadoop\u201d era \u201csmell\u201d which isn\u2019t considered very sexy nowadays Neo4j for it is the market leader in Graph database and also was proven by Airbnb\u2019s Data portal on their Data discovery tool. Why Atlas? \u00b6 Atlas has lineage support already available. It\u2019s been tried and tested. Tag propagation is supported It has a robust authentication and authorization system Atlas does data governance adding amundsen for discovery makes it best of both worlds It has support for push based due to its many plugins The free version of Neo4j does not have authorization support(Enterprise version does). Your question should actually be why use \u201cneo4j over janusgraph\u201d cause that is the right level of comparison. Atlas adds a whole bunch on top of the graph database. What are the prerequisites to use Apache Atlas as backend for Amundsen? \u00b6 To run Amundsen with Atlas, latest versions of following components should be used: 1. Apache Atlas - built from master branch. Ref 103e867cc126ddb84e64bf262791a01a55bee6e5 (or higher). 2. amundsenatlastypes - library for installing Atlas entity definitions specific to Amundsen integration. Version 1.1.0 (or higher). How to migrate from Amundsen 1.x -> 2.x? \u00b6 v2.0 renames a handful of fields in the services to be more consistent. Unfortunately one side effect is that the 2.0 versions of the services will need to be deployed simultaneously, as they are not interoperable with the 1.x versions. Additionally, some indexed field names in the elasticsearch document change as well, so if you\u2019re using elasticsearch, you\u2019ll need to republish Elasticsearch index via Databuilder job. The data in the metadata store, however, can be preserved when migrating from 1.x to 2.0. v2.0 deployments consists of deployment of all three services along with republishing Elasticsearch document on Table with v2.0 Databuilder. Keep in mind there is likely to be some downtime as v2.0.0, between deploying 3 services and re-seeding the elasticsearch indexes, so it might be ideal to stage a rollout by datacenter/environment if uptime is key","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-to-select-between-neo4j-and-atlas-as-backend-for-amundsen","text":"","title":"How to select between Neo4j and Atlas as backend for Amundsen?"},{"location":"faq/#why-neo4j","text":"Amundsen has direct influence over the data model if you use neo4j. This, at least initially, will benefit the speed by which new features in amundsen can arrive Atlas is developed with data governance in mind and not with data discovery. You could view \u201cslapping amundsen on top of Atlas\u201d as a kind of Frankenstein: never able to properly able to cater to your audience Atlas seems to have a slow development cycle and it\u2019s community is not very responsive although some small improvements have been made Atlas has the \u201cHadoop\u201d era \u201csmell\u201d which isn\u2019t considered very sexy nowadays Neo4j for it is the market leader in Graph database and also was proven by Airbnb\u2019s Data portal on their Data discovery tool.","title":"Why Neo4j?"},{"location":"faq/#why-atlas","text":"Atlas has lineage support already available. It\u2019s been tried and tested. Tag propagation is supported It has a robust authentication and authorization system Atlas does data governance adding amundsen for discovery makes it best of both worlds It has support for push based due to its many plugins The free version of Neo4j does not have authorization support(Enterprise version does). Your question should actually be why use \u201cneo4j over janusgraph\u201d cause that is the right level of comparison. Atlas adds a whole bunch on top of the graph database.","title":"Why Atlas?"},{"location":"faq/#what-are-the-prerequisites-to-use-apache-atlas-as-backend-for-amundsen","text":"To run Amundsen with Atlas, latest versions of following components should be used: 1. Apache Atlas - built from master branch. Ref 103e867cc126ddb84e64bf262791a01a55bee6e5 (or higher). 2. amundsenatlastypes - library for installing Atlas entity definitions specific to Amundsen integration. Version 1.1.0 (or higher).","title":"What are the prerequisites to use Apache Atlas as backend for Amundsen?"},{"location":"faq/#how-to-migrate-from-amundsen-1x-2x","text":"v2.0 renames a handful of fields in the services to be more consistent. Unfortunately one side effect is that the 2.0 versions of the services will need to be deployed simultaneously, as they are not interoperable with the 1.x versions. Additionally, some indexed field names in the elasticsearch document change as well, so if you\u2019re using elasticsearch, you\u2019ll need to republish Elasticsearch index via Databuilder job. The data in the metadata store, however, can be preserved when migrating from 1.x to 2.0. v2.0 deployments consists of deployment of all three services along with republishing Elasticsearch document on Table with v2.0 Databuilder. Keep in mind there is likely to be some downtime as v2.0.0, between deploying 3 services and re-seeding the elasticsearch indexes, so it might be ideal to stage a rollout by datacenter/environment if uptime is key","title":"How to migrate from Amundsen 1.x -&gt; 2.x?"},{"location":"installation/","text":"Installation \u00b6 Bootstrap a default version of Amundsen using Docker \u00b6 The following instructions are for setting up a version of Amundsen using Docker. Make sure you have at least 3GB available to docker. Install docker and docker-compose . Clone this repo and its submodules by running: $ git clone --recursive git@github.com:lyft/amundsen.git Enter the cloned directory and run: # For Neo4j Backend $ docker-compose -f docker-amundsen.yml up # For Atlas $ docker-compose -f docker-amundsen-atlas.yml up Ingest dummy data into Neo4j by doing the following: (Please skip if you are using Atlas backend) Change directory to the amundsendatabuilder submodule. Run the following commands in the amundsendatabuilder upstream directory: $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 example/scripts/sample_data_loader.py View UI at http://localhost:5000 and try to search test , it should return some result. We could also do an exact matched search for table entity. For example: search test_table1 in table field and it return the records that matched. Atlas Note: Atlas takes some time to boot properly. So you may not be able to see the results immediately after docker-compose up command. Atlas would be ready once you\u2019ll have the following output in the docker output Amundsen Entity Definitions Created... Verify setup \u00b6 You can verify dummy data has been ingested into Neo4j by by visiting http://localhost:7474/browser/ and run MATCH (n:Table) RETURN n LIMIT 25 in the query box. You should see two tables: hive.test_schema.test_table1 hive.test_schema.test_table2 You can verify the data has been loaded into the metadataservice by visiting: http://localhost:5000/table_detail/gold/hive/test_schema/test_table1 http://localhost:5000/table_detail/gold/dynamo/test_schema/test_table2 Troubleshooting \u00b6 If the docker container doesn\u2019t have enough heap memory for Elastic Search, es_amundsen will fail during docker-compose . docker-compose error: es_amundsen | [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] Increase the heap memory detailed instructions here Edit /etc/sysctl.conf Make entry vm.max_map_count=262144 . Save and exit. Reload settings $ sysctl -p Restart docker-compose If docker-amundsen-local.yml stops because of org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: Failed to create node environment , then es_amundsen cannot write to .local/elasticsearch . chown -R 1000:1000 .local/elasticsearch Restart docker-compose If when running the sample data loader you recieve a connection error related to ElasticSearch or like this for Neo4j: Traceback (most recent call last): File \"/home/ubuntu/amundsen/amundsendatabuilder/venv/lib/python3.6/site-packages/neobolt/direct.py\", line 831, in _connect s.connect(resolved_address) ConnectionRefusedError: [Errno 111] Connection refused If elastic search container stops with an error max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535] , then add the below code to the file docker-amundsen-local.yml in the elasticsearch definition. ulimits: nofile: soft: 65535 hard: 65535 Then check if all 5 Amundsen related containers are running with docker ps ? Can you connect to the Neo4j UI at http://localhost:7474/browser/ and similarly the raw ES API at http://localhost:9200 ? Does Docker logs reveal any serious issues?","title":"Quick Start"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#bootstrap-a-default-version-of-amundsen-using-docker","text":"The following instructions are for setting up a version of Amundsen using Docker. Make sure you have at least 3GB available to docker. Install docker and docker-compose . Clone this repo and its submodules by running: $ git clone --recursive git@github.com:lyft/amundsen.git Enter the cloned directory and run: # For Neo4j Backend $ docker-compose -f docker-amundsen.yml up # For Atlas $ docker-compose -f docker-amundsen-atlas.yml up Ingest dummy data into Neo4j by doing the following: (Please skip if you are using Atlas backend) Change directory to the amundsendatabuilder submodule. Run the following commands in the amundsendatabuilder upstream directory: $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 example/scripts/sample_data_loader.py View UI at http://localhost:5000 and try to search test , it should return some result. We could also do an exact matched search for table entity. For example: search test_table1 in table field and it return the records that matched. Atlas Note: Atlas takes some time to boot properly. So you may not be able to see the results immediately after docker-compose up command. Atlas would be ready once you\u2019ll have the following output in the docker output Amundsen Entity Definitions Created...","title":"Bootstrap a default version of Amundsen using Docker"},{"location":"installation/#verify-setup","text":"You can verify dummy data has been ingested into Neo4j by by visiting http://localhost:7474/browser/ and run MATCH (n:Table) RETURN n LIMIT 25 in the query box. You should see two tables: hive.test_schema.test_table1 hive.test_schema.test_table2 You can verify the data has been loaded into the metadataservice by visiting: http://localhost:5000/table_detail/gold/hive/test_schema/test_table1 http://localhost:5000/table_detail/gold/dynamo/test_schema/test_table2","title":"Verify setup"},{"location":"installation/#troubleshooting","text":"If the docker container doesn\u2019t have enough heap memory for Elastic Search, es_amundsen will fail during docker-compose . docker-compose error: es_amundsen | [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] Increase the heap memory detailed instructions here Edit /etc/sysctl.conf Make entry vm.max_map_count=262144 . Save and exit. Reload settings $ sysctl -p Restart docker-compose If docker-amundsen-local.yml stops because of org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: Failed to create node environment , then es_amundsen cannot write to .local/elasticsearch . chown -R 1000:1000 .local/elasticsearch Restart docker-compose If when running the sample data loader you recieve a connection error related to ElasticSearch or like this for Neo4j: Traceback (most recent call last): File \"/home/ubuntu/amundsen/amundsendatabuilder/venv/lib/python3.6/site-packages/neobolt/direct.py\", line 831, in _connect s.connect(resolved_address) ConnectionRefusedError: [Errno 111] Connection refused If elastic search container stops with an error max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535] , then add the below code to the file docker-amundsen-local.yml in the elasticsearch definition. ulimits: nofile: soft: 65535 hard: 65535 Then check if all 5 Amundsen related containers are running with docker ps ? Can you connect to the Neo4j UI at http://localhost:7474/browser/ and similarly the raw ES API at http://localhost:9200 ? Does Docker logs reveal any serious issues?","title":"Troubleshooting"},{"location":"k8s_install/","text":"Amundsen K8s Helm Charts \u00b6 Source code can be found here What is this? \u00b6 This is setup templates for deploying amundsen on k8s (kubernetes) , using helm. How do I get started? \u00b6 Make sure you have the following command line clients setup: k8s (kubectl) helm Build out a cloud based k8s cluster, such as Amazon EKS Ensure you can connect to your cluster with cli tools in step 1. Prerequisites \u00b6 Helm 2.14+ Kubernetes 1.14+ Chart Requirements \u00b6 Repository Name Version https://kubernetes-charts.storage.googleapis.com/ elasticsearch 1.24.0 Chart Values \u00b6 The following table lists the configurable parameters of the Amundsen charts and their default values. Key Type Default Description LONG_RANDOM_STRING int 1234 A long random string. You should probably provide your own. This is needed for OIDC. affinity object {} amundsen application wide configuration of affinity. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref dnsZone string \"teamname.company.com\" DEPRECATED - its not standard to pre construct urls this way. The dns zone (e.g. group-qa.myaccount.company.com) the app is running in. Used to construct dns hostnames (on aws only). dockerhubImagePath string \"amundsendev\" DEPRECATED - this is not useful, it would be better to just allow the whole image to be swapped instead. The image path for dockerhub. elasticsearch.client.replicas int 1 only running amundsen on 1 client replica elasticsearch.cluster.env.EXPECTED_MASTER_NODES int 1 required to match master.replicas elasticsearch.cluster.env.MINIMUM_MASTER_NODES int 1 required to match master.replicas elasticsearch.cluster.env.RECOVER_AFTER_MASTER_NODES int 1 required to match master.replicas elasticsearch.data.replicas int 1 only running amundsen on 1 data replica elasticsearch.enabled bool true set this to false, if you want to provide your own ES instance. elasticsearch.master.replicas int 1 only running amundsen on 1 master replica environment string \"dev\" DEPRECATED - its not standard to pre construct urls this way. The environment the app is running in. Used to construct dns hostnames (on aws only) and ports. frontEnd.OIDC_AUTH_SERVER_ID string nil The authorization server id for OIDC. frontEnd.OIDC_CLIENT_ID string nil The client id for OIDC. frontEnd.OIDC_CLIENT_SECRET string \"\" The client secret for OIDC. frontEnd.OIDC_ORG_URL string nil The organization URL for OIDC. frontEnd.affinity object {} Frontend pod specific affinity. frontEnd.annotations object {} Frontend service specific tolerations. frontEnd.baseUrl string \"http://localhost\" used by notifications util to provide links to amundsen pages in emails. frontEnd.createOidcSecret bool false OIDC needs some configuration. If you want the chart to make your secrets, set this to true and set the next four values. If you don\u2019t want to configure your secrets via helm, you can still use the amundsen-oidc-config.yaml as a template frontEnd.image string \"amundsendev/amundsen-frontend\" The image of the frontend container. frontEnd.imageTag string \"2.0.0\" The image tag of the frontend container. frontEnd.nodeSelector object {} Frontend pod specific nodeSelector. frontEnd.oidcEnabled bool false To enable auth via OIDC, set this to true. frontEnd.podAnnotations object {} Frontend pod specific annotations. frontEnd.replicas int 1 How many replicas of the frontend service to run. frontEnd.resources object {} See pod resourcing ref frontEnd.serviceName string \"frontend\" The frontend service name. frontEnd.servicePort int 80 The port the frontend service will be exposed on via the loadbalancer. frontEnd.serviceType string \"ClusterIP\" The frontend service type. See service types ref frontEnd.tolerations list [] Frontend pod specific tolerations. metadata.affinity object {} Metadata pod specific affinity. metadata.annotations object {} Metadata service specific tolerations. metadata.image string \"amundsendev/amundsen-metadata\" The image of the metadata container. metadata.imageTag string \"2.0.0\" The image tag of the metadata container. metadata.neo4jEndpoint string nil The name of the service hosting neo4j on your cluster, if you bring your own. You should only need to change this, if you don\u2019t use the version in this chart. metadata.nodeSelector object {} Metadata pod specific nodeSelector. metadata.podAnnotations object {} Metadata pod specific annotations. metadata.replicas int 1 How many replicas of the metadata service to run. metadata.resources object {} See pod resourcing ref metadata.serviceName string \"metadata\" The metadata service name. metadata.serviceType string \"ClusterIP\" The metadata service type. See service types ref metadata.tolerations list [] Metadata pod specific tolerations. neo4j.affinity object {} neo4j specific affinity. neo4j.annotations object {} neo4j service specific tolerations. neo4j.backup object {\"enabled\":false,\"s3Path\":\"s3://dev/null\",\"schedule\":\"0 * * * *\"} If enabled is set to true, make sure and set the s3 path as well. neo4j.backup.s3Path string \"s3://dev/null\" The s3path to write to for backups. neo4j.backup.schedule string \"0 * * * *\" The schedule to run backups on. Defaults to hourly. neo4j.config object {\"dbms\":{\"heap_initial_size\":\"23000m\",\"heap_max_size\":\"23000m\",\"pagecache_size\":\"26600m\"}} Neo4j application specific configuration. This type of configuration is why the charts/stable version is not used. See ref neo4j.config.dbms object {\"heap_initial_size\":\"23000m\",\"heap_max_size\":\"23000m\",\"pagecache_size\":\"26600m\"} dbms config for neo4j neo4j.config.dbms.heap_initial_size string \"23000m\" the initial java heap for neo4j neo4j.config.dbms.heap_max_size string \"23000m\" the max java heap for neo4j neo4j.config.dbms.pagecache_size string \"26600m\" the page cache size for neo4j neo4j.enabled bool true If neo4j is enabled as part of this chart, or not. Set this to false if you want to provide your own version. neo4j.nodeSelector object {} neo4j specific nodeSelector. neo4j.persistence object {} Neo4j persistence. Turn this on to keep your data between pod crashes, etc. This is also needed for backups. neo4j.podAnnotations object {} neo4j pod specific annotations. neo4j.resources object {} See pod resourcing ref neo4j.tolerations list [] neo4j specific tolerations. neo4j.version string \"3.3.0\" The neo4j application version used by amundsen. nodeSelector object {} amundsen application wide configuration of nodeSelector. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref podAnnotations object {} amundsen application wide configuration of podAnnotations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref provider string \"aws\" The cloud provider the app is running in. Used to construct dns hostnames (on aws only). search.affinity object {} Search pod specific affinity. search.annotations object {} Search service specific tolerations. search.elasticsearchEndpoint string nil The name of the service hosting elasticsearch on your cluster, if you bring your own. You should only need to change this, if you don\u2019t use the version in this chart. search.image string \"amundsendev/amundsen-search\" The image of the search container. search.imageTag string \"2.0.0\" The image tag of the search container. search.nodeSelector object {} Search pod specific nodeSelector. search.podAnnotations object {} Search pod specific annotations. search.replicas int 1 How many replicas of the search service to run. search.resources object {} See pod resourcing ref search.serviceName string \"search\" The search service name. search.serviceType string \"ClusterIP\" The search service type. See service types ref search.tolerations list [] Search pod specific tolerations. tolerations list [] amundsen application wide configuration of tolerations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref Neo4j DBMS Config? \u00b6 You may want to override the default memory usage for Neo4J. In particular, if you\u2019re just test-driving a deployment and your node exits with status 137, you should set the usage to smaller values: config: dbms: heap_initial_size: 1G heap_max_size: 2G pagecache_size: 2G With this values file, you can then install Amundsen using Helm 2 with: helm install ./templates/helm --values impl/helm/dev/values.yaml For Helm 3 it\u2019s now mandatory to specify a chart reference name e.g. my-amundsen : helm install my-amundsen ./templates/helm --values impl/helm/dev/values.yaml Other Notes \u00b6 For aws setup, you will also need to setup the external-dns plugin There is an existing helm chart for neo4j, but, it is missing some features necessary to for use such as: [stable/neo4j] make neo4j service definition more extensible ; without this, it is not possible to setup external load balancers, external-dns, etc [stable/neo4j] allow custom configuration of neo4j ; without this, custom configuration is not possible which includes setting configmap based settings, which also includes turning on apoc.","title":"K8S Installation"},{"location":"k8s_install/#amundsen-k8s-helm-charts","text":"Source code can be found here","title":"Amundsen K8s Helm Charts"},{"location":"k8s_install/#what-is-this","text":"This is setup templates for deploying amundsen on k8s (kubernetes) , using helm.","title":"What is this?"},{"location":"k8s_install/#how-do-i-get-started","text":"Make sure you have the following command line clients setup: k8s (kubectl) helm Build out a cloud based k8s cluster, such as Amazon EKS Ensure you can connect to your cluster with cli tools in step 1.","title":"How do I get started?"},{"location":"k8s_install/#prerequisites","text":"Helm 2.14+ Kubernetes 1.14+","title":"Prerequisites"},{"location":"k8s_install/#chart-requirements","text":"Repository Name Version https://kubernetes-charts.storage.googleapis.com/ elasticsearch 1.24.0","title":"Chart Requirements"},{"location":"k8s_install/#chart-values","text":"The following table lists the configurable parameters of the Amundsen charts and their default values. Key Type Default Description LONG_RANDOM_STRING int 1234 A long random string. You should probably provide your own. This is needed for OIDC. affinity object {} amundsen application wide configuration of affinity. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref dnsZone string \"teamname.company.com\" DEPRECATED - its not standard to pre construct urls this way. The dns zone (e.g. group-qa.myaccount.company.com) the app is running in. Used to construct dns hostnames (on aws only). dockerhubImagePath string \"amundsendev\" DEPRECATED - this is not useful, it would be better to just allow the whole image to be swapped instead. The image path for dockerhub. elasticsearch.client.replicas int 1 only running amundsen on 1 client replica elasticsearch.cluster.env.EXPECTED_MASTER_NODES int 1 required to match master.replicas elasticsearch.cluster.env.MINIMUM_MASTER_NODES int 1 required to match master.replicas elasticsearch.cluster.env.RECOVER_AFTER_MASTER_NODES int 1 required to match master.replicas elasticsearch.data.replicas int 1 only running amundsen on 1 data replica elasticsearch.enabled bool true set this to false, if you want to provide your own ES instance. elasticsearch.master.replicas int 1 only running amundsen on 1 master replica environment string \"dev\" DEPRECATED - its not standard to pre construct urls this way. The environment the app is running in. Used to construct dns hostnames (on aws only) and ports. frontEnd.OIDC_AUTH_SERVER_ID string nil The authorization server id for OIDC. frontEnd.OIDC_CLIENT_ID string nil The client id for OIDC. frontEnd.OIDC_CLIENT_SECRET string \"\" The client secret for OIDC. frontEnd.OIDC_ORG_URL string nil The organization URL for OIDC. frontEnd.affinity object {} Frontend pod specific affinity. frontEnd.annotations object {} Frontend service specific tolerations. frontEnd.baseUrl string \"http://localhost\" used by notifications util to provide links to amundsen pages in emails. frontEnd.createOidcSecret bool false OIDC needs some configuration. If you want the chart to make your secrets, set this to true and set the next four values. If you don\u2019t want to configure your secrets via helm, you can still use the amundsen-oidc-config.yaml as a template frontEnd.image string \"amundsendev/amundsen-frontend\" The image of the frontend container. frontEnd.imageTag string \"2.0.0\" The image tag of the frontend container. frontEnd.nodeSelector object {} Frontend pod specific nodeSelector. frontEnd.oidcEnabled bool false To enable auth via OIDC, set this to true. frontEnd.podAnnotations object {} Frontend pod specific annotations. frontEnd.replicas int 1 How many replicas of the frontend service to run. frontEnd.resources object {} See pod resourcing ref frontEnd.serviceName string \"frontend\" The frontend service name. frontEnd.servicePort int 80 The port the frontend service will be exposed on via the loadbalancer. frontEnd.serviceType string \"ClusterIP\" The frontend service type. See service types ref frontEnd.tolerations list [] Frontend pod specific tolerations. metadata.affinity object {} Metadata pod specific affinity. metadata.annotations object {} Metadata service specific tolerations. metadata.image string \"amundsendev/amundsen-metadata\" The image of the metadata container. metadata.imageTag string \"2.0.0\" The image tag of the metadata container. metadata.neo4jEndpoint string nil The name of the service hosting neo4j on your cluster, if you bring your own. You should only need to change this, if you don\u2019t use the version in this chart. metadata.nodeSelector object {} Metadata pod specific nodeSelector. metadata.podAnnotations object {} Metadata pod specific annotations. metadata.replicas int 1 How many replicas of the metadata service to run. metadata.resources object {} See pod resourcing ref metadata.serviceName string \"metadata\" The metadata service name. metadata.serviceType string \"ClusterIP\" The metadata service type. See service types ref metadata.tolerations list [] Metadata pod specific tolerations. neo4j.affinity object {} neo4j specific affinity. neo4j.annotations object {} neo4j service specific tolerations. neo4j.backup object {\"enabled\":false,\"s3Path\":\"s3://dev/null\",\"schedule\":\"0 * * * *\"} If enabled is set to true, make sure and set the s3 path as well. neo4j.backup.s3Path string \"s3://dev/null\" The s3path to write to for backups. neo4j.backup.schedule string \"0 * * * *\" The schedule to run backups on. Defaults to hourly. neo4j.config object {\"dbms\":{\"heap_initial_size\":\"23000m\",\"heap_max_size\":\"23000m\",\"pagecache_size\":\"26600m\"}} Neo4j application specific configuration. This type of configuration is why the charts/stable version is not used. See ref neo4j.config.dbms object {\"heap_initial_size\":\"23000m\",\"heap_max_size\":\"23000m\",\"pagecache_size\":\"26600m\"} dbms config for neo4j neo4j.config.dbms.heap_initial_size string \"23000m\" the initial java heap for neo4j neo4j.config.dbms.heap_max_size string \"23000m\" the max java heap for neo4j neo4j.config.dbms.pagecache_size string \"26600m\" the page cache size for neo4j neo4j.enabled bool true If neo4j is enabled as part of this chart, or not. Set this to false if you want to provide your own version. neo4j.nodeSelector object {} neo4j specific nodeSelector. neo4j.persistence object {} Neo4j persistence. Turn this on to keep your data between pod crashes, etc. This is also needed for backups. neo4j.podAnnotations object {} neo4j pod specific annotations. neo4j.resources object {} See pod resourcing ref neo4j.tolerations list [] neo4j specific tolerations. neo4j.version string \"3.3.0\" The neo4j application version used by amundsen. nodeSelector object {} amundsen application wide configuration of nodeSelector. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref podAnnotations object {} amundsen application wide configuration of podAnnotations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref provider string \"aws\" The cloud provider the app is running in. Used to construct dns hostnames (on aws only). search.affinity object {} Search pod specific affinity. search.annotations object {} Search service specific tolerations. search.elasticsearchEndpoint string nil The name of the service hosting elasticsearch on your cluster, if you bring your own. You should only need to change this, if you don\u2019t use the version in this chart. search.image string \"amundsendev/amundsen-search\" The image of the search container. search.imageTag string \"2.0.0\" The image tag of the search container. search.nodeSelector object {} Search pod specific nodeSelector. search.podAnnotations object {} Search pod specific annotations. search.replicas int 1 How many replicas of the search service to run. search.resources object {} See pod resourcing ref search.serviceName string \"search\" The search service name. search.serviceType string \"ClusterIP\" The search service type. See service types ref search.tolerations list [] Search pod specific tolerations. tolerations list [] amundsen application wide configuration of tolerations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref","title":"Chart Values"},{"location":"k8s_install/#neo4j-dbms-config","text":"You may want to override the default memory usage for Neo4J. In particular, if you\u2019re just test-driving a deployment and your node exits with status 137, you should set the usage to smaller values: config: dbms: heap_initial_size: 1G heap_max_size: 2G pagecache_size: 2G With this values file, you can then install Amundsen using Helm 2 with: helm install ./templates/helm --values impl/helm/dev/values.yaml For Helm 3 it\u2019s now mandatory to specify a chart reference name e.g. my-amundsen : helm install my-amundsen ./templates/helm --values impl/helm/dev/values.yaml","title":"Neo4j DBMS Config?"},{"location":"k8s_install/#other-notes","text":"For aws setup, you will also need to setup the external-dns plugin There is an existing helm chart for neo4j, but, it is missing some features necessary to for use such as: [stable/neo4j] make neo4j service definition more extensible ; without this, it is not possible to setup external load balancers, external-dns, etc [stable/neo4j] allow custom configuration of neo4j ; without this, custom configuration is not possible which includes setting configmap based settings, which also includes turning on apoc.","title":"Other Notes"},{"location":"roadmap/","text":"Amundsen Roadmap \u00b6 Mission : To organize all information about data and make it universally actionable Vision (2020) : Centralize a comprehensive and actionable map of all our data resources that can be leveraged to solve a growing number of use cases and workflows The following roadmap gives an overview of what we are currently working on and what we want to tackle next. We share it so that the community can plan work together. Let us know in the Slack channel if you are interested in taking a stab at leading the development of one of these features (or of a non listed one!). Current focus \u00b6 Index Dashboards \u00b6 What : We want to help with the discovery of existing analysis work, dashboards. This is going to help avoid reinventing the wheel, create value for less technical users and help give context on how tables are used. Status : In Progress Links : Designs | Product Specifications | Technical RFC Next steps \u00b6 Native lineage integration \u00b6 What : We want to create a native lineage integration in Amundsen, to better surface how data assets interact with each other Status : implementation has not started Landing page \u00b6 What : We are creating a proper landing page to provide more value, with an emphasis on helping users finding data when then don\u2019t really know what to search for (exploration) Status : being spec\u2019d out Push ingest API \u00b6 What : We want to create a push API so that it is as easy as possible for a new data resource type to be ingested Status : implementation has started (around 80% complete) GET Rest API \u00b6 What : enable users to access our data map programmatically through a Rest API Status : implementation has started Index Druid tables and S3 buckets \u00b6 What : add these new resource types to our data map and create resource pages for them Status : implementation has not started Granular Access Control \u00b6 What : we want to have a more granular control of the access. For example, only certain types of people would be able to see certain types of metadata/functionality Status : implementation has not started Show distinct column values \u00b6 What : When a column has a limited set of possible values, we want to make then easily discoverable Status : implementation has not started \u201cOrder by\u201d for columns \u00b6 What : we want to help users make sense of what are the columns people use in the tables we index. Within a frequently used table, a column might not be used anymore because it is know to be deprecated Status : implementation has not started Index online datastores \u00b6 What : We want to make our DynamoDB and other online datastores discoverable by indexing them. For this purpose, we will probably leverage the fact that we have a centralized IDL (interface definition language) Status : implementation has not started Integration with BI Tools \u00b6 What : get the richness of Amundsen\u2019s metadata to where the data is used: in Bi tools such as Mode, Superset and Tableau Status : implementation has not started Index Processes \u00b6 What : we want to index ETLs and pipelines from our Machine Learning Engine Status : implementation has not started Versioning system \u00b6 What : We want to create a versioning system for our indexed resources, to be able to index different versions of the same resource. This is especially required for machine learning purposes. Status : implementation has not started Index Teams \u00b6 What : We want to add teams pages to enable users to see what are the important tables and dashboard a team uses Status : implementation has not started Index Services \u00b6 What : With our microservices architecture, we want to index services and show how these services interact with data artifacts Status : implementation has not started Index Pub/Sub systems \u00b6 What : We want to make our pub/sub systems discoverable Status : implementation has not started","title":"Roadmap"},{"location":"roadmap/#amundsen-roadmap","text":"Mission : To organize all information about data and make it universally actionable Vision (2020) : Centralize a comprehensive and actionable map of all our data resources that can be leveraged to solve a growing number of use cases and workflows The following roadmap gives an overview of what we are currently working on and what we want to tackle next. We share it so that the community can plan work together. Let us know in the Slack channel if you are interested in taking a stab at leading the development of one of these features (or of a non listed one!).","title":"Amundsen Roadmap"},{"location":"roadmap/#current-focus","text":"","title":"Current focus"},{"location":"roadmap/#index-dashboards","text":"What : We want to help with the discovery of existing analysis work, dashboards. This is going to help avoid reinventing the wheel, create value for less technical users and help give context on how tables are used. Status : In Progress Links : Designs | Product Specifications | Technical RFC","title":"Index Dashboards"},{"location":"roadmap/#next-steps","text":"","title":"Next steps"},{"location":"roadmap/#native-lineage-integration","text":"What : We want to create a native lineage integration in Amundsen, to better surface how data assets interact with each other Status : implementation has not started","title":"Native lineage integration"},{"location":"roadmap/#landing-page","text":"What : We are creating a proper landing page to provide more value, with an emphasis on helping users finding data when then don\u2019t really know what to search for (exploration) Status : being spec\u2019d out","title":"Landing page"},{"location":"roadmap/#push-ingest-api","text":"What : We want to create a push API so that it is as easy as possible for a new data resource type to be ingested Status : implementation has started (around 80% complete)","title":"Push ingest API"},{"location":"roadmap/#get-rest-api","text":"What : enable users to access our data map programmatically through a Rest API Status : implementation has started","title":"GET Rest API"},{"location":"roadmap/#index-druid-tables-and-s3-buckets","text":"What : add these new resource types to our data map and create resource pages for them Status : implementation has not started","title":"Index Druid tables and S3 buckets"},{"location":"roadmap/#granular-access-control","text":"What : we want to have a more granular control of the access. For example, only certain types of people would be able to see certain types of metadata/functionality Status : implementation has not started","title":"Granular Access Control"},{"location":"roadmap/#show-distinct-column-values","text":"What : When a column has a limited set of possible values, we want to make then easily discoverable Status : implementation has not started","title":"Show distinct column values"},{"location":"roadmap/#order-by-for-columns","text":"What : we want to help users make sense of what are the columns people use in the tables we index. Within a frequently used table, a column might not be used anymore because it is know to be deprecated Status : implementation has not started","title":"\u201cOrder by\u201d for columns"},{"location":"roadmap/#index-online-datastores","text":"What : We want to make our DynamoDB and other online datastores discoverable by indexing them. For this purpose, we will probably leverage the fact that we have a centralized IDL (interface definition language) Status : implementation has not started","title":"Index online datastores"},{"location":"roadmap/#integration-with-bi-tools","text":"What : get the richness of Amundsen\u2019s metadata to where the data is used: in Bi tools such as Mode, Superset and Tableau Status : implementation has not started","title":"Integration with BI Tools"},{"location":"roadmap/#index-processes","text":"What : we want to index ETLs and pipelines from our Machine Learning Engine Status : implementation has not started","title":"Index Processes"},{"location":"roadmap/#versioning-system","text":"What : We want to create a versioning system for our indexed resources, to be able to index different versions of the same resource. This is especially required for machine learning purposes. Status : implementation has not started","title":"Versioning system"},{"location":"roadmap/#index-teams","text":"What : We want to add teams pages to enable users to see what are the important tables and dashboard a team uses Status : implementation has not started","title":"Index Teams"},{"location":"roadmap/#index-services","text":"What : With our microservices architecture, we want to index services and show how these services interact with data artifacts Status : implementation has not started","title":"Index Services"},{"location":"roadmap/#index-pubsub-systems","text":"What : We want to make our pub/sub systems discoverable Status : implementation has not started","title":"Index Pub/Sub systems"},{"location":"authentication/oidc/","text":"OIDC (Keycloak) Authentication \u00b6 Setting up end-to-end authentication using OIDC is fairly simple and can be done using a Flask wrapper i.e., flaskoidc . flaskoidc leverages the Flask\u2019s before_request functionality to authenticate each request before passing that to the views. It also accepts headers on each request if available in order to validate bearer token from incoming requests. Installation \u00b6 Please refer to the flaskoidc documentation for the installation and the configurations. Note: You need to install and configure flaskoidc for each microservice of Amundsen i.e., for frontendlibrary, metadatalibrary and searchlibrary in order to secure each of them. Amundsen Configuration \u00b6 Once you have flaskoidc installed and configured for each microservice, please set the following environment variables: amundsenfrontendlibrary: APP_WRAPPER: flaskoidc APP_WRAPPER_CLASS: FlaskOIDC amundsenmetadatalibrary: FLASK_APP_MODULE_NAME: flaskoidc FLASK_APP_CLASS_NAME: FlaskOIDC amundsensearchlibrary: (Needs to be implemented) FLASK_APP_MODULE_NAME: flaskoidc FLASK_APP_CLASS_NAME: FlaskOIDC By default flaskoidc whitelist the healthcheck URLs, to not authenticate them. In case of metadatalibrary and searchlibrary we may want to whitelist the healthcheck APIs explicitly using following environment variable. FLASK_OIDC_WHITELISTED_ENDPOINTS: 'api.healthcheck' Setting Up Request Headers \u00b6 To communicate securely between the microservices, you need to pass the bearer token from frontend in each request to metadatalibrary and searchlibrary. This should be done using REQUEST_HEADERS_METHOD config variable in frontendlibrary. Define a function to add the bearer token in each request in your config.py: def get_access_headers ( app ): \"\"\" Function to retrieve and format the Authorization Headers that can be passed to various microservices who are expecting that. :param oidc: OIDC object having authorization information :return: A formatted dictionary containing access token as Authorization header. \"\"\" try : access_token = app . oidc . get_access_token () return { 'Authorization' : 'Bearer {} ' . format ( access_token )} except Exception : return None Set the method as the request header method in your config.py: REQUEST_HEADERS_METHOD = get_access_headers This function will be called using the current app instance to add the headers in each request when calling any endpoint of metadatalibrary and searchlibrary here Setting Up Auth User Method \u00b6 In order to get the current authenticated user (which is being used in Amundsen for many operations), we need to set AUTH_USER_METHOD config variable in frontendlibrary. This function should return email address, user id and any other required information. Define a function to fetch the user information in your config.py: def get_auth_user ( app ): \"\"\" Retrieves the user information from oidc token, and then makes a dictionary 'UserInfo' from the token information dictionary. We need to convert it to a class in order to use the information in the rest of the Amundsen application. :param app: The instance of the current app. :return: A class UserInfo \"\"\" from flask import g user_info = type ( 'UserInfo' , ( object ,), g . oidc_id_token ) # noinspection PyUnresolvedReferences user_info . user_id = user_info . preferred_username return user_info Set the method as the auth user method in your config.py: AUTH_USER_METHOD = get_auth_user Once done, you\u2019ll have the end-to-end authentication in Amundsen without any proxy or code changes. Using Okta with Amundsen on K8s \u00b6 Assumptions: - You have access to okta (you can create a developer account for free!) - You are using k8s to setup amundsen. See amundsen-kube-helm You need to have a stable DNS entry for amundsen-frontend that can be registered in okta. for example in AWS you can setup route53 I will assume for the rest of this tutorial that your stable uri is \u201c http://amundsen-frontend \u201c You need to register amundsen in okta as an app. More info here . But here are specific instructions for amundsen: At this time, I have only succesfully tested integration after ALL grants were checked. Set the Login redirect URIs to: http://amundsen-frontend/oidc_callback No need to set a logout redirect URI Set the Initiate login URI to: http://amundsen-frontend/ (This is where okta will take you if users click on amundsen via okta landing page) Copy the Client ID and Client secret as you will need this later. At present, there is no oidc build of the frontend. So you will need to build an oidc build yourself and upload it to, for example ECR, for use by k8s. You can then specify which image you want to use as a property override for your helm install like so: frontEndServiceImage : 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test Please see further down in this doc for more instructions on how to build frontend. When you start up helm you will need to provide some properties. Here are the properties that need to be overridden for oidc to work: oidcEnabled : true createOidcSecret : true OIDC_CLIENT_ID : YOUR_CLIENT_ID OIDC_CLIENT_SECRET : YOUR_SECRET_ID OIDC_ORG_URL : https://edmunds.okta.com OIDC_AUTH_SERVER_ID : default # You also will need a custom oidc frontend build too frontEndServiceImage : 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test Building frontend with OIDC \u00b6 Please look at this guide for instructions on how to build a custom frontend docker image. The only difference to above is that in your docker file you will want to add the following at the end. This will make sure its ready to go for oidc. You can take alook at the public.Dockerfile as a reference. RUN pip3 install . [ oidc ] ENV FRONTEND_SVC_CONFIG_MODULE_CLASS amundsen_application.oidc_config.OidcConfig ENV APP_WRAPPER flaskoidc ENV APP_WRAPPER_CLASS FlaskOIDC ENV FLASK_OIDC_WHITELISTED_ENDPOINTS status,healthcheck,health ENV FLASK_OIDC_SQLALCHEMY_DATABASE_URI sqlite:///sessions.db","title":"Authentication"},{"location":"authentication/oidc/#oidc-keycloak-authentication","text":"Setting up end-to-end authentication using OIDC is fairly simple and can be done using a Flask wrapper i.e., flaskoidc . flaskoidc leverages the Flask\u2019s before_request functionality to authenticate each request before passing that to the views. It also accepts headers on each request if available in order to validate bearer token from incoming requests.","title":"OIDC (Keycloak) Authentication"},{"location":"authentication/oidc/#installation","text":"Please refer to the flaskoidc documentation for the installation and the configurations. Note: You need to install and configure flaskoidc for each microservice of Amundsen i.e., for frontendlibrary, metadatalibrary and searchlibrary in order to secure each of them.","title":"Installation"},{"location":"authentication/oidc/#amundsen-configuration","text":"Once you have flaskoidc installed and configured for each microservice, please set the following environment variables: amundsenfrontendlibrary: APP_WRAPPER: flaskoidc APP_WRAPPER_CLASS: FlaskOIDC amundsenmetadatalibrary: FLASK_APP_MODULE_NAME: flaskoidc FLASK_APP_CLASS_NAME: FlaskOIDC amundsensearchlibrary: (Needs to be implemented) FLASK_APP_MODULE_NAME: flaskoidc FLASK_APP_CLASS_NAME: FlaskOIDC By default flaskoidc whitelist the healthcheck URLs, to not authenticate them. In case of metadatalibrary and searchlibrary we may want to whitelist the healthcheck APIs explicitly using following environment variable. FLASK_OIDC_WHITELISTED_ENDPOINTS: 'api.healthcheck'","title":"Amundsen Configuration"},{"location":"authentication/oidc/#setting-up-request-headers","text":"To communicate securely between the microservices, you need to pass the bearer token from frontend in each request to metadatalibrary and searchlibrary. This should be done using REQUEST_HEADERS_METHOD config variable in frontendlibrary. Define a function to add the bearer token in each request in your config.py: def get_access_headers ( app ): \"\"\" Function to retrieve and format the Authorization Headers that can be passed to various microservices who are expecting that. :param oidc: OIDC object having authorization information :return: A formatted dictionary containing access token as Authorization header. \"\"\" try : access_token = app . oidc . get_access_token () return { 'Authorization' : 'Bearer {} ' . format ( access_token )} except Exception : return None Set the method as the request header method in your config.py: REQUEST_HEADERS_METHOD = get_access_headers This function will be called using the current app instance to add the headers in each request when calling any endpoint of metadatalibrary and searchlibrary here","title":"Setting Up Request Headers"},{"location":"authentication/oidc/#setting-up-auth-user-method","text":"In order to get the current authenticated user (which is being used in Amundsen for many operations), we need to set AUTH_USER_METHOD config variable in frontendlibrary. This function should return email address, user id and any other required information. Define a function to fetch the user information in your config.py: def get_auth_user ( app ): \"\"\" Retrieves the user information from oidc token, and then makes a dictionary 'UserInfo' from the token information dictionary. We need to convert it to a class in order to use the information in the rest of the Amundsen application. :param app: The instance of the current app. :return: A class UserInfo \"\"\" from flask import g user_info = type ( 'UserInfo' , ( object ,), g . oidc_id_token ) # noinspection PyUnresolvedReferences user_info . user_id = user_info . preferred_username return user_info Set the method as the auth user method in your config.py: AUTH_USER_METHOD = get_auth_user Once done, you\u2019ll have the end-to-end authentication in Amundsen without any proxy or code changes.","title":"Setting Up Auth User Method"},{"location":"authentication/oidc/#using-okta-with-amundsen-on-k8s","text":"Assumptions: - You have access to okta (you can create a developer account for free!) - You are using k8s to setup amundsen. See amundsen-kube-helm You need to have a stable DNS entry for amundsen-frontend that can be registered in okta. for example in AWS you can setup route53 I will assume for the rest of this tutorial that your stable uri is \u201c http://amundsen-frontend \u201c You need to register amundsen in okta as an app. More info here . But here are specific instructions for amundsen: At this time, I have only succesfully tested integration after ALL grants were checked. Set the Login redirect URIs to: http://amundsen-frontend/oidc_callback No need to set a logout redirect URI Set the Initiate login URI to: http://amundsen-frontend/ (This is where okta will take you if users click on amundsen via okta landing page) Copy the Client ID and Client secret as you will need this later. At present, there is no oidc build of the frontend. So you will need to build an oidc build yourself and upload it to, for example ECR, for use by k8s. You can then specify which image you want to use as a property override for your helm install like so: frontEndServiceImage : 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test Please see further down in this doc for more instructions on how to build frontend. When you start up helm you will need to provide some properties. Here are the properties that need to be overridden for oidc to work: oidcEnabled : true createOidcSecret : true OIDC_CLIENT_ID : YOUR_CLIENT_ID OIDC_CLIENT_SECRET : YOUR_SECRET_ID OIDC_ORG_URL : https://edmunds.okta.com OIDC_AUTH_SERVER_ID : default # You also will need a custom oidc frontend build too frontEndServiceImage : 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test","title":"Using Okta with Amundsen on K8s"},{"location":"authentication/oidc/#building-frontend-with-oidc","text":"Please look at this guide for instructions on how to build a custom frontend docker image. The only difference to above is that in your docker file you will want to add the following at the end. This will make sure its ready to go for oidc. You can take alook at the public.Dockerfile as a reference. RUN pip3 install . [ oidc ] ENV FRONTEND_SVC_CONFIG_MODULE_CLASS amundsen_application.oidc_config.OidcConfig ENV APP_WRAPPER flaskoidc ENV APP_WRAPPER_CLASS FlaskOIDC ENV FLASK_OIDC_WHITELISTED_ENDPOINTS status,healthcheck,health ENV FLASK_OIDC_SQLALCHEMY_DATABASE_URI sqlite:///sessions.db","title":"Building frontend with OIDC"},{"location":"common/","text":"Amundsen Common \u00b6 Amundsen Common library holds common codes among micro services in Amundsen. For information about Amundsen and our other services, visit the main repository . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.6 Doc \u00b6 https://lyft.github.io/amundsen/","title":"Overview"},{"location":"common/#amundsen-common","text":"Amundsen Common library holds common codes among micro services in Amundsen. For information about Amundsen and our other services, visit the main repository . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Common"},{"location":"common/#requirements","text":"Python >= 3.6","title":"Requirements"},{"location":"common/#doc","text":"https://lyft.github.io/amundsen/","title":"Doc"},{"location":"common/CODE_OF_CONDUCT/","text":"This project is governed by Lyft\u2019s code of conduct . All contributors and participants agree to abide by its terms.","title":"CODE OF CONDUCT"},{"location":"common/PULL_REQUEST_TEMPLATE/","text":"Summary of Changes \u00b6 Include a summary of changes then remove this line Tests \u00b6 What tests did you add or modify and why? If no tests were added or modified, explain why. Remove this line Documentation \u00b6 What documentation did you add or modify and why? Add any relevant links then remove this line CheckList \u00b6 Make sure you have checked all steps below to ensure a timely review. - [ ] PR title addresses the issue accurately and concisely. Example: \u201cUpdates the version of Flask to v1.0.2\u201d - In case you are adding a dependency, check if the license complies with the ASF 3 rd Party License Policy . - [ ] PR includes a summary of changes. - [ ] PR adds unit tests, updates existing unit tests, OR documents why no test additions or modifications are needed. - [ ] In case of new functionality, my PR adds documentation that describes how to use it. - All the public functions and the classes in the PR contain docstrings that explain what it does - [ ] PR passes make test","title":"PULL REQUEST TEMPLATE"},{"location":"common/PULL_REQUEST_TEMPLATE/#summary-of-changes","text":"Include a summary of changes then remove this line","title":"Summary of Changes"},{"location":"common/PULL_REQUEST_TEMPLATE/#tests","text":"What tests did you add or modify and why? If no tests were added or modified, explain why. Remove this line","title":"Tests"},{"location":"common/PULL_REQUEST_TEMPLATE/#documentation","text":"What documentation did you add or modify and why? Add any relevant links then remove this line","title":"Documentation"},{"location":"common/PULL_REQUEST_TEMPLATE/#checklist","text":"Make sure you have checked all steps below to ensure a timely review. - [ ] PR title addresses the issue accurately and concisely. Example: \u201cUpdates the version of Flask to v1.0.2\u201d - In case you are adding a dependency, check if the license complies with the ASF 3 rd Party License Policy . - [ ] PR includes a summary of changes. - [ ] PR adds unit tests, updates existing unit tests, OR documents why no test additions or modifications are needed. - [ ] In case of new functionality, my PR adds documentation that describes how to use it. - All the public functions and the classes in the PR contain docstrings that explain what it does - [ ] PR passes make test","title":"CheckList"},{"location":"databuilder/","text":"Amundsen Databuilder \u00b6 Amundsen Databuilder is a data ingestion library, which is inspired by Apache Gobblin . It could be used in an orchestration framework(e.g. Apache Airflow) to build data from Amundsen. You could use the library either with an adhoc python script( example ) or inside an Apache Airflow DAG( example ). For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python = 2.7.x or Python >= 3.6.x Doc \u00b6 https://lyft.github.io/amundsen/ Concept \u00b6 ETL job consists of extraction of records from the source, transform records, if necessary, and load records into the sink. Amundsen Databuilder is a ETL framework for Amundsen and there are corresponding components for ETL called Extractor, Transformer, and Loader that deals with record level operation. A component called task controls all these three components. Job is the highest level component in Databuilder that controls task and publisher and is the one that client use to launch the ETL job. In Databuilder, each components are highly modularized and each components are using namespace based config, HOCON config, which makes it highly reusable and pluggable. (e.g: transformer can be reused within extractor, or extractor can be reused within extractor) (Note that concept on components are highly motivated by Apache Gobblin ) Extractor \u00b6 Extractor extracts record from the source. This does not neccessarily mean that it only supports pull pattern in ETL. For example, extracting record from messaging bus make it a push pattern in ETL. Transformer \u00b6 Transfomer takes record from either extractor or from transformer itself (via ChainedTransformer) to transform record. Loader \u00b6 A loader takes record from transformer or from extractor directly and load it to sink, or staging area. As loader is operated in record level, it\u2019s not capable of supporting atomicity. Task \u00b6 A task orchestrates extractor, transformer, and loader to perform record level operation. Record \u00b6 A record is represented by one of models . Publisher \u00b6 A publisher is an optional component. It\u2019s common usage is to support atomicity in job level and/or to easily support bulk load into the sink. Job \u00b6 Job is the highest level component in Databuilder, and it orchestrates task, and publisher. Model \u00b6 Models are abstractions representing the domain. List of extractors \u00b6 DBAPIExtractor \u00b6 An extractor that uses Python Database API interface. DBAPI requires three information, connection object that conforms DBAPI spec, a SELECT SQL statement, and a model class that correspond to the output of each row of SQL statement. job_config = ConfigFactory . from_dict ({ 'extractor.dbapi {} ' . format ( DBAPIExtractor . CONNECTION_CONFIG_KEY ): db_api_conn , 'extractor.dbapi. {} ' . format ( DBAPIExtractor . SQL_CONFIG_KEY ): select_sql_stmt , 'extractor.dbapi.model_class' : 'package.module_name.class_name' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DBAPIExtractor (), loader = AnyLoader ())) job . launch () GenericExtractor \u00b6 An extractor that takes list of dict from user through config. HiveTableLastUpdatedExtractor \u00b6 An extractor that extracts last updated time from Hive metastore and underlying file system. Although, hive metastore has a parameter called \u201clast_modified_time\u201d, but it cannot be used as it provides DDL timestamp not DML timestamp. For this reason, HiveTableLastUpdatedExtractor is utilizing underlying file of Hive to fetch latest updated date. However, it is not efficient to poke all files in Hive, and it only pokes underlying storage for non-partitioned table. For partitioned table, it will fetch partition created timestamp, and it\u2019s close enough for last updated timestamp. As getting metadata from files could be time consuming there\u2019re several features to increase performance. 1. Support of multithreading to parallelize metadata fetching. Although, cpython\u2019s multithreading is not true multithreading as it\u2019s bounded by single core, getting metadata of file is mostly IO bound operation. Note that number of threads should be less or equal to number of connections. 1. User can pass where clause to only include certain schema and also remove certain tables. For example, by adding something like TBL_NAME NOT REGEXP '(tmp|temp) would eliminate unncecessary computation. job_config = ConfigFactory . from_dict ({ 'extractor.hive_table_last_updated.partitioned_table_where_clause_suffix' : partitioned_table_where_clause , 'extractor.hive_table_last_updated.non_partitioned_table_where_clause_suffix' ): non_partitioned_table_where_clause , 'extractor.hive_table_last_updated.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string , 'extractor.hive_table_last_updated.extractor.fs_worker_pool_size' : pool_size , 'extractor.hive_table_last_updated.filesystem. {} ' . format ( FileSystem . DASK_FILE_SYSTEM ): s3fs . S3FileSystem ( anon = False , config_kwargs = { 'max_pool_connections' : pool_size })}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = HiveTableLastUpdatedExtractor (), loader = AnyLoader ())) job . launch () HiveTableMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from Hive metastore database. job_config = ConfigFactory . from_dict ({ 'extractor.hive_table_metadata. {} ' . format ( HiveTableMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.hive_table_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = HiveTableMetadataExtractor (), loader = AnyLoader ())) job . launch () CassandraExtractor \u00b6 An extractor that extracts table and column metadata including keyspace, table name, column name and column type from Apache Cassandra databases job_config = ConfigFactory . from_dict ({ 'extractor.cassandra. {} ' . format ( CassandraExtractor . CLUSTER_KEY ): cluster_identifier_string , 'extractor.cassandra. {} ' . format ( CassandraExtractor . IPS_KEY ): [ 127.0 . 0.1 ], 'extractor.cassandra. {} ' . format ( CassandraExtractor . KWARGS_KEY ): {}, 'extractor.cassandra. {} ' . format ( CassandraExtractor . FILTER_FUNCTION_KEY ): my_filter_function , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = CassandraExtractor (), loader = AnyLoader ())) job . launch () If using the function filter options here is the function description def filter ( keytab , table ): # return False if you don't want to add that table and True if you want to add return True If needed to define more args on the cassandra cluster you can pass through kwargs args config = ConfigFactory . from_dict ({ 'extractor.cassandra. {} ' . format ( CassandraExtractor . IPS_KEY ): [ 127.0 . 0.1 ], 'extractor.cassandra. {} ' . format ( CassandraExtractor . KWARGS_KEY ): { 'port' : 9042 } }) # it will call the cluster constructor like this Cluster ([ 127.0 . 0.1 ], ** kwargs ) GlueExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from AWS Glue metastore. Before running make sure you have a working AWS profile configured and have access to search tables on Glue job_config = ConfigFactory . from_dict ({ 'extractor.glue. {} ' . format ( GlueExtractor . CLUSTER_KEY ): cluster_identifier_string , 'extractor.glue. {} ' . format ( GlueExtractor . FILTER_KEY ): []}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = GlueExtractor (), loader = AnyLoader ())) job . launch () If using the filters option here is the input format [ { \"Key\": \"string\", \"Value\": \"string\", \"Comparator\": \"EQUALS\"|\"GREATER_THAN\"|\"LESS_THAN\"|\"GREATER_THAN_EQUALS\"|\"LESS_THAN_EQUALS\" } ... ] PostgresMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Postgres or Redshift database. By default, the Postgres/Redshift database name is used as the cluter name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query (see the sample dag for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.postgres_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = PostgresMetadataExtractor (), loader = AnyLoader ())) job . launch () MysqlMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a MYSQL database. By default, the MYSQL database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query. The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.mysql_metadata. {} ' . format ( MysqlMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.mysql_metadata. {} ' . format ( MysqlMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.postgres_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = MysqlMetadataExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () #### [Db2MetadataExtractor](https://github.com/lyft/amundsendatabuilder/blob/master/databuilder/extractor/db2_metadata_extractor.py \"Db2MetadataExtractor\") An extractor that extracts table and column metadata including database , schema , table name , table description , column name and column description from a Unix , Windows or Linux Db2 database or BigSQL . The ` where_clause_suffix ` below should define which schemas you 'd like to query or those that you would not (see [the sample data loader](https://github.com/lyft/amundsendatabuilder/blob/master/example/sample_db2_data_loader.py) for an example). The SQL query driving the extraction is defined [ here ]( https : // github . com / lyft / amundsendatabuilder / blob / master / databuilder / extractor / db2_metadata_extractor . py ) ``` python job_config = ConfigFactory . from_dict ({ 'extractor.db2_metadata. {} ' . format ( Db2MetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.db2_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Db2MetadataExtractor (), loader = AnyLoader ())) job . launch () SnowflakeMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Snowflake database. By default, the Snowflake database name is used as the cluter name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. By default, the Snowflake database is set to PROD . To override this, set DATABASE_KEY to WhateverNameOfYourDb . The where_clause_suffix below should define which schemas you\u2019d like to query (see the sample dag for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . DATABASE_KEY ): 'YourDbName' , 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.postgres_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SnowflakeMetadataExtractor (), loader = AnyLoader ())) job . launch () BigQueryMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Bigquery database. The API calls driving the extraction is defined here You will need to create a service account for reading metadata and grant it \u201cBigQuery Metadata Viewer\u201d access to all of your datasets. This can all be done via the bigquery ui. Download the creditials file and store it securely. Set the GOOGLE_APPLICATION_CREDENTIALS environment varible to the location of your credtials files and your code should have access to everything it needs. You can configure bigquery like this. You can optionally set a label filter if you only want to pull tables with a certain label. job_config = { 'extractor.bigquery_table_metadata. {} ' . format ( BigQueryMetadataExtractor . PROJECT_ID_KEY ): gcloud_project } if label_filter : job_config [ 'extractor.bigquery_table_metadata. {} ' . format ( BigQueryMetadataExtractor . FILTER_KEY ) ] = label_filter task = DefaultTask ( extractor = BigQueryMetadataExtractor (), loader = csv_loader , transformer = NoopTransformer ()) job = DefaultJob ( conf = ConfigFactory . from_dict ( job_config ), task = task , publisher = Neo4jCsvPublisher ()) job . launch () Neo4jEsLastUpdatedExtractor \u00b6 An extractor that basically get current timestamp and passes it GenericExtractor. This extractor is basically being used to create timestamp for \u201cAmundsen was last indexed on \u2026\u201d in Amundsen web page\u2019s footer. Neo4jExtractor \u00b6 An extractor that extracts records from Neo4j based on provided Cypher query . One example is to extract data from Neo4j so that it can transform and publish to Elasticsearch. job_config = ConfigFactory . from_dict ({ 'extractor.neo4j. {} ' . format ( Neo4jExtractor . CYPHER_QUERY_CONFIG_KEY ) ': cypher_query, 'extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'package.module.class_name' , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jExtractor (), loader = AnyLoader ())) job . launch () Neo4jSearchDataExtractor \u00b6 An extractor that is extracting Neo4j utilizing Neo4jExtractor where CYPHER query is already embedded in it. job_config = ConfigFactory . from_dict ({ 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'databuilder.models.neo4j_data.Neo4jDataResult' , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jSearchDataExtractor (), loader = AnyLoader ())) job . launch () SQLAlchemyExtractor \u00b6 An extractor utilizes SQLAlchemy to extract record from any database that support SQL Alchemy. job_config = ConfigFactory . from_dict ({ 'extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string (), 'extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . EXTRACT_SQL ): sql , 'extractor.sqlalchemy.model_class' : 'package.module.class_name' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SQLAlchemyExtractor (), loader = AnyLoader ())) job . launch () List of transformers \u00b6 ChainedTransformer \u00b6 A chanined transformer that can take multiple transformer. RegexStrReplaceTransformer \u00b6 Generic string replacement transformer using REGEX. User can pass list of tuples where tuple contains regex and replacement pair. job_config = ConfigFactory . from_dict ({ 'transformer.regex_str_replace. {} ' . format ( REGEX_REPLACE_TUPLE_LIST ): [( ',' , ' ' ), ( '\"' , '' )], 'transformer.regex_str_replace. {} ' . format ( ATTRIBUTE_NAME ): 'instance_field_name' ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), transformer = RegexStrReplaceTransformer (), loader = AnyLoader ())) job . launch () List of loader \u00b6 FsNeo4jCSVLoader \u00b6 Write node and relationship CSV file(s) that can be consumed by Neo4jCsvPublisher. It assumes that the record it consumes is instance of Neo4jCsvSerializable. job_config = ConfigFactory . from_dict ({ 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder },) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () FSElasticsearchJSONLoader \u00b6 Write Elasticsearch document in JSON format which can be consumed by ElasticsearchPublisher. It assumes that the record it consumes is instance of ElasticsearchDocument. tmp_folder = '/var/tmp/amundsen/dummy_metadata' node_files_folder = ' {tmp_folder} /nodes/' . format ( tmp_folder = tmp_folder ) relationship_files_folder = ' {tmp_folder} /relationships/' . format ( tmp_folder = tmp_folder ) job_config = ConfigFactory . from_dict ({ 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): data_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch () List of publisher \u00b6 Neo4jCsvPublisher \u00b6 A Publisher takes two folders for input and publishes to Neo4j. One folder will contain CSV file(s) for Node where the other folder will contain CSV file(s) for Relationship. Neo4j follows Label Node properties Graph and refer to here for more information job_config = ConfigFactory . from_dict ({ 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NODE_FILES_DIR ): node_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . RELATION_FILES_DIR ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_END_POINT_KEY ): neo4j_endpoint , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_USER ): neo4j_user , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_PASSWORD ): neo4j_password ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () ElasticsearchPublisher \u00b6 Elasticsearch Publisher uses Bulk API to load data from JSON file. Elasticsearch publisher supports atomic operation by utilizing alias in Elasticsearch. A new index is created and data is uploaded into it. After the upload is complete, index alias is swapped to point to new index from old index and traffic is routed to new index. tmp_folder = '/var/tmp/amundsen/dummy_metadata' node_files_folder = ' {tmp_folder} /nodes/' . format ( tmp_folder = tmp_folder ) relationship_files_folder = ' {tmp_folder} /relationships/' . format ( tmp_folder = tmp_folder ) job_config = ConfigFactory . from_dict ({ 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): data_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_PATH_CONFIG_KEY ): data_file_path , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_MODE_CONFIG_KEY ): 'r' , 'publisher.elasticsearch {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_CLIENT_CONFIG_KEY ): elasticsearch_client , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_NEW_INDEX_CONFIG_KEY ): elasticsearch_new_index , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_DOC_TYPE_CONFIG_KEY ): elasticsearch_doc_type , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_ALIAS_CONFIG_KEY ): elasticsearch_index_alias ,) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch () Callback \u00b6 Callback interface is built upon a Observer pattern where the participant want to take any action when target\u2019s state changes. Publisher is the first one adopting Callback where registered Callback will be called either when publish succeeded or when publish failed. In order to register callback, Publisher provides register_call_back method. One use case is for Extractor that needs to commit when job is finished (e.g: Kafka). Having Extractor register a callback to Publisher to commit when publish is successful, extractor can safely commit by implementing commit logic into on_success method. REST API Query \u00b6 Databuilder now has a generic REST API Query capability that can be joined each other. Most of the cases of extraction is currently from Database or Datawarehouse that is queryable via SQL. However, not all metadata sources provide our access to its Database and they mostly provide REST API to consume their metadata. The challenges come with REST API is that: there\u2019s no explicit standard in REST API. Here, we need to conform to majority of cases (HTTP call with JSON payload & response) but open for extension for different authentication scheme, and different way of pagination, etc. It is hardly the case that you would get what you want from one REST API call. It is usually the case that you need to snitch (JOIN) multiple REST API calls together to get the information you want. To solve this challenges, we introduce RestApiQuery RestAPIQuery is: 1. Assuming that REST API is using HTTP(S) call with GET method \u2013 RestAPIQuery intention\u2019s is read , not write \u2013 where basic HTTP auth is supported out of the box. There\u2019s extension point on other authentication scheme such as Oauth, and pagination, etc. 2. Usually, you want the subset of the response you get from the REST API call \u2013 value extraction. To extract the value you want, RestApiQuery uses JSONPath which is similar product as XPATH of XML. 3. You can JOIN multiple RestApiQuery together. More detail on JOIN operation in RestApiQuery: 1. It joins multiple RestApiQuery together by accepting prior RestApiQuery as a constructor \u2013 a Decorator pattern 2. In REST API, URL is the one that locates the resource we want. Here, JOIN simply means we need to find resource based on the identifier that other query\u2019s result has . In other words, when RestApiQuery forms URL, it uses previous query\u2019s result to compute the URL e.g: Previous record: {\"dashboard_id\": \"foo\"}, URL before: http://foo.bar/dashboard/{dashboard_id} URL after compute: http://foo.bar/dashboard/foo With this pattern RestApiQuery supports 1:1 and 1:N JOIN relationship. (GROUP BY or any other aggregation, sub-query join is not supported) To see in action, take a peek at ModeDashboardExtractor Removing stale data in Neo4j \u2013 Neo4jStalenessRemovalTask : \u00b6 As Databuilder ingestion mostly consists of either INSERT OR UPDATE, there could be some stale data that has been removed from metadata source but still remains in Neo4j database. Neo4jStalenessRemovalTask basically detects staleness and removes it. In Neo4jCsvPublisher , it adds attributes \u201cpublished_tag\u201d and \u201cpublisher_last_updated_epoch_ms\u201d on every nodes and relations. You can use either of these two attributes to detect staleness and remove those stale node or relation from the database. Using \u201cpublished_tag\u201d to remove stale data \u00b6 Use published_tag to remove stale data, when it is certain that non-matching tag is stale once all the ingestion is completed. For example, suppose that you use current date (or execution date in Airflow) as a published_tag , \u201c2020-03-31\u201d. Once Databuilder ingests all tables and all columns, all table nodes and column nodes should have published_tag as \u201c2020-03-31\u201d. It is safe to assume that table nodes and column nodes whose published_tag is different \u2013 such as \u201c2020-03-30\u201d or \u201c2020-02-10\u201d \u2013 means that it is deleted from the source metadata. You can use Neo4jStalenessRemovalTask to delete those stale data. 1 2 3 4 5 6 7 8 9 10 11 12 13 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_nodes': ['Table', 'Column'], 'task.remove_stale_data.job_publish_tag': '2020-03-31' } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch() Note that there\u2019s protection mechanism, staleness_max_pct , that protect your data being wiped out when something is clearly wrong. \u201c staleness_max_pct \u201d basically first measure the proportion of elements that will be deleted and if it exceeds threshold per type ( 10% on the configuration above ), the deletion won\u2019t be executed and the task aborts. Using \u201cpublisher_last_updated_epoch_ms\u201d to remove stale data \u00b6 You can think this approach as TTL based eviction. This is particularly useful when there are multiple ingestion pipelines and you cannot be sure when all ingestion is done. In this case, you might still can say that if specific node or relation has not been published past 3 days, it\u2019s stale data. 1 2 3 4 5 6 7 8 9 10 11 12 13 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_relations': ['READ', 'READ_BY'], 'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3 } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch() Above configuration is trying to delete stale usage relation (READ, READ_BY), by deleting READ or READ_BY relation that has not been published past 3 days. If number of elements to be removed is more than 10% per type, this task will be aborted without executing any deletion. Dry run \u00b6 Deletion is always scary and it\u2019s better to perform dryrun before put this into action. You can use Dry run to see what sort of Cypher query will be executed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_relations': ['READ', 'READ_BY'], 'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3 'task.remove_stale_data.dry_run': True } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch()","title":"Overview"},{"location":"databuilder/#amundsen-databuilder","text":"Amundsen Databuilder is a data ingestion library, which is inspired by Apache Gobblin . It could be used in an orchestration framework(e.g. Apache Airflow) to build data from Amundsen. You could use the library either with an adhoc python script( example ) or inside an Apache Airflow DAG( example ). For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Databuilder"},{"location":"databuilder/#requirements","text":"Python = 2.7.x or Python >= 3.6.x","title":"Requirements"},{"location":"databuilder/#doc","text":"https://lyft.github.io/amundsen/","title":"Doc"},{"location":"databuilder/#concept","text":"ETL job consists of extraction of records from the source, transform records, if necessary, and load records into the sink. Amundsen Databuilder is a ETL framework for Amundsen and there are corresponding components for ETL called Extractor, Transformer, and Loader that deals with record level operation. A component called task controls all these three components. Job is the highest level component in Databuilder that controls task and publisher and is the one that client use to launch the ETL job. In Databuilder, each components are highly modularized and each components are using namespace based config, HOCON config, which makes it highly reusable and pluggable. (e.g: transformer can be reused within extractor, or extractor can be reused within extractor) (Note that concept on components are highly motivated by Apache Gobblin )","title":"Concept"},{"location":"databuilder/#extractor","text":"Extractor extracts record from the source. This does not neccessarily mean that it only supports pull pattern in ETL. For example, extracting record from messaging bus make it a push pattern in ETL.","title":"Extractor"},{"location":"databuilder/#transformer","text":"Transfomer takes record from either extractor or from transformer itself (via ChainedTransformer) to transform record.","title":"Transformer"},{"location":"databuilder/#loader","text":"A loader takes record from transformer or from extractor directly and load it to sink, or staging area. As loader is operated in record level, it\u2019s not capable of supporting atomicity.","title":"Loader"},{"location":"databuilder/#task","text":"A task orchestrates extractor, transformer, and loader to perform record level operation.","title":"Task"},{"location":"databuilder/#record","text":"A record is represented by one of models .","title":"Record"},{"location":"databuilder/#publisher","text":"A publisher is an optional component. It\u2019s common usage is to support atomicity in job level and/or to easily support bulk load into the sink.","title":"Publisher"},{"location":"databuilder/#job","text":"Job is the highest level component in Databuilder, and it orchestrates task, and publisher.","title":"Job"},{"location":"databuilder/#model","text":"Models are abstractions representing the domain.","title":"Model"},{"location":"databuilder/#list-of-extractors","text":"","title":"List of extractors"},{"location":"databuilder/#dbapiextractor","text":"An extractor that uses Python Database API interface. DBAPI requires three information, connection object that conforms DBAPI spec, a SELECT SQL statement, and a model class that correspond to the output of each row of SQL statement. job_config = ConfigFactory . from_dict ({ 'extractor.dbapi {} ' . format ( DBAPIExtractor . CONNECTION_CONFIG_KEY ): db_api_conn , 'extractor.dbapi. {} ' . format ( DBAPIExtractor . SQL_CONFIG_KEY ): select_sql_stmt , 'extractor.dbapi.model_class' : 'package.module_name.class_name' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DBAPIExtractor (), loader = AnyLoader ())) job . launch ()","title":"DBAPIExtractor"},{"location":"databuilder/#genericextractor","text":"An extractor that takes list of dict from user through config.","title":"GenericExtractor"},{"location":"databuilder/#hivetablelastupdatedextractor","text":"An extractor that extracts last updated time from Hive metastore and underlying file system. Although, hive metastore has a parameter called \u201clast_modified_time\u201d, but it cannot be used as it provides DDL timestamp not DML timestamp. For this reason, HiveTableLastUpdatedExtractor is utilizing underlying file of Hive to fetch latest updated date. However, it is not efficient to poke all files in Hive, and it only pokes underlying storage for non-partitioned table. For partitioned table, it will fetch partition created timestamp, and it\u2019s close enough for last updated timestamp. As getting metadata from files could be time consuming there\u2019re several features to increase performance. 1. Support of multithreading to parallelize metadata fetching. Although, cpython\u2019s multithreading is not true multithreading as it\u2019s bounded by single core, getting metadata of file is mostly IO bound operation. Note that number of threads should be less or equal to number of connections. 1. User can pass where clause to only include certain schema and also remove certain tables. For example, by adding something like TBL_NAME NOT REGEXP '(tmp|temp) would eliminate unncecessary computation. job_config = ConfigFactory . from_dict ({ 'extractor.hive_table_last_updated.partitioned_table_where_clause_suffix' : partitioned_table_where_clause , 'extractor.hive_table_last_updated.non_partitioned_table_where_clause_suffix' ): non_partitioned_table_where_clause , 'extractor.hive_table_last_updated.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string , 'extractor.hive_table_last_updated.extractor.fs_worker_pool_size' : pool_size , 'extractor.hive_table_last_updated.filesystem. {} ' . format ( FileSystem . DASK_FILE_SYSTEM ): s3fs . S3FileSystem ( anon = False , config_kwargs = { 'max_pool_connections' : pool_size })}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = HiveTableLastUpdatedExtractor (), loader = AnyLoader ())) job . launch ()","title":"HiveTableLastUpdatedExtractor"},{"location":"databuilder/#hivetablemetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from Hive metastore database. job_config = ConfigFactory . from_dict ({ 'extractor.hive_table_metadata. {} ' . format ( HiveTableMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.hive_table_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = HiveTableMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"HiveTableMetadataExtractor"},{"location":"databuilder/#cassandraextractor","text":"An extractor that extracts table and column metadata including keyspace, table name, column name and column type from Apache Cassandra databases job_config = ConfigFactory . from_dict ({ 'extractor.cassandra. {} ' . format ( CassandraExtractor . CLUSTER_KEY ): cluster_identifier_string , 'extractor.cassandra. {} ' . format ( CassandraExtractor . IPS_KEY ): [ 127.0 . 0.1 ], 'extractor.cassandra. {} ' . format ( CassandraExtractor . KWARGS_KEY ): {}, 'extractor.cassandra. {} ' . format ( CassandraExtractor . FILTER_FUNCTION_KEY ): my_filter_function , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = CassandraExtractor (), loader = AnyLoader ())) job . launch () If using the function filter options here is the function description def filter ( keytab , table ): # return False if you don't want to add that table and True if you want to add return True If needed to define more args on the cassandra cluster you can pass through kwargs args config = ConfigFactory . from_dict ({ 'extractor.cassandra. {} ' . format ( CassandraExtractor . IPS_KEY ): [ 127.0 . 0.1 ], 'extractor.cassandra. {} ' . format ( CassandraExtractor . KWARGS_KEY ): { 'port' : 9042 } }) # it will call the cluster constructor like this Cluster ([ 127.0 . 0.1 ], ** kwargs )","title":"CassandraExtractor"},{"location":"databuilder/#glueextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from AWS Glue metastore. Before running make sure you have a working AWS profile configured and have access to search tables on Glue job_config = ConfigFactory . from_dict ({ 'extractor.glue. {} ' . format ( GlueExtractor . CLUSTER_KEY ): cluster_identifier_string , 'extractor.glue. {} ' . format ( GlueExtractor . FILTER_KEY ): []}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = GlueExtractor (), loader = AnyLoader ())) job . launch () If using the filters option here is the input format [ { \"Key\": \"string\", \"Value\": \"string\", \"Comparator\": \"EQUALS\"|\"GREATER_THAN\"|\"LESS_THAN\"|\"GREATER_THAN_EQUALS\"|\"LESS_THAN_EQUALS\" } ... ]","title":"GlueExtractor"},{"location":"databuilder/#postgresmetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Postgres or Redshift database. By default, the Postgres/Redshift database name is used as the cluter name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query (see the sample dag for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.postgres_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = PostgresMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"PostgresMetadataExtractor"},{"location":"databuilder/#mysqlmetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a MYSQL database. By default, the MYSQL database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query. The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.mysql_metadata. {} ' . format ( MysqlMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.mysql_metadata. {} ' . format ( MysqlMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.postgres_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = MysqlMetadataExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () #### [Db2MetadataExtractor](https://github.com/lyft/amundsendatabuilder/blob/master/databuilder/extractor/db2_metadata_extractor.py \"Db2MetadataExtractor\") An extractor that extracts table and column metadata including database , schema , table name , table description , column name and column description from a Unix , Windows or Linux Db2 database or BigSQL . The ` where_clause_suffix ` below should define which schemas you 'd like to query or those that you would not (see [the sample data loader](https://github.com/lyft/amundsendatabuilder/blob/master/example/sample_db2_data_loader.py) for an example). The SQL query driving the extraction is defined [ here ]( https : // github . com / lyft / amundsendatabuilder / blob / master / databuilder / extractor / db2_metadata_extractor . py ) ``` python job_config = ConfigFactory . from_dict ({ 'extractor.db2_metadata. {} ' . format ( Db2MetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.db2_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Db2MetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"MysqlMetadataExtractor"},{"location":"databuilder/#snowflakemetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Snowflake database. By default, the Snowflake database name is used as the cluter name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. By default, the Snowflake database is set to PROD . To override this, set DATABASE_KEY to WhateverNameOfYourDb . The where_clause_suffix below should define which schemas you\u2019d like to query (see the sample dag for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . DATABASE_KEY ): 'YourDbName' , 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.postgres_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SnowflakeMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"SnowflakeMetadataExtractor"},{"location":"databuilder/#bigquerymetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Bigquery database. The API calls driving the extraction is defined here You will need to create a service account for reading metadata and grant it \u201cBigQuery Metadata Viewer\u201d access to all of your datasets. This can all be done via the bigquery ui. Download the creditials file and store it securely. Set the GOOGLE_APPLICATION_CREDENTIALS environment varible to the location of your credtials files and your code should have access to everything it needs. You can configure bigquery like this. You can optionally set a label filter if you only want to pull tables with a certain label. job_config = { 'extractor.bigquery_table_metadata. {} ' . format ( BigQueryMetadataExtractor . PROJECT_ID_KEY ): gcloud_project } if label_filter : job_config [ 'extractor.bigquery_table_metadata. {} ' . format ( BigQueryMetadataExtractor . FILTER_KEY ) ] = label_filter task = DefaultTask ( extractor = BigQueryMetadataExtractor (), loader = csv_loader , transformer = NoopTransformer ()) job = DefaultJob ( conf = ConfigFactory . from_dict ( job_config ), task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"BigQueryMetadataExtractor"},{"location":"databuilder/#neo4jeslastupdatedextractor","text":"An extractor that basically get current timestamp and passes it GenericExtractor. This extractor is basically being used to create timestamp for \u201cAmundsen was last indexed on \u2026\u201d in Amundsen web page\u2019s footer.","title":"Neo4jEsLastUpdatedExtractor"},{"location":"databuilder/#neo4jextractor","text":"An extractor that extracts records from Neo4j based on provided Cypher query . One example is to extract data from Neo4j so that it can transform and publish to Elasticsearch. job_config = ConfigFactory . from_dict ({ 'extractor.neo4j. {} ' . format ( Neo4jExtractor . CYPHER_QUERY_CONFIG_KEY ) ': cypher_query, 'extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'package.module.class_name' , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jExtractor (), loader = AnyLoader ())) job . launch ()","title":"Neo4jExtractor"},{"location":"databuilder/#neo4jsearchdataextractor","text":"An extractor that is extracting Neo4j utilizing Neo4jExtractor where CYPHER query is already embedded in it. job_config = ConfigFactory . from_dict ({ 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'databuilder.models.neo4j_data.Neo4jDataResult' , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jSearchDataExtractor (), loader = AnyLoader ())) job . launch ()","title":"Neo4jSearchDataExtractor"},{"location":"databuilder/#sqlalchemyextractor","text":"An extractor utilizes SQLAlchemy to extract record from any database that support SQL Alchemy. job_config = ConfigFactory . from_dict ({ 'extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string (), 'extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . EXTRACT_SQL ): sql , 'extractor.sqlalchemy.model_class' : 'package.module.class_name' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SQLAlchemyExtractor (), loader = AnyLoader ())) job . launch ()","title":"SQLAlchemyExtractor"},{"location":"databuilder/#list-of-transformers","text":"","title":"List of transformers"},{"location":"databuilder/#chainedtransformer","text":"A chanined transformer that can take multiple transformer.","title":"ChainedTransformer"},{"location":"databuilder/#regexstrreplacetransformer","text":"Generic string replacement transformer using REGEX. User can pass list of tuples where tuple contains regex and replacement pair. job_config = ConfigFactory . from_dict ({ 'transformer.regex_str_replace. {} ' . format ( REGEX_REPLACE_TUPLE_LIST ): [( ',' , ' ' ), ( '\"' , '' )], 'transformer.regex_str_replace. {} ' . format ( ATTRIBUTE_NAME ): 'instance_field_name' ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), transformer = RegexStrReplaceTransformer (), loader = AnyLoader ())) job . launch ()","title":"RegexStrReplaceTransformer"},{"location":"databuilder/#list-of-loader","text":"","title":"List of loader"},{"location":"databuilder/#fsneo4jcsvloader","text":"Write node and relationship CSV file(s) that can be consumed by Neo4jCsvPublisher. It assumes that the record it consumes is instance of Neo4jCsvSerializable. job_config = ConfigFactory . from_dict ({ 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder },) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch ()","title":"FsNeo4jCSVLoader"},{"location":"databuilder/#fselasticsearchjsonloader","text":"Write Elasticsearch document in JSON format which can be consumed by ElasticsearchPublisher. It assumes that the record it consumes is instance of ElasticsearchDocument. tmp_folder = '/var/tmp/amundsen/dummy_metadata' node_files_folder = ' {tmp_folder} /nodes/' . format ( tmp_folder = tmp_folder ) relationship_files_folder = ' {tmp_folder} /relationships/' . format ( tmp_folder = tmp_folder ) job_config = ConfigFactory . from_dict ({ 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): data_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch ()","title":"FSElasticsearchJSONLoader"},{"location":"databuilder/#list-of-publisher","text":"","title":"List of publisher"},{"location":"databuilder/#neo4jcsvpublisher","text":"A Publisher takes two folders for input and publishes to Neo4j. One folder will contain CSV file(s) for Node where the other folder will contain CSV file(s) for Relationship. Neo4j follows Label Node properties Graph and refer to here for more information job_config = ConfigFactory . from_dict ({ 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NODE_FILES_DIR ): node_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . RELATION_FILES_DIR ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_END_POINT_KEY ): neo4j_endpoint , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_USER ): neo4j_user , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_PASSWORD ): neo4j_password ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch ()","title":"Neo4jCsvPublisher"},{"location":"databuilder/#elasticsearchpublisher","text":"Elasticsearch Publisher uses Bulk API to load data from JSON file. Elasticsearch publisher supports atomic operation by utilizing alias in Elasticsearch. A new index is created and data is uploaded into it. After the upload is complete, index alias is swapped to point to new index from old index and traffic is routed to new index. tmp_folder = '/var/tmp/amundsen/dummy_metadata' node_files_folder = ' {tmp_folder} /nodes/' . format ( tmp_folder = tmp_folder ) relationship_files_folder = ' {tmp_folder} /relationships/' . format ( tmp_folder = tmp_folder ) job_config = ConfigFactory . from_dict ({ 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): data_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_PATH_CONFIG_KEY ): data_file_path , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_MODE_CONFIG_KEY ): 'r' , 'publisher.elasticsearch {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_CLIENT_CONFIG_KEY ): elasticsearch_client , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_NEW_INDEX_CONFIG_KEY ): elasticsearch_new_index , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_DOC_TYPE_CONFIG_KEY ): elasticsearch_doc_type , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_ALIAS_CONFIG_KEY ): elasticsearch_index_alias ,) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch ()","title":"ElasticsearchPublisher"},{"location":"databuilder/#callback","text":"Callback interface is built upon a Observer pattern where the participant want to take any action when target\u2019s state changes. Publisher is the first one adopting Callback where registered Callback will be called either when publish succeeded or when publish failed. In order to register callback, Publisher provides register_call_back method. One use case is for Extractor that needs to commit when job is finished (e.g: Kafka). Having Extractor register a callback to Publisher to commit when publish is successful, extractor can safely commit by implementing commit logic into on_success method.","title":"Callback"},{"location":"databuilder/#rest-api-query","text":"Databuilder now has a generic REST API Query capability that can be joined each other. Most of the cases of extraction is currently from Database or Datawarehouse that is queryable via SQL. However, not all metadata sources provide our access to its Database and they mostly provide REST API to consume their metadata. The challenges come with REST API is that: there\u2019s no explicit standard in REST API. Here, we need to conform to majority of cases (HTTP call with JSON payload & response) but open for extension for different authentication scheme, and different way of pagination, etc. It is hardly the case that you would get what you want from one REST API call. It is usually the case that you need to snitch (JOIN) multiple REST API calls together to get the information you want. To solve this challenges, we introduce RestApiQuery RestAPIQuery is: 1. Assuming that REST API is using HTTP(S) call with GET method \u2013 RestAPIQuery intention\u2019s is read , not write \u2013 where basic HTTP auth is supported out of the box. There\u2019s extension point on other authentication scheme such as Oauth, and pagination, etc. 2. Usually, you want the subset of the response you get from the REST API call \u2013 value extraction. To extract the value you want, RestApiQuery uses JSONPath which is similar product as XPATH of XML. 3. You can JOIN multiple RestApiQuery together. More detail on JOIN operation in RestApiQuery: 1. It joins multiple RestApiQuery together by accepting prior RestApiQuery as a constructor \u2013 a Decorator pattern 2. In REST API, URL is the one that locates the resource we want. Here, JOIN simply means we need to find resource based on the identifier that other query\u2019s result has . In other words, when RestApiQuery forms URL, it uses previous query\u2019s result to compute the URL e.g: Previous record: {\"dashboard_id\": \"foo\"}, URL before: http://foo.bar/dashboard/{dashboard_id} URL after compute: http://foo.bar/dashboard/foo With this pattern RestApiQuery supports 1:1 and 1:N JOIN relationship. (GROUP BY or any other aggregation, sub-query join is not supported) To see in action, take a peek at ModeDashboardExtractor","title":"REST API Query"},{"location":"databuilder/#removing-stale-data-in-neo4j-neo4jstalenessremovaltask","text":"As Databuilder ingestion mostly consists of either INSERT OR UPDATE, there could be some stale data that has been removed from metadata source but still remains in Neo4j database. Neo4jStalenessRemovalTask basically detects staleness and removes it. In Neo4jCsvPublisher , it adds attributes \u201cpublished_tag\u201d and \u201cpublisher_last_updated_epoch_ms\u201d on every nodes and relations. You can use either of these two attributes to detect staleness and remove those stale node or relation from the database.","title":"Removing stale data in Neo4j -- Neo4jStalenessRemovalTask:"},{"location":"databuilder/#using-published_tag-to-remove-stale-data","text":"Use published_tag to remove stale data, when it is certain that non-matching tag is stale once all the ingestion is completed. For example, suppose that you use current date (or execution date in Airflow) as a published_tag , \u201c2020-03-31\u201d. Once Databuilder ingests all tables and all columns, all table nodes and column nodes should have published_tag as \u201c2020-03-31\u201d. It is safe to assume that table nodes and column nodes whose published_tag is different \u2013 such as \u201c2020-03-30\u201d or \u201c2020-02-10\u201d \u2013 means that it is deleted from the source metadata. You can use Neo4jStalenessRemovalTask to delete those stale data. 1 2 3 4 5 6 7 8 9 10 11 12 13 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_nodes': ['Table', 'Column'], 'task.remove_stale_data.job_publish_tag': '2020-03-31' } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch() Note that there\u2019s protection mechanism, staleness_max_pct , that protect your data being wiped out when something is clearly wrong. \u201c staleness_max_pct \u201d basically first measure the proportion of elements that will be deleted and if it exceeds threshold per type ( 10% on the configuration above ), the deletion won\u2019t be executed and the task aborts.","title":"Using \"published_tag\" to remove stale data"},{"location":"databuilder/#using-publisher_last_updated_epoch_ms-to-remove-stale-data","text":"You can think this approach as TTL based eviction. This is particularly useful when there are multiple ingestion pipelines and you cannot be sure when all ingestion is done. In this case, you might still can say that if specific node or relation has not been published past 3 days, it\u2019s stale data. 1 2 3 4 5 6 7 8 9 10 11 12 13 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_relations': ['READ', 'READ_BY'], 'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3 } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch() Above configuration is trying to delete stale usage relation (READ, READ_BY), by deleting READ or READ_BY relation that has not been published past 3 days. If number of elements to be removed is more than 10% per type, this task will be aborted without executing any deletion.","title":"Using \"publisher_last_updated_epoch_ms\" to remove stale data"},{"location":"databuilder/#dry-run","text":"Deletion is always scary and it\u2019s better to perform dryrun before put this into action. You can use Dry run to see what sort of Cypher query will be executed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_relations': ['READ', 'READ_BY'], 'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3 'task.remove_stale_data.dry_run': True } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch()","title":"Dry run"},{"location":"databuilder/CODE_OF_CONDUCT/","text":"This project is governed by Lyft\u2019s code of conduct . All contributors and participants agree to abide by its terms.","title":"CODE OF CONDUCT"},{"location":"databuilder/docs/models/","text":"Amundsen Models \u00b6 Overview \u00b6 These are the python classes that live in databuilder/models/ . Models represent the data structures that live in either neo4j (if the model extends Neo4jSerializable) or in elasticsearch. Models that extend Neo4jSerializable have methods to create: - the nodes - the relationships In this way, amundsendatabuilder pipelines can create python objects that can then be loaded into neo4j / elastic search without developers needing to know the internals of the neo4j schema. The Models \u00b6 TableMetadata \u00b6 python class What datasets does my org have? Description \u00b6 This corresponds to a dataset in amundsen and is the core building block. In addition to ColumnMetadata, tableMetadata is one of the first datasets you should extract as almost everything else depends on these being populated. Extraction \u00b6 In general, for Table and Column Metadata, you should be able to use one of the pre-made extractors in the extractor package Watermark \u00b6 python class What is the earliest data that this table has? What is the latest data? This is NOT the same as when the data was last updated. Description \u00b6 Corresponds to the earliest and latest date that a dataset has. Only makes sense if the dataset is timeseries data. For example, a given table may have data from 2019/01/01 -> 2020/01/01 In that case the low watermark is 2019/01/01 and the high watermark is 2020/01/01. Extraction \u00b6 Depending on the datastore of your dataset, you would extract this by: - a query on the minimum and maximum partition (hive) - a query for the minimum and maximum record of a given timestamp column ColumnUsageModel \u00b6 python class How many queries is a given column getting? By which users? Has query counts per a given column per a user. This can help identify Description \u00b6 who uses given datasets so people can contact them if they have questions on how to use a given dataset or if a dataset is changing. It is also used as a search boost so that the most used tables are put to the top of the search results. For more traditional databases, there should be system tables where you can obtain Extraction \u00b6 these sorts of usage statistics. In other cases, you may need to use audit logs which could require a custom solution. Finally, for none traditional data lakes, getting this information exactly maybe difficult and you may need to rely on a heuristic. User \u00b6 python class What users are there out there? Which team is this user on? Description \u00b6 Represents all of the metadata for a user at your company. This is required if you are going to be having authentication turned on. Extraction \u00b6 TODO TableColumnStats \u00b6 python class What are the min/max values for this column? How many nulls are in this column? * Description \u00b6 This represents statistics on the column level (this is not for table level metrics). The idea is that different companies will want to track different things about different columns, so this is highly customizable. It also will probably require a distributed cluster in order to calculate these regularly and in general is probably the least accessible metrics to get at without a custom solution. Extraction \u00b6 The idea here would be to implement something that does the following: For each table you care about: For each column you care about: Calculate statistics that you care about such as min/max/average etc. Application \u00b6 python class What job/application is writing to this table? * Description \u00b6 This is used to provide users a way to find out what job/application is responsible for writing to this dataset. Currently the model assumes the application has to be in airflow, but in theory it could be generalized to other orchestration frameworks. Extraction \u00b6 TODO Table Owner \u00b6 python class What team or user owns this dataset? * Description \u00b6 A dataset can have one or more owners. These owners are used when requesting table descriptions or could be just a useful point of contact for a user inquiring about how to use a dataset. Extraction \u00b6 Although the main point of entry for owners is through the WebUI, you could in theory extract this information based on who created a given table. Table Source \u00b6 python class Where is the source code for the application that writes to this dataset? * Description \u00b6 Generally there is going to be code that your company owns that describes how a dataset is created. This model is what represents the link and type of repository to this source code so it is available to users. Extraction \u00b6 You will need a github/gitlab/your repository crawler in order to populate this automatically. The idea there would be to search for a given table name or something else that is a unique identifier such that you can be confident that the source correctly matches to this table. TableLastUpdated \u00b6 python class When was the last time this data was updated? Is this table stale or deprecated? * Description \u00b6 This value is used to describe the last time the table had datapoints inserted into it. It is a very useful value as it can help users identify if there are tables that are no longer being updated. Extraction \u00b6 There are some extractors available for this like hive_table_last_updated_extractor that you can refer to. But you will need access to history that provides information on when the last data write happened on a given table. If this data isn\u2019t available for your data source, you maybe able to approximate it by looking at the max of some timestamp column.","title":"Models"},{"location":"databuilder/docs/models/#amundsen-models","text":"","title":"Amundsen Models"},{"location":"databuilder/docs/models/#overview","text":"These are the python classes that live in databuilder/models/ . Models represent the data structures that live in either neo4j (if the model extends Neo4jSerializable) or in elasticsearch. Models that extend Neo4jSerializable have methods to create: - the nodes - the relationships In this way, amundsendatabuilder pipelines can create python objects that can then be loaded into neo4j / elastic search without developers needing to know the internals of the neo4j schema.","title":"Overview"},{"location":"databuilder/docs/models/#the-models","text":"","title":"The Models"},{"location":"databuilder/docs/models/#tablemetadata","text":"python class What datasets does my org have?","title":"TableMetadata"},{"location":"databuilder/docs/models/#description","text":"This corresponds to a dataset in amundsen and is the core building block. In addition to ColumnMetadata, tableMetadata is one of the first datasets you should extract as almost everything else depends on these being populated.","title":"Description"},{"location":"databuilder/docs/models/#extraction","text":"In general, for Table and Column Metadata, you should be able to use one of the pre-made extractors in the extractor package","title":"Extraction"},{"location":"databuilder/docs/models/#watermark","text":"python class What is the earliest data that this table has? What is the latest data? This is NOT the same as when the data was last updated.","title":"Watermark"},{"location":"databuilder/docs/models/#description_1","text":"Corresponds to the earliest and latest date that a dataset has. Only makes sense if the dataset is timeseries data. For example, a given table may have data from 2019/01/01 -> 2020/01/01 In that case the low watermark is 2019/01/01 and the high watermark is 2020/01/01.","title":"Description"},{"location":"databuilder/docs/models/#extraction_1","text":"Depending on the datastore of your dataset, you would extract this by: - a query on the minimum and maximum partition (hive) - a query for the minimum and maximum record of a given timestamp column","title":"Extraction"},{"location":"databuilder/docs/models/#columnusagemodel","text":"python class How many queries is a given column getting? By which users? Has query counts per a given column per a user. This can help identify","title":"ColumnUsageModel"},{"location":"databuilder/docs/models/#description_2","text":"who uses given datasets so people can contact them if they have questions on how to use a given dataset or if a dataset is changing. It is also used as a search boost so that the most used tables are put to the top of the search results. For more traditional databases, there should be system tables where you can obtain","title":"Description"},{"location":"databuilder/docs/models/#extraction_2","text":"these sorts of usage statistics. In other cases, you may need to use audit logs which could require a custom solution. Finally, for none traditional data lakes, getting this information exactly maybe difficult and you may need to rely on a heuristic.","title":"Extraction"},{"location":"databuilder/docs/models/#user","text":"python class What users are there out there? Which team is this user on?","title":"User"},{"location":"databuilder/docs/models/#description_3","text":"Represents all of the metadata for a user at your company. This is required if you are going to be having authentication turned on.","title":"Description"},{"location":"databuilder/docs/models/#extraction_3","text":"TODO","title":"Extraction"},{"location":"databuilder/docs/models/#tablecolumnstats","text":"python class What are the min/max values for this column? How many nulls are in this column? *","title":"TableColumnStats"},{"location":"databuilder/docs/models/#description_4","text":"This represents statistics on the column level (this is not for table level metrics). The idea is that different companies will want to track different things about different columns, so this is highly customizable. It also will probably require a distributed cluster in order to calculate these regularly and in general is probably the least accessible metrics to get at without a custom solution.","title":"Description"},{"location":"databuilder/docs/models/#extraction_4","text":"The idea here would be to implement something that does the following: For each table you care about: For each column you care about: Calculate statistics that you care about such as min/max/average etc.","title":"Extraction"},{"location":"databuilder/docs/models/#application","text":"python class What job/application is writing to this table? *","title":"Application"},{"location":"databuilder/docs/models/#description_5","text":"This is used to provide users a way to find out what job/application is responsible for writing to this dataset. Currently the model assumes the application has to be in airflow, but in theory it could be generalized to other orchestration frameworks.","title":"Description"},{"location":"databuilder/docs/models/#extraction_5","text":"TODO","title":"Extraction"},{"location":"databuilder/docs/models/#table-owner","text":"python class What team or user owns this dataset? *","title":"Table Owner"},{"location":"databuilder/docs/models/#description_6","text":"A dataset can have one or more owners. These owners are used when requesting table descriptions or could be just a useful point of contact for a user inquiring about how to use a dataset.","title":"Description"},{"location":"databuilder/docs/models/#extraction_6","text":"Although the main point of entry for owners is through the WebUI, you could in theory extract this information based on who created a given table.","title":"Extraction"},{"location":"databuilder/docs/models/#table-source","text":"python class Where is the source code for the application that writes to this dataset? *","title":"Table Source"},{"location":"databuilder/docs/models/#description_7","text":"Generally there is going to be code that your company owns that describes how a dataset is created. This model is what represents the link and type of repository to this source code so it is available to users.","title":"Description"},{"location":"databuilder/docs/models/#extraction_7","text":"You will need a github/gitlab/your repository crawler in order to populate this automatically. The idea there would be to search for a given table name or something else that is a unique identifier such that you can be confident that the source correctly matches to this table.","title":"Extraction"},{"location":"databuilder/docs/models/#tablelastupdated","text":"python class When was the last time this data was updated? Is this table stale or deprecated? *","title":"TableLastUpdated"},{"location":"databuilder/docs/models/#description_8","text":"This value is used to describe the last time the table had datapoints inserted into it. It is a very useful value as it can help users identify if there are tables that are no longer being updated.","title":"Description"},{"location":"databuilder/docs/models/#extraction_8","text":"There are some extractors available for this like hive_table_last_updated_extractor that you can refer to. But you will need access to history that provides information on when the last data write happened on a given table. If this data isn\u2019t available for your data source, you maybe able to approximate it by looking at the max of some timestamp column.","title":"Extraction"},{"location":"frontend/","text":"Amundsen Frontend Service \u00b6 Amundsen is a metadata driven application for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as Google search for data. The project is named after Norwegian explorer Roald Amundsen , the first person to discover South Pole. The frontend service leverages a separate search service for allowing users to search for data resources, and a separate metadata service for viewing and editing metadata for a given resource. It is a Flask application with a React frontend. For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.6 Node = v10 or v12 npm >= 6.x.x Doc \u00b6 https://lyft.github.io/amundsen/ User Interface \u00b6 Please note that the mock images only served as demonstration purpose. Landing Page : The landing page for Amundsen including 1. search bars; 2. popular used tables; Search Preview : See inline search results as you type Table Detail Page : Visualization of a Hive / Redshift table Column detail : Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page : Visualization of table data preview which could integrate with Apache Superset Installation \u00b6 Please visit Installation guideline on how to install Amundsen. Configuration \u00b6 Please visit Configuration doc on how to configure Amundsen various enviroment settings(local vs production). Developer Guidelines \u00b6 Please visit Developer guidelines if you want to build Amundsen in your local environment. License \u00b6 Apache 2.0 License.","title":"Overview"},{"location":"frontend/#amundsen-frontend-service","text":"Amundsen is a metadata driven application for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as Google search for data. The project is named after Norwegian explorer Roald Amundsen , the first person to discover South Pole. The frontend service leverages a separate search service for allowing users to search for data resources, and a separate metadata service for viewing and editing metadata for a given resource. It is a Flask application with a React frontend. For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Frontend Service"},{"location":"frontend/#requirements","text":"Python >= 3.6 Node = v10 or v12 npm >= 6.x.x","title":"Requirements"},{"location":"frontend/#doc","text":"https://lyft.github.io/amundsen/","title":"Doc"},{"location":"frontend/#user-interface","text":"Please note that the mock images only served as demonstration purpose. Landing Page : The landing page for Amundsen including 1. search bars; 2. popular used tables; Search Preview : See inline search results as you type Table Detail Page : Visualization of a Hive / Redshift table Column detail : Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page : Visualization of table data preview which could integrate with Apache Superset","title":"User Interface"},{"location":"frontend/#installation","text":"Please visit Installation guideline on how to install Amundsen.","title":"Installation"},{"location":"frontend/#configuration","text":"Please visit Configuration doc on how to configure Amundsen various enviroment settings(local vs production).","title":"Configuration"},{"location":"frontend/#developer-guidelines","text":"Please visit Developer guidelines if you want to build Amundsen in your local environment.","title":"Developer Guidelines"},{"location":"frontend/#license","text":"Apache 2.0 License.","title":"License"},{"location":"frontend/CODE_OF_CONDUCT/","text":"This project is governed by Lyft\u2019s code of conduct . All contributors and participants agree to abide by its terms.","title":"CODE OF CONDUCT"},{"location":"frontend/docs/application_config/","text":"Application configuration \u00b6 This document describes how to leverage the frontend service\u2019s application configuration to configure particular features. After modifying the AppConfigCustom object in config-custom.ts in the ways described in this document, be sure to rebuild your application with these changes. NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document Badge Config \u00b6 Badges are a special type of tag that cannot be edited through the UI. BadgeConfig can be used to customize the text and color of badges. This config defines a mapping of badge name to a BadgeStyle and optional displayName . Badges that are not defined will default to use the BadgeStyle.default style and displayName use the badge name with any _ or - characters replaced with a space. Browse Tags Feature \u00b6 TODO: Please add doc Custom Logo \u00b6 Add your logo to the folder in amundsen_application/static/images/ . Set the the logoPath key on the to the location of your image. Date \u00b6 This config allows you to specify various date formats across the app. There are three date formats in use shown below. These correspond to the formatDate , formatDateTimeShort and formatDateTimeLong utility functions. 1 2 3 default: 'MMM DD, YYYY' dateTimeShort: 'MMM DD, YYYY ha z' dateTimeLong: 'MMMM Do YYYY [at] h:mm:ss a' Reference for formatting: https://devhints.io/datetime#momentjs-format Google Analytics \u00b6 TODO: Please add doc Index Users \u00b6 In Amundsen, users themselves are data resources and user metadata helps to facilitate network based discovery. When users are indexed they will show up in search results, and selecting a user surfaces a profile page that displays that user\u2019s relationships with different data resources. After ingesting user metadata into the search and metadata services, set IndexUsersConfig.enabled to true on the application configuration to display the UI for the aforementioned features. Mail Client Features \u00b6 Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. As these are optional features, our MailClientFeaturesConfig can be used to hide/display any UI related to these features: 1. Set MailClientFeaturesConfig.feedbackEnabled to true in order to display the Feedback component in the UI. 2. Set MailClientFeaturesConfig.notificationsEnabled to true in order to display the optional UI for users to request more information about resources on the TableDetail page. For information about how to configure a custom mail client, please see this entry in our flask configuration doc. Navigation Links \u00b6 TODO: Please add doc Resource Configurations \u00b6 This configuration drives resource specific aspects of the application\u2019s user interface. Each supported resource should be mapped to an object that matches or extends the BaseResourceConfig . Base Configuration \u00b6 All resource configurations must match or extend the BaseResourceConfig . This configuration supports the following options: 1. displayName : The name displayed throughout the application to refer to this resource type. 2. filterCategories : An optional FilterConfig object. When set for a given resource, that resource will display filter options in the search page UI. Filter Categories \u00b6 The FilterConfig is an array of objects that match any of the supported filter options. We currently support a MultiSelectFilterCategory and a SingleFilterCategory . See our config-types for more information about each option. Table Configuration \u00b6 TableResourceConfig extends BaseResourceConfig with a supportedDatabases option. This can be used for the following customizations: Custom Icons \u00b6 You can configure custom icons to be used throughout the UI when representing datasets from particular sources/databases. On the TableResourceConfig.supportedDatabases object, add an entry with the id used to reference that database and map to an object that specifies the iconClass for that database. This iconClass should be defined in icons.scss . Display Names \u00b6 You can configure a specific display name to be used throughout the UI when representing datasets from particular sources/databases. On the TableResourceConfig.supportedDatabases object, add an entry with the id used to reference that database and map to an object that specified the displayName for that database. Table Lineage \u00b6 TODO: Please add doc Table Profile \u00b6 TODO: Please add doc* Issue Tracking Features \u00b6 In order to enable Issue Tracking set IssueTrackingConfig.enabled to true to see UI features. Further configuration is required to fully enable the feature, please see this entry","title":"Application Config"},{"location":"frontend/docs/application_config/#application-configuration","text":"This document describes how to leverage the frontend service\u2019s application configuration to configure particular features. After modifying the AppConfigCustom object in config-custom.ts in the ways described in this document, be sure to rebuild your application with these changes. NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document","title":"Application configuration"},{"location":"frontend/docs/application_config/#badge-config","text":"Badges are a special type of tag that cannot be edited through the UI. BadgeConfig can be used to customize the text and color of badges. This config defines a mapping of badge name to a BadgeStyle and optional displayName . Badges that are not defined will default to use the BadgeStyle.default style and displayName use the badge name with any _ or - characters replaced with a space.","title":"Badge Config"},{"location":"frontend/docs/application_config/#browse-tags-feature","text":"TODO: Please add doc","title":"Browse Tags Feature"},{"location":"frontend/docs/application_config/#custom-logo","text":"Add your logo to the folder in amundsen_application/static/images/ . Set the the logoPath key on the to the location of your image.","title":"Custom Logo"},{"location":"frontend/docs/application_config/#date","text":"This config allows you to specify various date formats across the app. There are three date formats in use shown below. These correspond to the formatDate , formatDateTimeShort and formatDateTimeLong utility functions. 1 2 3 default: 'MMM DD, YYYY' dateTimeShort: 'MMM DD, YYYY ha z' dateTimeLong: 'MMMM Do YYYY [at] h:mm:ss a' Reference for formatting: https://devhints.io/datetime#momentjs-format","title":"Date"},{"location":"frontend/docs/application_config/#google-analytics","text":"TODO: Please add doc","title":"Google Analytics"},{"location":"frontend/docs/application_config/#index-users","text":"In Amundsen, users themselves are data resources and user metadata helps to facilitate network based discovery. When users are indexed they will show up in search results, and selecting a user surfaces a profile page that displays that user\u2019s relationships with different data resources. After ingesting user metadata into the search and metadata services, set IndexUsersConfig.enabled to true on the application configuration to display the UI for the aforementioned features.","title":"Index Users"},{"location":"frontend/docs/application_config/#mail-client-features","text":"Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. As these are optional features, our MailClientFeaturesConfig can be used to hide/display any UI related to these features: 1. Set MailClientFeaturesConfig.feedbackEnabled to true in order to display the Feedback component in the UI. 2. Set MailClientFeaturesConfig.notificationsEnabled to true in order to display the optional UI for users to request more information about resources on the TableDetail page. For information about how to configure a custom mail client, please see this entry in our flask configuration doc.","title":"Mail Client Features"},{"location":"frontend/docs/application_config/#navigation-links","text":"TODO: Please add doc","title":"Navigation Links"},{"location":"frontend/docs/application_config/#resource-configurations","text":"This configuration drives resource specific aspects of the application\u2019s user interface. Each supported resource should be mapped to an object that matches or extends the BaseResourceConfig .","title":"Resource Configurations"},{"location":"frontend/docs/application_config/#base-configuration","text":"All resource configurations must match or extend the BaseResourceConfig . This configuration supports the following options: 1. displayName : The name displayed throughout the application to refer to this resource type. 2. filterCategories : An optional FilterConfig object. When set for a given resource, that resource will display filter options in the search page UI.","title":"Base Configuration"},{"location":"frontend/docs/application_config/#filter-categories","text":"The FilterConfig is an array of objects that match any of the supported filter options. We currently support a MultiSelectFilterCategory and a SingleFilterCategory . See our config-types for more information about each option.","title":"Filter Categories"},{"location":"frontend/docs/application_config/#table-configuration","text":"TableResourceConfig extends BaseResourceConfig with a supportedDatabases option. This can be used for the following customizations:","title":"Table Configuration"},{"location":"frontend/docs/application_config/#custom-icons","text":"You can configure custom icons to be used throughout the UI when representing datasets from particular sources/databases. On the TableResourceConfig.supportedDatabases object, add an entry with the id used to reference that database and map to an object that specifies the iconClass for that database. This iconClass should be defined in icons.scss .","title":"Custom Icons"},{"location":"frontend/docs/application_config/#display-names","text":"You can configure a specific display name to be used throughout the UI when representing datasets from particular sources/databases. On the TableResourceConfig.supportedDatabases object, add an entry with the id used to reference that database and map to an object that specified the displayName for that database.","title":"Display Names"},{"location":"frontend/docs/application_config/#table-lineage","text":"TODO: Please add doc","title":"Table Lineage"},{"location":"frontend/docs/application_config/#table-profile","text":"TODO: Please add doc*","title":"Table Profile"},{"location":"frontend/docs/application_config/#issue-tracking-features","text":"In order to enable Issue Tracking set IssueTrackingConfig.enabled to true to see UI features. Further configuration is required to fully enable the feature, please see this entry","title":"Issue Tracking Features"},{"location":"frontend/docs/configuration/","text":"Configuration \u00b6 Flask \u00b6 The default Flask application uses a LocalConfig that looks for the metadata and search services running on localhost. In order to use different end point, you need to create a custom config class suitable for your use case. Once the config class has been created, it can be referenced via the environment variable : FRONTEND_SVC_CONFIG_MODULE_CLASS For more examples of how to leverage the Flask configuration for specific features, please see this extended doc . For more information on Flask configurations, please reference the official Flask documentation . React Application \u00b6 Application Config \u00b6 Certain features of the React application import variables from an AppConfig object. The configuration can be customized by modifying config-custom.ts . For examples of how to leverage the application configuration for specific features, please see this extended doc . Custom Fonts & Styles \u00b6 Fonts and css variables can be customized by modifying fonts-custom.scss and variables-custom.scss . Python Entry Points \u00b6 The application also leverages python entry points for custom features. In your local setup.py , point the entry points detailed below to custom classes or methods that have to be implemented for a given feature. Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect. entry_points=\"\"\" [action_log.post_exec.plugin] analytic_clients_action_log = path.to.file:custom_action_log_method [preview_client] table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient [announcement_client] announcement_client_class = path.to.file:CustomAnnouncementClient \"\"\" Action Logging \u00b6 Create a custom method to handle action logging. Under the [action_log.post_exec.plugin] group, point the analytic_clients_action_log entry point in your local setup.py to that method. Preview Client \u00b6 Create a custom implementation of base_preview_client . Under the [preview_client] group, point the table_preview_client_class entry point in your local setup.py to that class. For those who use Apache Superset for data exploration, see this doc for how to implement a preview client for Superset. Announcement Client \u00b6 Create a custom implementation of base_announcement_client . Under the [announcement_client] group, point the announcement_client_class entry point in your local setup.py to that class. Currently Amundsen does not own the input and storage of announcements. Consider having the client fetch announcement information from an external web feed. Authentication \u00b6 Authentication can be hooked within Amundsen using either wrapper class or using proxy to secure the microservices on the nginx/server level. Following are the ways to setup the end-to-end authentication. - OIDC / Keycloak","title":"React Configuration"},{"location":"frontend/docs/configuration/#configuration","text":"","title":"Configuration"},{"location":"frontend/docs/configuration/#flask","text":"The default Flask application uses a LocalConfig that looks for the metadata and search services running on localhost. In order to use different end point, you need to create a custom config class suitable for your use case. Once the config class has been created, it can be referenced via the environment variable : FRONTEND_SVC_CONFIG_MODULE_CLASS For more examples of how to leverage the Flask configuration for specific features, please see this extended doc . For more information on Flask configurations, please reference the official Flask documentation .","title":"Flask"},{"location":"frontend/docs/configuration/#react-application","text":"","title":"React Application"},{"location":"frontend/docs/configuration/#application-config","text":"Certain features of the React application import variables from an AppConfig object. The configuration can be customized by modifying config-custom.ts . For examples of how to leverage the application configuration for specific features, please see this extended doc .","title":"Application Config"},{"location":"frontend/docs/configuration/#custom-fonts-styles","text":"Fonts and css variables can be customized by modifying fonts-custom.scss and variables-custom.scss .","title":"Custom Fonts &amp; Styles"},{"location":"frontend/docs/configuration/#python-entry-points","text":"The application also leverages python entry points for custom features. In your local setup.py , point the entry points detailed below to custom classes or methods that have to be implemented for a given feature. Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect. entry_points=\"\"\" [action_log.post_exec.plugin] analytic_clients_action_log = path.to.file:custom_action_log_method [preview_client] table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient [announcement_client] announcement_client_class = path.to.file:CustomAnnouncementClient \"\"\"","title":"Python Entry Points"},{"location":"frontend/docs/configuration/#action-logging","text":"Create a custom method to handle action logging. Under the [action_log.post_exec.plugin] group, point the analytic_clients_action_log entry point in your local setup.py to that method.","title":"Action Logging"},{"location":"frontend/docs/configuration/#preview-client","text":"Create a custom implementation of base_preview_client . Under the [preview_client] group, point the table_preview_client_class entry point in your local setup.py to that class. For those who use Apache Superset for data exploration, see this doc for how to implement a preview client for Superset.","title":"Preview Client"},{"location":"frontend/docs/configuration/#announcement-client","text":"Create a custom implementation of base_announcement_client . Under the [announcement_client] group, point the announcement_client_class entry point in your local setup.py to that class. Currently Amundsen does not own the input and storage of announcements. Consider having the client fetch announcement information from an external web feed.","title":"Announcement Client"},{"location":"frontend/docs/configuration/#authentication","text":"Authentication can be hooked within Amundsen using either wrapper class or using proxy to secure the microservices on the nginx/server level. Following are the ways to setup the end-to-end authentication. - OIDC / Keycloak","title":"Authentication"},{"location":"frontend/docs/developer_guide/","text":"Developer Guide \u00b6 Environment \u00b6 Follow the installation instructions in the section Install standalone application directly from the source . Install the javascript development requirements: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ cd static $ npm install --only = dev To test local changes to the javascript static files: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ cd static $ npm run dev-build # builds the development bundle To test local changes to the python files, re-run the wsgi: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ python3 wsgi.py Contributing \u00b6 Testing \u00b6 Python \u00b6 If changes were made to any python files, run the python unit tests, linter, and type checker. Unit tests are run with py.test . They are located in tests/unit . Type checks are run with mypy . Linting is flake8 . There are friendly make targets for each of these tests: # after setting up environment make test # unit tests in Python 3 make lint # flake8 make mypy # type checks Fix all errors before submitting a PR. JS Assets \u00b6 Type Checking \u00b6 npm run tsc conducts type checking. The build commands npm run build and npm run dev-build also conduct type checking, but are slower because they also build the source code. Run any of these commands and fix all failed checks before submitting a PR. Lint \u00b6 npm run lint runs the linter. Fix all lint errors before submitting a PR. npm run lint-fix can help auto-fix most common errors. Unit Tests \u00b6 npm run test runs unit tests. Add unit tests to cover new code additions and fix any test failures before submitting a PR. To run specific tests, run npm run test-nocov -t <regex> , where <regex> is any pattern that matches the names of the test blocks that you want to run. See our recommendations for writing unit tests here .","title":"FE Developer Guide"},{"location":"frontend/docs/developer_guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"frontend/docs/developer_guide/#environment","text":"Follow the installation instructions in the section Install standalone application directly from the source . Install the javascript development requirements: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ cd static $ npm install --only = dev To test local changes to the javascript static files: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ cd static $ npm run dev-build # builds the development bundle To test local changes to the python files, re-run the wsgi: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ python3 wsgi.py","title":"Environment"},{"location":"frontend/docs/developer_guide/#contributing","text":"","title":"Contributing"},{"location":"frontend/docs/developer_guide/#testing","text":"","title":"Testing"},{"location":"frontend/docs/developer_guide/#python","text":"If changes were made to any python files, run the python unit tests, linter, and type checker. Unit tests are run with py.test . They are located in tests/unit . Type checks are run with mypy . Linting is flake8 . There are friendly make targets for each of these tests: # after setting up environment make test # unit tests in Python 3 make lint # flake8 make mypy # type checks Fix all errors before submitting a PR.","title":"Python"},{"location":"frontend/docs/developer_guide/#js-assets","text":"","title":"JS Assets"},{"location":"frontend/docs/developer_guide/#type-checking","text":"npm run tsc conducts type checking. The build commands npm run build and npm run dev-build also conduct type checking, but are slower because they also build the source code. Run any of these commands and fix all failed checks before submitting a PR.","title":"Type Checking"},{"location":"frontend/docs/developer_guide/#lint","text":"npm run lint runs the linter. Fix all lint errors before submitting a PR. npm run lint-fix can help auto-fix most common errors.","title":"Lint"},{"location":"frontend/docs/developer_guide/#unit-tests","text":"npm run test runs unit tests. Add unit tests to cover new code additions and fix any test failures before submitting a PR. To run specific tests, run npm run test-nocov -t <regex> , where <regex> is any pattern that matches the names of the test blocks that you want to run. See our recommendations for writing unit tests here .","title":"Unit Tests"},{"location":"frontend/docs/flask_config/","text":"Flask configuration \u00b6 After modifying any variable in config.py described in this document, be sure to rebuild your application with these changes. NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document Custom Routes \u00b6 In order to add any custom Flask endpoints to Amundsen\u2019s frontend application, configure a function on the INIT_CUSTOM_ROUTES variable. This function takes the created Flask application and can leverage Flask\u2019s add_url_rule method to add custom routes. Example: Setting INIT_CUSTOM_ROUTES to the init_custom_routes method below will expose a /custom_route endpoint on the frontend application. def init_custom_routes ( app: Flask ) -> None: app.add_url_rule ( '/custom_route' , 'custom_route' , custom_route ) def custom_route () : pass Mail Client Features \u00b6 Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. For these features a custom implementation of base_mail_client must be mapped to the MAIL_CLIENT configuration variable. To fully enable these features in the UI, the application configuration variables for these features must also be set to true. Please see this entry in our application configuration doc for further information. Issue Tracking Integration Features \u00b6 Amundsen has a feature to allow display of associated tickets within the table detail view. The feature both displays open tickets and allows users to report new tickets associated with the table. These tickets must contain the table_uri within the ticket text in order to be displayed; the table_uri is automatically added to tickets created via the feature. Tickets are displayed from most recent to oldest, and currently only open tickets are displayed. Currently only JIRA is supported. The UI must also be enabled to use this feature, please see configuration notes here . There are several configuration settings in config.py that should be set in order to use this feature. Here are the settings and what they should be set to ISSUE_TRACKER_URL = None # type: str (Your JIRA environment, IE 'https://jira.net') ISSUE_TRACKER_USER = None # type: str (Recommended to be a service account) ISSUE_TRACKER_PASSWORD = None # type: str ISSUE_TRACKER_PROJECT_ID = None # type: int (Project ID for the project you would like JIRA tickets to be created in) ISSUE_TRACKER_CLIENT = None # type: str (Fully qualified class name and path) ISSUE_TRACKER_CLIENT_ENABLED = False # type: bool (Enabling the feature, must be set to True) ISSUE_TRACKER_MAX_RESULTS = None # type: int (Max issues to display at a time) Programmatic Descriptions \u00b6 Amundsen supports configuring other mark down supported non-editable description boxes on the table page. This can be useful if you have multiple writers which want to write different pieces of information to amundsen that are either very company specific and thus would never be directly integrated into amundsen or require long form text to properly convey the information. What are some more specific examples of what could be used for this? - You have an existing process that generates quality reports for a dataset that you want to embed in the table page. - You have a process that detects pii information (also adding the appropriate tag/badge) but also generates a simple report to provide context. - You have extended table information that is applicable to your datastore which you want to scrape and provide in the table page Programmatic Descriptions are referred to by a \u201cdescription source\u201d which is a unique identifier. You can then configure the descriptions to have a custom order in the config.py file like so: PROGRAMMATIC_DISPLAY = { \"s3_crawler\": { \"display_order\": 0 }, \"quality_service\": { \"display_order\": 1 }, \"doesnt_exist\": { \"display_order\": 2 } } description sources not mentioned in the configuration will be alphabetically placed at the end of the above list. If PROGRAMMATIC_DISPLAY is left at None all added fields are still showing up, so that display is entirely dynamically data-driven without configuration. Meaning configuration merely adds the (nice) benefit of setting display order. Here is a screenshot of what it would look like in the bottom left here:","title":"Flask Configuration"},{"location":"frontend/docs/flask_config/#flask-configuration","text":"After modifying any variable in config.py described in this document, be sure to rebuild your application with these changes. NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document","title":"Flask configuration"},{"location":"frontend/docs/flask_config/#custom-routes","text":"In order to add any custom Flask endpoints to Amundsen\u2019s frontend application, configure a function on the INIT_CUSTOM_ROUTES variable. This function takes the created Flask application and can leverage Flask\u2019s add_url_rule method to add custom routes. Example: Setting INIT_CUSTOM_ROUTES to the init_custom_routes method below will expose a /custom_route endpoint on the frontend application. def init_custom_routes ( app: Flask ) -> None: app.add_url_rule ( '/custom_route' , 'custom_route' , custom_route ) def custom_route () : pass","title":"Custom Routes"},{"location":"frontend/docs/flask_config/#mail-client-features","text":"Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. For these features a custom implementation of base_mail_client must be mapped to the MAIL_CLIENT configuration variable. To fully enable these features in the UI, the application configuration variables for these features must also be set to true. Please see this entry in our application configuration doc for further information.","title":"Mail Client Features"},{"location":"frontend/docs/flask_config/#issue-tracking-integration-features","text":"Amundsen has a feature to allow display of associated tickets within the table detail view. The feature both displays open tickets and allows users to report new tickets associated with the table. These tickets must contain the table_uri within the ticket text in order to be displayed; the table_uri is automatically added to tickets created via the feature. Tickets are displayed from most recent to oldest, and currently only open tickets are displayed. Currently only JIRA is supported. The UI must also be enabled to use this feature, please see configuration notes here . There are several configuration settings in config.py that should be set in order to use this feature. Here are the settings and what they should be set to ISSUE_TRACKER_URL = None # type: str (Your JIRA environment, IE 'https://jira.net') ISSUE_TRACKER_USER = None # type: str (Recommended to be a service account) ISSUE_TRACKER_PASSWORD = None # type: str ISSUE_TRACKER_PROJECT_ID = None # type: int (Project ID for the project you would like JIRA tickets to be created in) ISSUE_TRACKER_CLIENT = None # type: str (Fully qualified class name and path) ISSUE_TRACKER_CLIENT_ENABLED = False # type: bool (Enabling the feature, must be set to True) ISSUE_TRACKER_MAX_RESULTS = None # type: int (Max issues to display at a time)","title":"Issue Tracking Integration Features"},{"location":"frontend/docs/flask_config/#programmatic-descriptions","text":"Amundsen supports configuring other mark down supported non-editable description boxes on the table page. This can be useful if you have multiple writers which want to write different pieces of information to amundsen that are either very company specific and thus would never be directly integrated into amundsen or require long form text to properly convey the information. What are some more specific examples of what could be used for this? - You have an existing process that generates quality reports for a dataset that you want to embed in the table page. - You have a process that detects pii information (also adding the appropriate tag/badge) but also generates a simple report to provide context. - You have extended table information that is applicable to your datastore which you want to scrape and provide in the table page Programmatic Descriptions are referred to by a \u201cdescription source\u201d which is a unique identifier. You can then configure the descriptions to have a custom order in the config.py file like so: PROGRAMMATIC_DISPLAY = { \"s3_crawler\": { \"display_order\": 0 }, \"quality_service\": { \"display_order\": 1 }, \"doesnt_exist\": { \"display_order\": 2 } } description sources not mentioned in the configuration will be alphabetically placed at the end of the above list. If PROGRAMMATIC_DISPLAY is left at None all added fields are still showing up, so that display is entirely dynamically data-driven without configuration. Meaning configuration merely adds the (nice) benefit of setting display order. Here is a screenshot of what it would look like in the bottom left here:","title":"Programmatic Descriptions"},{"location":"frontend/docs/installation/","text":"Installation \u00b6 Install standalone application directly from the source \u00b6 The following instructions are for setting up a standalone version of the Amundsen application. This approach is ideal for local development. # Clone repo $ git clone https://github.com/lyft/amundsenfrontendlibrary.git # Build static content $ cd amundsenfrontendlibrary/amundsen_application/static $ npm install $ npm run build # or npm run dev-build for un-minified source $ cd ../../ # Install python resources $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -r requirements.txt $ python3 setup.py install # Start server $ python3 amundsen_application/wsgi.py # visit http://localhost:5000 to confirm the application is running You should now have the application running at http://localhost:5000 , but will notice that there is no data and interactions will throw errors. The next step is to connect the standalone application to make calls to the search and metadata services. 1. Setup a local copy of the metadata service using the instructions found here . 2. Setup a local copy of the search service using the instructions found here . 3. Modify the LOCAL_HOST , METADATA_PORT , and SEARCH_PORT variables in the LocalConfig to point to where your local metadata and search services are running, and restart the application with $ python3 amundsen_application/wsgi.py","title":"FE Installation Guide"},{"location":"frontend/docs/installation/#installation","text":"","title":"Installation"},{"location":"frontend/docs/installation/#install-standalone-application-directly-from-the-source","text":"The following instructions are for setting up a standalone version of the Amundsen application. This approach is ideal for local development. # Clone repo $ git clone https://github.com/lyft/amundsenfrontendlibrary.git # Build static content $ cd amundsenfrontendlibrary/amundsen_application/static $ npm install $ npm run build # or npm run dev-build for un-minified source $ cd ../../ # Install python resources $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -r requirements.txt $ python3 setup.py install # Start server $ python3 amundsen_application/wsgi.py # visit http://localhost:5000 to confirm the application is running You should now have the application running at http://localhost:5000 , but will notice that there is no data and interactions will throw errors. The next step is to connect the standalone application to make calls to the search and metadata services. 1. Setup a local copy of the metadata service using the instructions found here . 2. Setup a local copy of the search service using the instructions found here . 3. Modify the LOCAL_HOST , METADATA_PORT , and SEARCH_PORT variables in the LocalConfig to point to where your local metadata and search services are running, and restart the application with $ python3 amundsen_application/wsgi.py","title":"Install standalone application directly from the source"},{"location":"frontend/docs/recommended_practices/","text":"Recommended Practices \u00b6 This document serves as reference for current practices and patterns that we want to standardize across Amundsen\u2019s frontend application code. Below, we provide some high-level guidelines targeted towards new contributors or any contributor who does not yet have domain knowledge in a particular framework or core library. This document is not intended to provide an exhaustive checklist for completing certain tasks. We aim to maintain a reasonably consistent code base through these practices and welcome PRs to update and improve these recommendations. Application \u00b6 Unit Testing \u00b6 We use Jest as our test framework. We leverage utility methods from Enzyme to test React components, and use redux-saga-test-plan to test our redux-saga middleware logic. General \u00b6 Leverage TypeScript to prevent bugs in unit tests and ensure that code is tested with inputs that match the defined interfaces and types. Adding and updating test fixtures helps to provide re-useable pieces of typed test data or mock implementations for this purpose. Leverage beforeAll() / beforeEach() for test setup when applicable. Leverage afterAll() / afterEach for test teardown when applicable to remove any side effects of the test block. For example if a mock implementation of a method was created in beforeAll() , the original implementation should be restored in afterAll() . See Jest\u2019s setup-teardown documentation for further understanding. Use descriptive title strings. To assist with debugging we should be able to understand what a test is checking for and under what conditions. Become familiar with the variety of Jest matchers that are available. Understanding the nuances of different matchers and the cases they are each ideal for assists with writing more robust tests. For example, there are many different ways to verify objects and the best matcher to use will depend on what exactly we are testing for. Examples: If asserting that inputObject is assigned to variable x , asserting the equivalence of x using .toBe() creates a more robust test for this case because .toBe() will verify that the variable is actually referencing the given object. Contrast this to a matcher like .toEqual() which will verify whether or not the object happens to have a particular set of properties and values. In this case using .toEqual() would risk hiding bugs where x is not actually referencing inputObject as expected, yet happens to have the same key value pairs perhaps due to side effects in the code. If asserting that outputObject matches expectedObject , asserting the equivalence of each property on outputObject using .toBe() or asserting the equality of the two objects using .toMatchObject() is useful when we only care that certain values exist on outputObject . However if it matters that certain values do not exist on outputObject \u2013 as is the case with reducer outputs \u2013 .toEqual() is a more robust alternative as it compares all properties on both objects for equivalence. When testing logic that makes use of JavaScript\u2019s Date object, note that our Jest scripts are configured to run in the UTC timezone. Developers should either: Mock the Date object and its methods\u2019 return values, and run assertions based on the mock values. Create assertions knowing that the unit test suite will run as if we are in the UTC timezone. Code coverage is important to track but it only informs us of what code was actually run and executed during the test. The onus is on the developer to focus on use case coverage and make sure that right assertions are run so that all logic is adequately tested. React \u00b6 Enzyme provides 3 different utilities for rendering React components for testing. We recommend using shallow rendering to start off. If a component has a use case that requires full DOM rendering, those cases will become apparent. See Enzyme\u2019s api documentation to read more about the recommendations for each option. Create a re-useable setup() function that will take any arguments needed to test conditional logic. Look for opportunities to organize tests a way such that one setup() can be used to test assertions that occur under the same conditions. For example, a test block for a method that has no conditional logic should only have one setup() . However, it is not recommended to share a setup() result across tests for different methods, or across tests for a method that has a dependency on a mutable piece of state. The reason is that we risk propagating side effects from one test block to another. Consider refactoring components or other files if they become burdensome to test. Potential options include (but are not limited to): Create subcomponents for large components. This is also especially useful for reducing the burden of updating tests when component layouts are changed. Break down large functions into smaller functions. Unit test the logic of the smaller functions individually, and mock their results when testing the larger function. Export constants from a separate file for hardcoded values and import them into the relevant source files and test files. This is especially helpful for strings. Redux \u00b6 Because the majority of Redux code consists of functions, we unit test those methods as usual and ensure the functions produce the expected output for any given input. See Redux\u2019s documentation on testing action creators , async action creators , and reducers , or check out examples in our code. Unless an action creator includes any logic other than returning the action, unit testing the reducer and saga middleware logic is sufficient and provides the most value. redux-saga generator functions can be tested by iterating through it step-by-step and running assertions at each step, or by executing the entire saga and running assertions on the side effects. See redux-saga\u2019s documentation on testing sagas for a wider breadth of examples.","title":"Recommended Practices"},{"location":"frontend/docs/recommended_practices/#recommended-practices","text":"This document serves as reference for current practices and patterns that we want to standardize across Amundsen\u2019s frontend application code. Below, we provide some high-level guidelines targeted towards new contributors or any contributor who does not yet have domain knowledge in a particular framework or core library. This document is not intended to provide an exhaustive checklist for completing certain tasks. We aim to maintain a reasonably consistent code base through these practices and welcome PRs to update and improve these recommendations.","title":"Recommended Practices"},{"location":"frontend/docs/recommended_practices/#application","text":"","title":"Application"},{"location":"frontend/docs/recommended_practices/#unit-testing","text":"We use Jest as our test framework. We leverage utility methods from Enzyme to test React components, and use redux-saga-test-plan to test our redux-saga middleware logic.","title":"Unit Testing"},{"location":"frontend/docs/recommended_practices/#general","text":"Leverage TypeScript to prevent bugs in unit tests and ensure that code is tested with inputs that match the defined interfaces and types. Adding and updating test fixtures helps to provide re-useable pieces of typed test data or mock implementations for this purpose. Leverage beforeAll() / beforeEach() for test setup when applicable. Leverage afterAll() / afterEach for test teardown when applicable to remove any side effects of the test block. For example if a mock implementation of a method was created in beforeAll() , the original implementation should be restored in afterAll() . See Jest\u2019s setup-teardown documentation for further understanding. Use descriptive title strings. To assist with debugging we should be able to understand what a test is checking for and under what conditions. Become familiar with the variety of Jest matchers that are available. Understanding the nuances of different matchers and the cases they are each ideal for assists with writing more robust tests. For example, there are many different ways to verify objects and the best matcher to use will depend on what exactly we are testing for. Examples: If asserting that inputObject is assigned to variable x , asserting the equivalence of x using .toBe() creates a more robust test for this case because .toBe() will verify that the variable is actually referencing the given object. Contrast this to a matcher like .toEqual() which will verify whether or not the object happens to have a particular set of properties and values. In this case using .toEqual() would risk hiding bugs where x is not actually referencing inputObject as expected, yet happens to have the same key value pairs perhaps due to side effects in the code. If asserting that outputObject matches expectedObject , asserting the equivalence of each property on outputObject using .toBe() or asserting the equality of the two objects using .toMatchObject() is useful when we only care that certain values exist on outputObject . However if it matters that certain values do not exist on outputObject \u2013 as is the case with reducer outputs \u2013 .toEqual() is a more robust alternative as it compares all properties on both objects for equivalence. When testing logic that makes use of JavaScript\u2019s Date object, note that our Jest scripts are configured to run in the UTC timezone. Developers should either: Mock the Date object and its methods\u2019 return values, and run assertions based on the mock values. Create assertions knowing that the unit test suite will run as if we are in the UTC timezone. Code coverage is important to track but it only informs us of what code was actually run and executed during the test. The onus is on the developer to focus on use case coverage and make sure that right assertions are run so that all logic is adequately tested.","title":"General"},{"location":"frontend/docs/recommended_practices/#react","text":"Enzyme provides 3 different utilities for rendering React components for testing. We recommend using shallow rendering to start off. If a component has a use case that requires full DOM rendering, those cases will become apparent. See Enzyme\u2019s api documentation to read more about the recommendations for each option. Create a re-useable setup() function that will take any arguments needed to test conditional logic. Look for opportunities to organize tests a way such that one setup() can be used to test assertions that occur under the same conditions. For example, a test block for a method that has no conditional logic should only have one setup() . However, it is not recommended to share a setup() result across tests for different methods, or across tests for a method that has a dependency on a mutable piece of state. The reason is that we risk propagating side effects from one test block to another. Consider refactoring components or other files if they become burdensome to test. Potential options include (but are not limited to): Create subcomponents for large components. This is also especially useful for reducing the burden of updating tests when component layouts are changed. Break down large functions into smaller functions. Unit test the logic of the smaller functions individually, and mock their results when testing the larger function. Export constants from a separate file for hardcoded values and import them into the relevant source files and test files. This is especially helpful for strings.","title":"React"},{"location":"frontend/docs/recommended_practices/#redux","text":"Because the majority of Redux code consists of functions, we unit test those methods as usual and ensure the functions produce the expected output for any given input. See Redux\u2019s documentation on testing action creators , async action creators , and reducers , or check out examples in our code. Unless an action creator includes any logic other than returning the action, unit testing the reducer and saga middleware logic is sufficient and provides the most value. redux-saga generator functions can be tested by iterating through it step-by-step and running assertions at each step, or by executing the entire saga and running assertions on the side effects. See redux-saga\u2019s documentation on testing sagas for a wider breadth of examples.","title":"Redux"},{"location":"frontend/docs/authentication/oidc/","text":"See this doc in our main repository for information on how to set up end-to-end authentication using OIDC.","title":"Oidc"},{"location":"frontend/docs/examples/superset_preview_client/","text":"Overview \u00b6 Amundsen\u2019s data preview feature requires that developers create a custom implementation of base_preview_client for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search. Apache Superset is an open-source business intelligence tool that can be used for data exploration. Amundsen\u2019s data preview feature was created with Superset in mind, and it is what we leverage internally at Lyft to support the feature. This document provides some insight into how to configure Amundsen\u2019s frontend application to leverage Superset for data previews. Implementation \u00b6 Implement the base_superset_preview_client to make a request to an instance of Superset. Shared Logic \u00b6 base_superset_preview_client implements get_preview_data() of base_preview_client with the minimal logic for this use case. It updates the headers for the request if optionalHeaders are passed in get_preview_data() # Clone headers so that it does not mutate instance's state headers = dict(self.headers) # Merge optionalHeaders into headers if optionalHeaders is not None: headers.update(optionalHeaders) It verifies the shape of the data before returning it to the application. If the data does not match the PreviewDataSchema , the request will fail. # Verify and return the results response_dict = response.json() columns = [ColumnItem(c['name'], c['type']) for c in response_dict['columns']] preview_data = PreviewData(columns, response_dict['data']) data, errors = PreviewDataSchema().dump(preview_data) if not errors: payload = jsonify({'preview_data': data}) return make_response(payload, response.status_code) else: return make_response(jsonify({'preview_data': {}}), HTTPStatus.INTERNAL_SERVER_ERROR) Custom Logic \u00b6 base_superset_preview_client has an abstract method post_to_sql_json() . This method will contain whatever custom logic is needed to make a successful request to the sql_json enpoint based on the protections you have configured on this endpoint on your instance of Superset. For example, this may be where you have to append other values to the headers, or generate SQL queries based on your use case. See the following example_superset_preview_client for an example implementation of base_superset_preview_client and post_to_sql_json() . This example assumes a local instance of Superset running on port 8088 with no security, authentication, or authorization configured on the endpoint. Usage \u00b6 Under the [preview_client] group, point the table_preview_client_class entry point in your local setup.py to your custom class. entry_points=\"\"\" ... [preview_client] table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient \"\"\" Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect","title":"Preview Client Setup"},{"location":"frontend/docs/examples/superset_preview_client/#overview","text":"Amundsen\u2019s data preview feature requires that developers create a custom implementation of base_preview_client for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search. Apache Superset is an open-source business intelligence tool that can be used for data exploration. Amundsen\u2019s data preview feature was created with Superset in mind, and it is what we leverage internally at Lyft to support the feature. This document provides some insight into how to configure Amundsen\u2019s frontend application to leverage Superset for data previews.","title":"Overview"},{"location":"frontend/docs/examples/superset_preview_client/#implementation","text":"Implement the base_superset_preview_client to make a request to an instance of Superset.","title":"Implementation"},{"location":"frontend/docs/examples/superset_preview_client/#shared-logic","text":"base_superset_preview_client implements get_preview_data() of base_preview_client with the minimal logic for this use case. It updates the headers for the request if optionalHeaders are passed in get_preview_data() # Clone headers so that it does not mutate instance's state headers = dict(self.headers) # Merge optionalHeaders into headers if optionalHeaders is not None: headers.update(optionalHeaders) It verifies the shape of the data before returning it to the application. If the data does not match the PreviewDataSchema , the request will fail. # Verify and return the results response_dict = response.json() columns = [ColumnItem(c['name'], c['type']) for c in response_dict['columns']] preview_data = PreviewData(columns, response_dict['data']) data, errors = PreviewDataSchema().dump(preview_data) if not errors: payload = jsonify({'preview_data': data}) return make_response(payload, response.status_code) else: return make_response(jsonify({'preview_data': {}}), HTTPStatus.INTERNAL_SERVER_ERROR)","title":"Shared Logic"},{"location":"frontend/docs/examples/superset_preview_client/#custom-logic","text":"base_superset_preview_client has an abstract method post_to_sql_json() . This method will contain whatever custom logic is needed to make a successful request to the sql_json enpoint based on the protections you have configured on this endpoint on your instance of Superset. For example, this may be where you have to append other values to the headers, or generate SQL queries based on your use case. See the following example_superset_preview_client for an example implementation of base_superset_preview_client and post_to_sql_json() . This example assumes a local instance of Superset running on port 8088 with no security, authentication, or authorization configured on the endpoint.","title":"Custom Logic"},{"location":"frontend/docs/examples/superset_preview_client/#usage","text":"Under the [preview_client] group, point the table_preview_client_class entry point in your local setup.py to your custom class. entry_points=\"\"\" ... [preview_client] table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient \"\"\" Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect","title":"Usage"},{"location":"installation-aws-ecs/aws-ecs-deployment/","text":"Deployment of non-production Amundsen on AWS ECS using aws-cli \u00b6 The following is a set of instructions to run Amundsen on AWS Elastic Container Service. The current configuration is very basic but it is working. It is a migration of the docker-amundsen.yml to run on AWS ECS. Install ECS CLI \u00b6 The first step is to install ECS CLI, please follow the instructions from AWS documentation Get your access and secret keys from IAM \u00b6 # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ export AWS_ACCESS_KEY_ID = xxxxxxxx $ export AWS_SECRET_ACCESS_KEY = xxxxxx $ export AWS_PROFILE = profilename For the purpose of this instruction we used the tutorial on AWS documentation STEP 1: Create a cluster configuration: \u00b6 # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli configure --cluster amundsen --region us-west-2 --default-launch-type EC2 --config-name amundsen STEP 2: Create a profile using your access key and secret key: \u00b6 # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli configure profile --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --profile-name amundsen STEP 3: Create the Cluster Use profile name from \\~/.aws/credentials \u00b6 # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli up --keypair JoaoCorreia --extra-user-data userData.sh --capability-iam --size 1 --instance-type t2.large --cluster-config amundsen --verbose --force --aws-profile $AWS_PROFILE STEP 4: Deploy the Compose File to a Cluster \u00b6 # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli compose --cluster-config amundsen --file docker-ecs-amundsen.yml up --create-log-groups You can use the ECS CLI to see what tasks are running. $ ecs-cli ps STEP 5 Open the EC2 Instance \u00b6 Edit the Security Group to allow traffic to your IP, you should be able to see the frontend, elasticsearch and neo4j by visiting the URLs: http://xxxxxxx:5000/ http://xxxxxxx:9200/ http://xxxxxxx:7474/browser/ TODO \u00b6 Configuration sent to services not working properly (amunsen.db vs graph.db) Create a persistent volume for graph/metadata storage. See this Refactor the VPC and default security group permissions","title":"AWS ECS Installation"},{"location":"installation-aws-ecs/aws-ecs-deployment/#deployment-of-non-production-amundsen-on-aws-ecs-using-aws-cli","text":"The following is a set of instructions to run Amundsen on AWS Elastic Container Service. The current configuration is very basic but it is working. It is a migration of the docker-amundsen.yml to run on AWS ECS.","title":"Deployment of non-production Amundsen on AWS ECS using aws-cli"},{"location":"installation-aws-ecs/aws-ecs-deployment/#install-ecs-cli","text":"The first step is to install ECS CLI, please follow the instructions from AWS documentation","title":"Install ECS CLI"},{"location":"installation-aws-ecs/aws-ecs-deployment/#get-your-access-and-secret-keys-from-iam","text":"# in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ export AWS_ACCESS_KEY_ID = xxxxxxxx $ export AWS_SECRET_ACCESS_KEY = xxxxxx $ export AWS_PROFILE = profilename For the purpose of this instruction we used the tutorial on AWS documentation","title":"Get your access and secret keys from IAM"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-1-create-a-cluster-configuration","text":"# in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli configure --cluster amundsen --region us-west-2 --default-launch-type EC2 --config-name amundsen","title":"STEP 1: Create a cluster configuration:"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-2-create-a-profile-using-your-access-key-and-secret-key","text":"# in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli configure profile --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --profile-name amundsen","title":"STEP 2: Create a profile using your access key and secret key:"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-3-create-the-cluster-use-profile-name-from-awscredentials","text":"# in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli up --keypair JoaoCorreia --extra-user-data userData.sh --capability-iam --size 1 --instance-type t2.large --cluster-config amundsen --verbose --force --aws-profile $AWS_PROFILE","title":"STEP 3: Create the Cluster Use profile name from \\~/.aws/credentials"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-4-deploy-the-compose-file-to-a-cluster","text":"# in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli compose --cluster-config amundsen --file docker-ecs-amundsen.yml up --create-log-groups You can use the ECS CLI to see what tasks are running. $ ecs-cli ps","title":"STEP 4: Deploy the Compose File to a Cluster"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-5-open-the-ec2-instance","text":"Edit the Security Group to allow traffic to your IP, you should be able to see the frontend, elasticsearch and neo4j by visiting the URLs: http://xxxxxxx:5000/ http://xxxxxxx:9200/ http://xxxxxxx:7474/browser/","title":"STEP 5 Open the EC2 Instance"},{"location":"installation-aws-ecs/aws-ecs-deployment/#todo","text":"Configuration sent to services not working properly (amunsen.db vs graph.db) Create a persistent volume for graph/metadata storage. See this Refactor the VPC and default security group permissions","title":"TODO"},{"location":"metadata/","text":"Amundsen Metadata Service \u00b6 Amundsen Metadata service serves Restful API and is responsible for providing and also updating metadata, such as table & column description, and tags. Metadata service can use Neo4j or Apache Atlas as a persistent layer. For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.7 Doc \u00b6 https://lyft.github.io/amundsen/ Instructions to start the Metadata service from distribution \u00b6 $ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install amundsen-metadata $ python3 metadata_service/metadata_wsgi.py -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck Instructions to start the Metadata service from the source \u00b6 $ git clone https://github.com/lyft/amundsenmetadatalibrary.git $ cd amundsenmetadatalibrary $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 metadata_service/metadata_wsgi.py -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck Instructions to start the service from Docker \u00b6 $ docker pull amundsendev/amundsen-metadata:latest $ docker run -p 5002 :5002 amundsendev/amundsen-metadata # - alternative, for production environment with Gunicorn (see its homepage link below) $ ## docker run -p 5002:5002 amundsendev/amundsen-metadata gunicorn --bind 0.0.0.0:5002 metadata_service.metadata_wsgi -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck Production environment \u00b6 By default, Flask comes with Werkzeug webserver, which is for development. For production environment use production grade web server such as Gunicorn . $ pip install gunicorn $ gunicorn metadata_service.metadata_wsgi Here is documentation of gunicorn configuration. Configuration outside local environment \u00b6 By default, Metadata service uses LocalConfig that looks for Neo4j running in localhost. In order to use different end point, you need to create Config suitable for your use case. Once config class has been created, it can be referenced by environment variable : METADATA_SVC_CONFIG_MODULE_CLASS For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in metadata_service.config module. then you can set as below: METADATA_SVC_CONFIG_MODULE_CLASS=metadata_service.config.ProdConfig This way Metadata service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc . Apache Atlas \u00b6 Amundsen Metadata service can use Apache Atlas as a backend. Some of the benefits of using Apache Atlas instead of Neo4j is that Apache Atlas offers plugins to several services (e.g. Apache Hive, Apache Spark) that allow for push based updates. It also allows to set policies on what metadata is accesible and editable by means of Apache Ranger. If you would like to use Apache Atlas as a backend for Metadata service you will need to create a Config as mentioned above. Make sure to include the following: PROXY_CLIENT = PROXY_CLIENTS [ 'ATLAS' ] # or env PROXY_CLIENT='ATLAS' PROXY_PORT = 21000 # or env PROXY_PORT PROXY_USER = 'atlasuser' # or env CREDENTIALS_PROXY_USER PROXY_PASSWORD = 'password' # or env CREDENTIALS_PROXY_PASSWORD To start the service with Atlas from Docker. Make sure you have atlasserver configured in DNS (or docker-compose) $ docker run -p 5002 :5002 --env PROXY_CLIENT = ATLAS --env PROXY_PORT = 21000 --env PROXY_HOST = atlasserver --env CREDENTIALS_PROXY_USER = atlasuser --env CREDENTIALS_PROXY_PASSWORD = password amundsen-metadata:latest NOTE The support for Apache Atlas is work in progress. For example, while Apache Atlas supports fine grained access, Amundsen does not support this yet. Developer guide \u00b6 Code style \u00b6 PEP 8: Amundsen Metadata service follows PEP8 - Style Guide for Python Code . Typing hints: Amundsen Metadata service also utilizes Typing hint for better readability. API documentation \u00b6 We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger. When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5002/apidocs/. Currently the documentation only works with local configuration. Code structure \u00b6 Please visit Code Structure to read how different modules are structured in Amundsen Metadata service.","title":"Overview"},{"location":"metadata/#amundsen-metadata-service","text":"Amundsen Metadata service serves Restful API and is responsible for providing and also updating metadata, such as table & column description, and tags. Metadata service can use Neo4j or Apache Atlas as a persistent layer. For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Metadata Service"},{"location":"metadata/#requirements","text":"Python >= 3.7","title":"Requirements"},{"location":"metadata/#doc","text":"https://lyft.github.io/amundsen/","title":"Doc"},{"location":"metadata/#instructions-to-start-the-metadata-service-from-distribution","text":"$ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install amundsen-metadata $ python3 metadata_service/metadata_wsgi.py -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck","title":"Instructions to start the Metadata service from distribution"},{"location":"metadata/#instructions-to-start-the-metadata-service-from-the-source","text":"$ git clone https://github.com/lyft/amundsenmetadatalibrary.git $ cd amundsenmetadatalibrary $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 metadata_service/metadata_wsgi.py -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck","title":"Instructions to start the Metadata service from the source"},{"location":"metadata/#instructions-to-start-the-service-from-docker","text":"$ docker pull amundsendev/amundsen-metadata:latest $ docker run -p 5002 :5002 amundsendev/amundsen-metadata # - alternative, for production environment with Gunicorn (see its homepage link below) $ ## docker run -p 5002:5002 amundsendev/amundsen-metadata gunicorn --bind 0.0.0.0:5002 metadata_service.metadata_wsgi -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck","title":"Instructions to start the service from Docker"},{"location":"metadata/#production-environment","text":"By default, Flask comes with Werkzeug webserver, which is for development. For production environment use production grade web server such as Gunicorn . $ pip install gunicorn $ gunicorn metadata_service.metadata_wsgi Here is documentation of gunicorn configuration.","title":"Production environment"},{"location":"metadata/#configuration-outside-local-environment","text":"By default, Metadata service uses LocalConfig that looks for Neo4j running in localhost. In order to use different end point, you need to create Config suitable for your use case. Once config class has been created, it can be referenced by environment variable : METADATA_SVC_CONFIG_MODULE_CLASS For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in metadata_service.config module. then you can set as below: METADATA_SVC_CONFIG_MODULE_CLASS=metadata_service.config.ProdConfig This way Metadata service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc .","title":"Configuration outside local environment"},{"location":"metadata/#apache-atlas","text":"Amundsen Metadata service can use Apache Atlas as a backend. Some of the benefits of using Apache Atlas instead of Neo4j is that Apache Atlas offers plugins to several services (e.g. Apache Hive, Apache Spark) that allow for push based updates. It also allows to set policies on what metadata is accesible and editable by means of Apache Ranger. If you would like to use Apache Atlas as a backend for Metadata service you will need to create a Config as mentioned above. Make sure to include the following: PROXY_CLIENT = PROXY_CLIENTS [ 'ATLAS' ] # or env PROXY_CLIENT='ATLAS' PROXY_PORT = 21000 # or env PROXY_PORT PROXY_USER = 'atlasuser' # or env CREDENTIALS_PROXY_USER PROXY_PASSWORD = 'password' # or env CREDENTIALS_PROXY_PASSWORD To start the service with Atlas from Docker. Make sure you have atlasserver configured in DNS (or docker-compose) $ docker run -p 5002 :5002 --env PROXY_CLIENT = ATLAS --env PROXY_PORT = 21000 --env PROXY_HOST = atlasserver --env CREDENTIALS_PROXY_USER = atlasuser --env CREDENTIALS_PROXY_PASSWORD = password amundsen-metadata:latest NOTE The support for Apache Atlas is work in progress. For example, while Apache Atlas supports fine grained access, Amundsen does not support this yet.","title":"Apache Atlas"},{"location":"metadata/#developer-guide","text":"","title":"Developer guide"},{"location":"metadata/#code-style","text":"PEP 8: Amundsen Metadata service follows PEP8 - Style Guide for Python Code . Typing hints: Amundsen Metadata service also utilizes Typing hint for better readability.","title":"Code style"},{"location":"metadata/#api-documentation","text":"We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger. When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5002/apidocs/. Currently the documentation only works with local configuration.","title":"API documentation"},{"location":"metadata/#code-structure","text":"Please visit Code Structure to read how different modules are structured in Amundsen Metadata service.","title":"Code structure"},{"location":"metadata/CODE_OF_CONDUCT/","text":"This project is governed by Lyft\u2019s code of conduct . All contributors and participants agree to abide by its terms.","title":"CODE OF CONDUCT"},{"location":"metadata/docs/configurations/","text":"Most of the configurations are set through Flask Config Class . USER_DETAIL_METHOD OPTIONAL \u00b6 This is a method that can be used to get the user details from any third-party or custom system. This custom function takes user_id as a parameter, and returns a tuple consisting user details defined in UserSchema along with the status code. Example: def get_user_details ( user_id ): from amundsen_common.models.user import UserSchema from http import HTTPStatus user_info = { 'email' : 'test@email.com' , 'user_id' : user_id , 'first_name' : 'Firstname' , 'last_name' : 'Lastname' , 'full_name' : 'Firstname Lastname' , } return UserSchema () . dump ( user_info ) . data , HTTPStatus . OK USER_DETAIL_METHOD = get_user_details","title":"Overview"},{"location":"metadata/docs/configurations/#user_detail_method-optional","text":"This is a method that can be used to get the user details from any third-party or custom system. This custom function takes user_id as a parameter, and returns a tuple consisting user details defined in UserSchema along with the status code. Example: def get_user_details ( user_id ): from amundsen_common.models.user import UserSchema from http import HTTPStatus user_info = { 'email' : 'test@email.com' , 'user_id' : user_id , 'first_name' : 'Firstname' , 'last_name' : 'Lastname' , 'full_name' : 'Firstname Lastname' , } return UserSchema () . dump ( user_info ) . data , HTTPStatus . OK USER_DETAIL_METHOD = get_user_details","title":"USER_DETAIL_METHOD OPTIONAL"},{"location":"metadata/docs/structure/","text":"Amundsen metadata service consists of three packages, API, Entity, and Proxy. API package \u00b6 A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here . Proxy package \u00b6 Proxy package contains proxy modules that talks dependencies of Metadata service. There are currently three modules in Proxy package, Neo4j , Statsd and Atlas Selecting the appropriate proxy (Neo4j or Atlas) is configurable using a config variable PROXY_CLIENT , which takes the path to class name of proxy module available here . Note: Proxy\u2019s host and port are configured using config variables PROXY_HOST and PROXY_PORT respectively. Both of these variables can be set using environment variables. Neo4j proxy module \u00b6 Neo4j proxy module serves various use case of getting metadata or updating metadata from or into Neo4j. Most of the methods have Cypher query for the use case, execute the query and transform into entity . Apache Atlas proxy module \u00b6 Apache Atlas proxy module serves all of the metadata from Apache Atlas, using pyatlasclient . More information on how to setup Apache Atlas to make it compatible with Amundsen can be found here Statsd utilities module \u00b6 Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Metadata service configuration . For specific configuration related to statsd, you can configure it through environment variable. Entity package \u00b6 Entity package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Metadata service use classes in Entity to ensure validity of itself and improve readability and mainatability. Configurations \u00b6 There are different settings you might want to change depending on the application environment like toggling the debug mode, setting the proxy, and other such environment-specific things.","title":"Metadata API Structure"},{"location":"metadata/docs/structure/#api-package","text":"A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here .","title":"API package"},{"location":"metadata/docs/structure/#proxy-package","text":"Proxy package contains proxy modules that talks dependencies of Metadata service. There are currently three modules in Proxy package, Neo4j , Statsd and Atlas Selecting the appropriate proxy (Neo4j or Atlas) is configurable using a config variable PROXY_CLIENT , which takes the path to class name of proxy module available here . Note: Proxy\u2019s host and port are configured using config variables PROXY_HOST and PROXY_PORT respectively. Both of these variables can be set using environment variables.","title":"Proxy package"},{"location":"metadata/docs/structure/#neo4j-proxy-module","text":"Neo4j proxy module serves various use case of getting metadata or updating metadata from or into Neo4j. Most of the methods have Cypher query for the use case, execute the query and transform into entity .","title":"Neo4j proxy module"},{"location":"metadata/docs/structure/#apache-atlas-proxy-module","text":"Apache Atlas proxy module serves all of the metadata from Apache Atlas, using pyatlasclient . More information on how to setup Apache Atlas to make it compatible with Amundsen can be found here","title":"Apache Atlas proxy module"},{"location":"metadata/docs/structure/#statsd-utilities-module","text":"Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Metadata service configuration . For specific configuration related to statsd, you can configure it through environment variable.","title":"Statsd utilities module"},{"location":"metadata/docs/structure/#entity-package","text":"Entity package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Metadata service use classes in Entity to ensure validity of itself and improve readability and mainatability.","title":"Entity package"},{"location":"metadata/docs/structure/#configurations","text":"There are different settings you might want to change depending on the application environment like toggling the debug mode, setting the proxy, and other such environment-specific things.","title":"Configurations"},{"location":"metadata/docs/proxy/atlas_proxy/","text":"Atlas Proxy \u00b6 In order to make the Atlas-Amundsen integration smooth, we\u2019ve released a python package, amundsenatlastypes that has all the required entity definitions along with helper functions needed to make Atlas compatible with Amundsen. Usage and Installation of amundsenatlastypes can be found here Configurations \u00b6 Once you are done with setting up required entity definitions using amundsenatlastypes , you are all set to use Atlas with Amundsen. Other things to configure: Popular Tables","title":"Overview"},{"location":"metadata/docs/proxy/atlas_proxy/#atlas-proxy","text":"In order to make the Atlas-Amundsen integration smooth, we\u2019ve released a python package, amundsenatlastypes that has all the required entity definitions along with helper functions needed to make Atlas compatible with Amundsen. Usage and Installation of amundsenatlastypes can be found here","title":"Atlas Proxy"},{"location":"metadata/docs/proxy/atlas_proxy/#configurations","text":"Once you are done with setting up required entity definitions using amundsenatlastypes , you are all set to use Atlas with Amundsen. Other things to configure: Popular Tables","title":"Configurations"},{"location":"metadata/docs/proxy/gremlin/","text":"Gremlin Proxy \u00b6 What the heck is Gremlin? Why is it named Gremlin? \u00b6 Gremin is the graph traversal language of Apache TinkerPop . Why not Gremlin? Documentation \u00b6 The docs linked from Gremin are a good start. For example, the Getting Started and the PRACTICAL GREMLIN book How to target a new Gremlin backend \u00b6 This is not an exhaustive list, but some issues we\u2019ve found along the way: - Are there restricted property names? For example JanusGraph does not allow a property named key , so the base Gremlin proxy has a property named key_property_name which is set to _key for JanusGraph but key for others. - Is there database management required? For example AWS Neptune does now allow explicit creation of indexes, nor assigning data types to properties, but JanusGraph does and practically requires the creation of indexes. - Are there restrictions on the methods? For example, JanusGraph accepts any of the Java or Groovy names, but Neptune accepts a strict subset. JanusGraph can install any script engine, e.g. to allow Python lambdas but Neptune only allows Groovy lambdas. Other differences between Janusgraph and Neptune can be found here: https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html","title":"Gremlin Backend"},{"location":"metadata/docs/proxy/gremlin/#gremlin-proxy","text":"","title":"Gremlin Proxy"},{"location":"metadata/docs/proxy/gremlin/#what-the-heck-is-gremlin-why-is-it-named-gremlin","text":"Gremin is the graph traversal language of Apache TinkerPop . Why not Gremlin?","title":"What the heck is Gremlin?  Why is it named Gremlin?"},{"location":"metadata/docs/proxy/gremlin/#documentation","text":"The docs linked from Gremin are a good start. For example, the Getting Started and the PRACTICAL GREMLIN book","title":"Documentation"},{"location":"metadata/docs/proxy/gremlin/#how-to-target-a-new-gremlin-backend","text":"This is not an exhaustive list, but some issues we\u2019ve found along the way: - Are there restricted property names? For example JanusGraph does not allow a property named key , so the base Gremlin proxy has a property named key_property_name which is set to _key for JanusGraph but key for others. - Is there database management required? For example AWS Neptune does now allow explicit creation of indexes, nor assigning data types to properties, but JanusGraph does and practically requires the creation of indexes. - Are there restrictions on the methods? For example, JanusGraph accepts any of the Java or Groovy names, but Neptune accepts a strict subset. JanusGraph can install any script engine, e.g. to allow Python lambdas but Neptune only allows Groovy lambdas. Other differences between Janusgraph and Neptune can be found here: https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html","title":"How to target a new Gremlin backend"},{"location":"metadata/docs/proxy/neptune/","text":"Neptune \u00b6 Documentation \u00b6 In particular, see Gremlin differences , and Gremlin sessions . And any time you see docs from Kelvin (like the PRACTICAL GREMLIN book or lots of stackoverflow) pay attention, he works for AWS on Neptune. IAM authentication \u00b6 The gremlin transport is usually websockets, and the requests-aws4auth library we use elsewhere is for requests, which does not support websockets at all. So we rolled our in aws4authwebsocket . The saving grace of websockets and IAM is that the IAM authentication really only applies to the initialization request and the rest of the data flows over the existing TCP connection. The usual gremlin-python transport is Tornado, which was a huge pain to try and insinuate the aws4 autentication in to, so we use the websockets-client library instead. How to get a gremlin console for AWS \u00b6 They have pretty decent recipe here","title":"Neptune Backend"},{"location":"metadata/docs/proxy/neptune/#neptune","text":"","title":"Neptune"},{"location":"metadata/docs/proxy/neptune/#documentation","text":"In particular, see Gremlin differences , and Gremlin sessions . And any time you see docs from Kelvin (like the PRACTICAL GREMLIN book or lots of stackoverflow) pay attention, he works for AWS on Neptune.","title":"Documentation"},{"location":"metadata/docs/proxy/neptune/#iam-authentication","text":"The gremlin transport is usually websockets, and the requests-aws4auth library we use elsewhere is for requests, which does not support websockets at all. So we rolled our in aws4authwebsocket . The saving grace of websockets and IAM is that the IAM authentication really only applies to the initialization request and the rest of the data flows over the existing TCP connection. The usual gremlin-python transport is Tornado, which was a huge pain to try and insinuate the aws4 autentication in to, so we use the websockets-client library instead.","title":"IAM authentication"},{"location":"metadata/docs/proxy/neptune/#how-to-get-a-gremlin-console-for-aws","text":"They have pretty decent recipe here","title":"How to get a gremlin console for AWS"},{"location":"metadata/docs/proxy/atlas/popular_tables/","text":"Popular Tables Configurations \u00b6 The required entity definitions for Atlas can be applied using amundsenatlastypes . Popular Tables \u00b6 Amundsen has a concept of popular tables, which is a default entry point of the application for now. Popular Tables API leverages popularityScore attribute of Table super type to enable custom sorting strategy. The suggested formula to generate the popularity score is provided below and should be applied by the external script or batch/stream process to update Atlas entities accordingly. Popularity score = number of distinct readers * log(total number of reads) Table entity definition with popularityScore attribute amundsenatlastypes==1.0.2 . { \"entityDefs\" : [ { \"name\" : \"Table\" , \"superTypes\" : [ \"DataSet\" ], \"attributeDefs\" : [ { \"name\" : \"popularityScore\" , \"typeName\" : \"float\" , \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"isUnique\" : false , \"isIndexable\" : false , \"defaultValue\" : \"0.0\" }, { \"name\" : \"readers\" , \"typeName\" : \"array<Reader>\" , \"isOptional\" : true , \"cardinality\" : \"LIST\" , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false } ] } ] }","title":"Popular Table"},{"location":"metadata/docs/proxy/atlas/popular_tables/#popular-tables-configurations","text":"The required entity definitions for Atlas can be applied using amundsenatlastypes .","title":"Popular Tables Configurations"},{"location":"metadata/docs/proxy/atlas/popular_tables/#popular-tables","text":"Amundsen has a concept of popular tables, which is a default entry point of the application for now. Popular Tables API leverages popularityScore attribute of Table super type to enable custom sorting strategy. The suggested formula to generate the popularity score is provided below and should be applied by the external script or batch/stream process to update Atlas entities accordingly. Popularity score = number of distinct readers * log(total number of reads) Table entity definition with popularityScore attribute amundsenatlastypes==1.0.2 . { \"entityDefs\" : [ { \"name\" : \"Table\" , \"superTypes\" : [ \"DataSet\" ], \"attributeDefs\" : [ { \"name\" : \"popularityScore\" , \"typeName\" : \"float\" , \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"isUnique\" : false , \"isIndexable\" : false , \"defaultValue\" : \"0.0\" }, { \"name\" : \"readers\" , \"typeName\" : \"array<Reader>\" , \"isOptional\" : true , \"cardinality\" : \"LIST\" , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false } ] } ] }","title":"Popular Tables"},{"location":"search/","text":"Amundsen Search Service \u00b6 Amundsen Search service serves a Restful API and is responsible for searching metadata. The service leverages Elasticsearch for most of it\u2019s search capabilites. For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.6 elasticsearch 6.x (currently it doesn\u2019t support 7.x) Doc \u00b6 https://lyft.github.io/amundsen/ Instructions to start the Search service from distribution \u00b6 $ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install amundsensearch $ python3 search_service/search_wsgi.py # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck Instructions to start the Search service from source \u00b6 $ git clone https://github.com/lyft/amundsensearchlibrary.git $ cd amundsensearchlibrary $ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 search_service/search_wsgi.py # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck Instructions to start the service from Docker \u00b6 $ docker pull amundsendev/amundsen-search:latest $ docker run -p 5001 :5001 amundsendev/amundsen-search # - alternative, for production environment with Gunicorn (see its homepage link below) $ ## docker run -p 5001:5001 amundsendev/amundsen-search gunicorn --bind 0.0.0.0:5001 search_service.search_wsgi # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck Production environment \u00b6 By default, Flask comes with a Werkzeug webserver, which is used for development. For production environments a production grade web server such as Gunicorn should be used. $ pip3 install gunicorn $ gunicorn search_service.search_wsgi # In a different terminal, verify the service is up by running $ curl -v http://localhost:8000/healthcheck For more imformation see the Gunicorn configuration documentation . Configuration outside local environment \u00b6 By default, Search service uses LocalConfig that looks for Elasticsearch running in localhost. In order to use different end point, you need to create a Config suitable for your use case. Once a config class has been created, it can be referenced by an environment variable : SEARCH_SVC_CONFIG_MODULE_CLASS For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in search_service.config module. then you can set as below: SEARCH_SVC_CONFIG_MODULE_CLASS=search_service.config.ProdConfig This way Search service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc . Developer guide \u00b6 Code style \u00b6 PEP 8: Amundsen Search service follows PEP8 - Style Guide for Python Code . Typing hints: Amundsen Search service also utilizes Typing hint for better readability. API documentation \u00b6 We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger . When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5001/apidocs/ . Currently the documentation only works with local configuration. Code structure \u00b6 Amundsen Search service consists of three packages, API, Models, and Proxy. API package \u00b6 A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here . Proxy package \u00b6 Proxy package contains proxy modules that talks dependencies of Search service. There are currently two modules in Proxy package, Elasticsearch and Statsd . Elasticsearch proxy module \u00b6 Elasticsearch proxy module serves various use case of searching metadata from Elasticsearch. It uses Query DSL for the use case, execute the search query and transform into model . Atlas proxy module \u00b6 Apache Atlas proxy module uses Atlas to serve the Atlas requests. At the moment the search DSL REST api is used via the Python Client . Statsd utilities module \u00b6 Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Search service configuration . For specific configuration related to statsd, you can configure it through environment variable. Models package \u00b6 Models package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Search service use classes in Models to ensure validity of itself and improve readability and maintainability.","title":"Overview"},{"location":"search/#amundsen-search-service","text":"Amundsen Search service serves a Restful API and is responsible for searching metadata. The service leverages Elasticsearch for most of it\u2019s search capabilites. For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Search Service"},{"location":"search/#requirements","text":"Python >= 3.6 elasticsearch 6.x (currently it doesn\u2019t support 7.x)","title":"Requirements"},{"location":"search/#doc","text":"https://lyft.github.io/amundsen/","title":"Doc"},{"location":"search/#instructions-to-start-the-search-service-from-distribution","text":"$ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install amundsensearch $ python3 search_service/search_wsgi.py # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck","title":"Instructions to start the Search service from distribution"},{"location":"search/#instructions-to-start-the-search-service-from-source","text":"$ git clone https://github.com/lyft/amundsensearchlibrary.git $ cd amundsensearchlibrary $ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 search_service/search_wsgi.py # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck","title":"Instructions to start the Search service from source"},{"location":"search/#instructions-to-start-the-service-from-docker","text":"$ docker pull amundsendev/amundsen-search:latest $ docker run -p 5001 :5001 amundsendev/amundsen-search # - alternative, for production environment with Gunicorn (see its homepage link below) $ ## docker run -p 5001:5001 amundsendev/amundsen-search gunicorn --bind 0.0.0.0:5001 search_service.search_wsgi # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck","title":"Instructions to start the service from Docker"},{"location":"search/#production-environment","text":"By default, Flask comes with a Werkzeug webserver, which is used for development. For production environments a production grade web server such as Gunicorn should be used. $ pip3 install gunicorn $ gunicorn search_service.search_wsgi # In a different terminal, verify the service is up by running $ curl -v http://localhost:8000/healthcheck For more imformation see the Gunicorn configuration documentation .","title":"Production environment"},{"location":"search/#configuration-outside-local-environment","text":"By default, Search service uses LocalConfig that looks for Elasticsearch running in localhost. In order to use different end point, you need to create a Config suitable for your use case. Once a config class has been created, it can be referenced by an environment variable : SEARCH_SVC_CONFIG_MODULE_CLASS For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in search_service.config module. then you can set as below: SEARCH_SVC_CONFIG_MODULE_CLASS=search_service.config.ProdConfig This way Search service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc .","title":"Configuration outside local environment"},{"location":"search/#developer-guide","text":"","title":"Developer guide"},{"location":"search/#code-style","text":"PEP 8: Amundsen Search service follows PEP8 - Style Guide for Python Code . Typing hints: Amundsen Search service also utilizes Typing hint for better readability.","title":"Code style"},{"location":"search/#api-documentation","text":"We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger . When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5001/apidocs/ . Currently the documentation only works with local configuration.","title":"API documentation"},{"location":"search/#code-structure","text":"Amundsen Search service consists of three packages, API, Models, and Proxy.","title":"Code structure"},{"location":"search/#api-package","text":"A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here .","title":"API package"},{"location":"search/#proxy-package","text":"Proxy package contains proxy modules that talks dependencies of Search service. There are currently two modules in Proxy package, Elasticsearch and Statsd .","title":"Proxy package"},{"location":"search/#elasticsearch-proxy-module","text":"Elasticsearch proxy module serves various use case of searching metadata from Elasticsearch. It uses Query DSL for the use case, execute the search query and transform into model .","title":"Elasticsearch proxy module"},{"location":"search/#atlas-proxy-module","text":"Apache Atlas proxy module uses Atlas to serve the Atlas requests. At the moment the search DSL REST api is used via the Python Client .","title":"Atlas proxy module"},{"location":"search/#statsd-utilities-module","text":"Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Search service configuration . For specific configuration related to statsd, you can configure it through environment variable.","title":"Statsd utilities module"},{"location":"search/#models-package","text":"Models package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Search service use classes in Models to ensure validity of itself and improve readability and maintainability.","title":"Models package"},{"location":"search/CODE_OF_CONDUCT/","text":"This project is governed by Lyft\u2019s code of conduct . All contributors and participants agree to abide by its terms.","title":"CODE OF CONDUCT"},{"location":"search/docs/atlas-search/","text":"Atlas search investigation \u00b6 There are several approaches to integrate searching within Apache Atlas , we describe multiple options below: Use REST API\u2019s Directly using the Atlas API\u2019s is quick to implement and easy to setup for administrators. Atlas uses a search engine underwater (embedded Solr) to perform search queries, thus in theory this method should scale up. Disadvantages are that we are limited to the REST api that Atlas offers, we could potentially add functionality via pull requests and extend the search capabilities. The advanced search provides a DSL which contains basic forms of aggregation and arithmetic. Use Data Builder to fill Elasticsearch from Atlas Adopting Atlas within the Data Builder to fill Elasticsearch is a relatively straightforward way of staying compatible with the Neo4j database. It could either be pulling data from Atlas or being pushed by Kafka. This method requires a setup of Elasticsearch and Airflow, which increases the amount of infrastructure and maintenance. Another disadvantage is that with a big inflow of metadata this method might not scale as well as the other methods. Use underlying Solr or Elasticsearch from Apache Atlas Within Atlas there is the possibility to open up either Solr or the experimental Elasticsearch. It depends on janusgraph (the behind the scenes graph database) which populates the search engine. Therefore the search engine would not be compatible with the data builder setup. Adoption of such a search engine would require either new queries, some kind of transformer within the search engine, or changes within Atlas itself. Discussion \u00b6 Both the REST API approach and the data builder approach can be implemented and be configurable. Both approaches have their own benefits, the data builder together provides a more fine-tuned search whereas the Atlas REST API comes out of the box with Atlas. The last approach of using the underlying search engine from Atlas provides direct access to all the meta data with a decent search API. However, integration would be less straight forward as the indexes would differ from the data builders search engine loader. The focus is initially to implement the REST API approach and afterwards potentially implement an Atlas data extractor and importer within the Amundsen Data Builder. So that administrators have more flexibility in combining data sources.","title":"Atlas Backend"},{"location":"search/docs/atlas-search/#atlas-search-investigation","text":"There are several approaches to integrate searching within Apache Atlas , we describe multiple options below: Use REST API\u2019s Directly using the Atlas API\u2019s is quick to implement and easy to setup for administrators. Atlas uses a search engine underwater (embedded Solr) to perform search queries, thus in theory this method should scale up. Disadvantages are that we are limited to the REST api that Atlas offers, we could potentially add functionality via pull requests and extend the search capabilities. The advanced search provides a DSL which contains basic forms of aggregation and arithmetic. Use Data Builder to fill Elasticsearch from Atlas Adopting Atlas within the Data Builder to fill Elasticsearch is a relatively straightforward way of staying compatible with the Neo4j database. It could either be pulling data from Atlas or being pushed by Kafka. This method requires a setup of Elasticsearch and Airflow, which increases the amount of infrastructure and maintenance. Another disadvantage is that with a big inflow of metadata this method might not scale as well as the other methods. Use underlying Solr or Elasticsearch from Apache Atlas Within Atlas there is the possibility to open up either Solr or the experimental Elasticsearch. It depends on janusgraph (the behind the scenes graph database) which populates the search engine. Therefore the search engine would not be compatible with the data builder setup. Adoption of such a search engine would require either new queries, some kind of transformer within the search engine, or changes within Atlas itself.","title":"Atlas search investigation"},{"location":"search/docs/atlas-search/#discussion","text":"Both the REST API approach and the data builder approach can be implemented and be configurable. Both approaches have their own benefits, the data builder together provides a more fine-tuned search whereas the Atlas REST API comes out of the box with Atlas. The last approach of using the underlying search engine from Atlas provides direct access to all the meta data with a decent search API. However, integration would be less straight forward as the indexes would differ from the data builders search engine loader. The focus is initially to implement the REST API approach and afterwards potentially implement an Atlas data extractor and importer within the Amundsen Data Builder. So that administrators have more flexibility in combining data sources.","title":"Discussion"},{"location":"tutorials/data-preview-with-superset/","text":"How to setup a preview client with Apache Superset \u00b6 In the previous tutorial , we talked about how to index the table metadata for a postgres database. In this tutorial, we will walk through how to configure data preview for this films table using Apache Superset. Amundsen provides an integration between Amundsen and BI Viz tool for data preview. It is not necessary to use Apache Superset as long as the BI Viz tool provides endpoint to do querying and get the results back from the BI tool. Apache Superset is an open-source business intelligence tool that can be used for data exploration and it is what we leverage internally at Lyft to support the feature. Please setup Apache Superset following its official installation guide : # Install superset pip install apache-superset # Initialize the database superset db upgrade # Create an admin user (you will be prompted to set a username, first and last name before setting a password) $ export FLASK_APP = superset superset fab create-admin # Load some data to play with superset load_examples # Create default roles and permissions superset init # To start a development web server on port 8088, use -p to bind to another port superset run -p 8088 --with-threads --reload --debugger Once setup properly, you could view the superset UI as following: We need to add the postgres database to superset as the following: We could verify the content of the films table using superset\u2019s sqlab feature: Next, We need to build a preview client following this guide and the example client code . There are a couple of things to keep in mind: We could start with an unauthenticated Superset( example superset config ), but in production, we will need to send the impersonate info to Superset to properly verify whether the given user could view the data. When we build the client, we could need to configure the database id instead of the database name when send the request to superset. Once we configure the preview client, put it in the frontend service entry point ( example ) and restart the frontend. We could now view the preview data for the films table in Amundsen. From the above figure, the preview button on the table page is clickable. Once it clicked, you could see the actual data queried from Apache Superset:","title":"How to setup a preview client with Apache Superset"},{"location":"tutorials/data-preview-with-superset/#how-to-setup-a-preview-client-with-apache-superset","text":"In the previous tutorial , we talked about how to index the table metadata for a postgres database. In this tutorial, we will walk through how to configure data preview for this films table using Apache Superset. Amundsen provides an integration between Amundsen and BI Viz tool for data preview. It is not necessary to use Apache Superset as long as the BI Viz tool provides endpoint to do querying and get the results back from the BI tool. Apache Superset is an open-source business intelligence tool that can be used for data exploration and it is what we leverage internally at Lyft to support the feature. Please setup Apache Superset following its official installation guide : # Install superset pip install apache-superset # Initialize the database superset db upgrade # Create an admin user (you will be prompted to set a username, first and last name before setting a password) $ export FLASK_APP = superset superset fab create-admin # Load some data to play with superset load_examples # Create default roles and permissions superset init # To start a development web server on port 8088, use -p to bind to another port superset run -p 8088 --with-threads --reload --debugger Once setup properly, you could view the superset UI as following: We need to add the postgres database to superset as the following: We could verify the content of the films table using superset\u2019s sqlab feature: Next, We need to build a preview client following this guide and the example client code . There are a couple of things to keep in mind: We could start with an unauthenticated Superset( example superset config ), but in production, we will need to send the impersonate info to Superset to properly verify whether the given user could view the data. When we build the client, we could need to configure the database id instead of the database name when send the request to superset. Once we configure the preview client, put it in the frontend service entry point ( example ) and restart the frontend. We could now view the preview data for the films table in Amundsen. From the above figure, the preview button on the table page is clickable. Once it clicked, you could see the actual data queried from Apache Superset:","title":"How to setup a preview client with Apache Superset"},{"location":"tutorials/index-postgres/","text":"How to index metadata for real life databases \u00b6 From previous doc , we have indexed tables from a csv files. In real production cases, the table metadata is stored in data warehouses(e.g Hive, Postgres, Mysql, Snowflake, Bigquery etc.) which Amundsen has the extractors for metadata extraction. In this tutorial, we will use a postgres db as an example to walk through how to index metadata for a postgres database. The doc won\u2019t cover how to setup a postgres database. In the example, we have a postgres table in localhost postgres named films . We leverage the postgres metadata extractor to extract the metadata information of the postgres database. We could call the metadata extractor in an adhoc python function as this example or from an Airflow DAG. Once we run the script, we could search the films table using Amundsen Search. We could also find and view the films table in the table detail page. This tutorial uses postgres to serve as an example, but you could apply the same approach for your various data warehouses. If Amundsen doesn\u2019t provide the extractor, you could build one based on the API and contribute the extractor back to us!","title":"How to index metadata for real life databases"},{"location":"tutorials/index-postgres/#how-to-index-metadata-for-real-life-databases","text":"From previous doc , we have indexed tables from a csv files. In real production cases, the table metadata is stored in data warehouses(e.g Hive, Postgres, Mysql, Snowflake, Bigquery etc.) which Amundsen has the extractors for metadata extraction. In this tutorial, we will use a postgres db as an example to walk through how to index metadata for a postgres database. The doc won\u2019t cover how to setup a postgres database. In the example, we have a postgres table in localhost postgres named films . We leverage the postgres metadata extractor to extract the metadata information of the postgres database. We could call the metadata extractor in an adhoc python function as this example or from an Airflow DAG. Once we run the script, we could search the films table using Amundsen Search. We could also find and view the films table in the table detail page. This tutorial uses postgres to serve as an example, but you could apply the same approach for your various data warehouses. If Amundsen doesn\u2019t provide the extractor, you could build one based on the API and contribute the extractor back to us!","title":"How to index metadata for real life databases"},{"location":"tutorials/user-profiles/","text":"People resources \u00b6 What can I do with User Resources? \u00b6 User profile pages and the ability to bookmark/favorite and search for users is also available as of now. See a demo of what they feels like from an end user viewpoint from around the 36 minute mark of this September 2019 talk - so you could actually argue that this video snippet can work as an end user guide. How do I enable User pages? \u00b6 The configuration to have Users available consists of: Enable the users profile page index and display feature by performing this frontend configuration There are two different alternative ways to populate user profile data. You can either: Configure the Metadata service to a do a live lookup in some directory service, like LDAP or a HR system. Setup ongoing ingest of user profile data as they onboard/change/offboard into Neo4j and Elasticsearch effectively caching it with the pros/cons of that (similar to what the Databuilder sample loader does from user CSV, see the \u201cpre-cooked demo data\u201d link in the Architecture overview Note Currently, for both of these options Amundsen only provides these hooks/interfaces to add your own implementation. If you build something you think is generally useful, contributions are welcome! Configure login, according to the Authentication guide","title":"How to setup user profiles"},{"location":"tutorials/user-profiles/#people-resources","text":"","title":"People resources"},{"location":"tutorials/user-profiles/#what-can-i-do-with-user-resources","text":"User profile pages and the ability to bookmark/favorite and search for users is also available as of now. See a demo of what they feels like from an end user viewpoint from around the 36 minute mark of this September 2019 talk - so you could actually argue that this video snippet can work as an end user guide.","title":"What can I do with User Resources?"},{"location":"tutorials/user-profiles/#how-do-i-enable-user-pages","text":"The configuration to have Users available consists of: Enable the users profile page index and display feature by performing this frontend configuration There are two different alternative ways to populate user profile data. You can either: Configure the Metadata service to a do a live lookup in some directory service, like LDAP or a HR system. Setup ongoing ingest of user profile data as they onboard/change/offboard into Neo4j and Elasticsearch effectively caching it with the pros/cons of that (similar to what the Databuilder sample loader does from user CSV, see the \u201cpre-cooked demo data\u201d link in the Architecture overview Note Currently, for both of these options Amundsen only provides these hooks/interfaces to add your own implementation. If you build something you think is generally useful, contributions are welcome! Configure login, according to the Authentication guide","title":"How do I enable User pages?"}]}